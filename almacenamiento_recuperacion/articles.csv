identifier,title,description,subject,creator
http://arxiv.org/abs/0704.3504,Smooth R\'enyi Entropy of Ergodic Quantum Information Sources,"  We prove that the average smooth Renyi entropy rate will approach the entropy rate of a stationary, ergodic information source, which is equal to the Shannon entropy rate for a classical information source and the von Neumann entropy rate for a quantum information source. ",Quantum Physics ; Computer Science - Information Theory ; ,"Schoenmakers, Berry ; Tjoelker, Jilles ; Tuyls, Pim ; Verbitskiy, Evgeny ; "
http://arxiv.org/abs/0706.1402,Analyzing Design Process and Experiments on the AnITA Generic Tutoring   System,"  In the field of tutoring systems, investigations have shown that there are many tutoring systems specific to a specific domain that, because of their static architecture, cannot be adapted to other domains. As consequence, often neither methods nor knowledge can be reused. In addition, the knowledge engineer must have programming skills in order to enhance and evaluate the system. One particular challenge is to tackle these problems with the development of a generic tutoring system. AnITA, as a stand-alone application, has been developed and implemented particularly for this purpose. However, in the testing phase, we discovered that this architecture did not fully match the user's intuitive understanding of the use of a learning tool. Therefore, AnITA has been redesigned to exclusively work as a client/server application and renamed to AnITA2. This paper discusses the evolvements made on the AnITA tutoring system, the goal of which is to use generic principles for system re-use in any domain. Two experiments were conducted, and the results are presented in this paper. ",Computer Science - Computers and Society ; Computer Science - Human-Computer Interaction ; ,"Brust, Matthias R. ; Rothkugel, Steffen ; "
http://arxiv.org/abs/0710.0736,Colour image segmentation by the vector-valued Allen-Cahn phase-field   model: a multigrid solution,"  We propose a new method for the numerical solution of a PDE-driven model for colour image segmentation and give numerical examples of the results. The method combines the vector-valued Allen-Cahn phase field equation with initial data fitting terms. This method is known to be closely related to the Mumford-Shah problem and the level set segmentation by Chan and Vese. Our numerical solution is performed using a multigrid splitting of a finite element space, thereby producing an efficient and robust method for the segmentation of large images. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Numerical Analysis ; I.4.6 ; G.1.8 ; ,"Kay, David A ; Tomasi, Alessandro ; "
http://arxiv.org/abs/0803.2570,Unequal Error Protection: An Information Theoretic Perspective,"  An information theoretic framework for unequal error protection is developed in terms of the exponential error bounds. The fundamental difference between the bit-wise and message-wise unequal error protection (UEP) is demonstrated, for fixed length block codes on DMCs without feedback. Effect of feedback is investigated via variable length block codes. It is shown that, feedback results in a significant improvement in both bit-wise and message-wise UEP (except the single message case for missed detection). The distinction between false-alarm and missed-detection formalizations for message-wise UEP is also considered. All results presented are at rates close to capacity. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Borade, Shashi ; Nakiboglu, Baris ; Zheng, Lizhong ; "
http://arxiv.org/abs/0808.0084,On the hitting times of quantum versus random walks,"  In this paper we define new Monte Carlo type classical and quantum hitting times, and we prove several relationships among these and the already existing Las Vegas type definitions. In particular, we show that for some marked state the two types of hitting time are of the same order in both the classical and the quantum case.   Further, we prove that for any reversible ergodic Markov chain $P$, the quantum hitting time of the quantum analogue of $P$ has the same order as the square root of the classical hitting time of $P$. We also investigate the (im)possibility of achieving a gap greater than quadratic using an alternative quantum walk.   Finally, we present new quantum algorithms for the detection and finding problems. The complexities of both algorithms are related to the new, potentially smaller, quantum hitting times. The detection algorithm is based on phase estimation and is particularly simple. The finding algorithm combines a similar phase estimation based procedure with ideas of Tulsi from his recent theorem for the 2D grid. Extending his result, we show that for any state-transitive Markov chain with unique marked state, the quantum hitting time is of the same order for both the detection and finding problems. ",Quantum Physics ; Computer Science - Data Structures and Algorithms ; ,"Magniez, Frederic ; Nayak, Ashwin ; Richter, Peter C. ; Santha, Miklos ; "
http://arxiv.org/abs/0811.1254,Coding Theory and Algebraic Combinatorics,"  This chapter introduces and elaborates on the fruitful interplay of coding theory and algebraic combinatorics, with most of the focus on the interaction of codes with combinatorial designs, finite geometries, simple groups, sphere packings, kissing numbers, lattices, and association schemes. In particular, special interest is devoted to the relationship between codes and combinatorial designs. We describe and recapitulate important results in the development of the state of the art. In addition, we give illustrative examples and constructions, and highlight recent advances. Finally, we provide a collection of significant open problems and challenges concerning future research. ",Mathematics - Combinatorics ; Computer Science - Information Theory ; ,"Huber, Michael ; "
http://arxiv.org/abs/0811.2853,Generating Random Networks Without Short Cycles,"  Random graph generation is an important tool for studying large complex networks. Despite abundance of random graph models, constructing models with application-driven constraints is poorly understood. In order to advance state-of-the-art in this area, we focus on random graphs without short cycles as a stylized family of graphs, and propose the RandGraph algorithm for randomly generating them. For any constant k, when m=O(n^{1+1/[2k(k+3)]}), RandGraph generates an asymptotically uniform random graph with n vertices, m edges, and no cycle of length at most k using O(n^2m) operations. We also characterize the approximation error for finite values of n. To the best of our knowledge, this is the first polynomial-time algorithm for the problem. RandGraph works by sequentially adding $m$ edges to an empty graph with n vertices. Recently, such sequential algorithms have been successful for random sampling problems. Our main contributions to this line of research includes introducing a new approach for sequentially approximating edge-specific probabilities at each step of the algorithm, and providing a new method for analyzing such algorithms. ",Computer Science - Data Structures and Algorithms ; Computer Science - Information Theory ; ,"Bayati, Mohsen ; Montanari, Andrea ; Saberi, Amin ; "
http://arxiv.org/abs/0812.2709,Variations on a theme by Schalkwijk and Kailath,"  Schalkwijk and Kailath (1966) developed a class of block codes for Gaussian channels with ideal feedback for which the probability of decoding error decreases as a second-order exponent in block length for rates below capacity. This well-known but surprising result is explained and simply derived here in terms of a result by Elias (1956) concerning the minimum mean-square distortion achievable in transmitting a single Gaussian random variable over multiple uses of the same Gaussian channel. A simple modification of the Schalkwijk-Kailath scheme is then shown to have an error probability that decreases with an exponential order which is linearly increasing with block length. In the infinite bandwidth limit, this scheme produces zero error probability using bounded expected energy at all rates below capacity. A lower bound on error probability for the finite bandwidth case is then derived in which the error probability decreases with an exponential order which is linearly increasing in block length at the same rate as the upper bound. ",Computer Science - Information Theory ; ,"Gallager, Robert G. ; Nakiboglu, Baris ; "
http://arxiv.org/abs/0903.0197,Rotation Distance is Fixed-Parameter Tractable,"  Rotation distance between trees measures the number of simple operations it takes to transform one tree into another. There are no known polynomial-time algorithms for computing rotation distance. In the case of ordered rooted trees, we show that the rotation distance between two ordered trees is fixed-parameter tractable, in the parameter, k, the rotation distance. The proof relies on the kernalization of the initial trees to trees with size bounded by 7k. ",Computer Science - Data Structures and Algorithms ; ,"Cleary, Sean ; John, Katherine St. ; "
http://arxiv.org/abs/0903.0199,A Linear-Time Approximation Algorithm for Rotation Distance,"  Rotation distance between rooted binary trees measures the number of simple operations it takes to transform one tree into another. There are no known polynomial-time algorithms for computing rotation distance. We give an efficient, linear-time approximation algorithm, which estimates the rotation distance, within a provable factor of 2, between ordered rooted binary trees. . ",Computer Science - Data Structures and Algorithms ; ,"Cleary, Sean ; John, Katherine St. ; "
http://arxiv.org/abs/0903.1291,The quantum query complexity of certification,"  We study the quantum query complexity of finding a certificate for a d-regular, k-level balanced NAND formula. Up to logarithmic factors, we show that the query complexity is Theta(d^{(k+1)/2}) for 0-certificates, and Theta(d^{k/2}) for 1-certificates. In particular, this shows that the zero-error quantum query complexity of evaluating such formulas is O(d^{(k+1)/2}) (again neglecting a logarithmic factor). Our lower bound relies on the fact that the quantum adversary method obeys a direct sum theorem. ",Quantum Physics ; Computer Science - Computational Complexity ; ,"Ambainis, Andris ; Childs, Andrew M. ; Gall, François Le ; Tani, Seiichiro ; "
http://arxiv.org/abs/0903.2923,On uncertainty principles in the finite dimensional setting,  The aim of this paper is to prove an uncertainty principle for the representation of a vector in two bases. Our result extends previously known qualitative uncertainty principles into quantitative estimates. We then show how to transfer this result to the discrete version of the Short Time Fourier Transform. An application to trigonometric polynomials is also given. ,Mathematics - Classical Analysis and ODEs ; Computer Science - Information Theory ; ,"Ghobber, Saifallah ; Jaming, Philippe ; "
http://arxiv.org/abs/0903.4386,Error-and-Erasure Decoding for Block Codes with Feedback,"  Inner and outer bounds are derived on the optimal performance of fixed length block codes on discrete memoryless channels with feedback and errors-and-erasures decoding. First an inner bound is derived using a two phase encoding scheme with communication and control phases together with the optimal decoding rule for the given encoding scheme, among decoding rules that can be represented in terms of pairwise comparisons between the messages. Then an outer bound is derived using a generalization of the straight-line bound to errors-and-erasures decoders and the optimal error exponent trade off of a feedback encoder with two messages. In addition upper and lower bounds are derived, for the optimal erasure exponent of error free block codes in terms of the rate. Finally we present a proof of the fact that the optimal trade off between error exponents of a two message code does not increase with feedback on DMCs. ",Computer Science - Information Theory ; ,"Nakiboglu, Baris ; Zheng, Lizhong ; "
http://arxiv.org/abs/0904.2051,Joint-sparse recovery from multiple measurements,"  The joint-sparse recovery problem aims to recover, from sets of compressed measurements, unknown sparse matrices with nonzero entries restricted to a subset of rows. This is an extension of the single-measurement-vector (SMV) problem widely studied in compressed sensing. We analyze the recovery properties for two types of recovery algorithms. First, we show that recovery using sum-of-norm minimization cannot exceed the uniform recovery rate of sequential SMV using $\ell_1$ minimization, and that there are problems that can be solved with one approach but not with the other. Second, we analyze the performance of the ReMBo algorithm [M. Mishali and Y. Eldar, IEEE Trans. Sig. Proc., 56 (2008)] in combination with $\ell_1$ minimization, and show how recovery improves as more measurements are taken. From this analysis it follows that having more measurements than number of nonzero rows does not improve the potential theoretical recovery rate. ",Computer Science - Information Theory ; ,"Berg, Ewout van den ; Friedlander, Michael P. ; "
http://arxiv.org/abs/0907.3220,Inter Genre Similarity Modelling For Automatic Music Genre   Classification,"  Music genre classification is an essential tool for music information retrieval systems and it has been finding critical applications in various media platforms. Two important problems of the automatic music genre classification are feature extraction and classifier design. This paper investigates inter-genre similarity modelling (IGS) to improve the performance of automatic music genre classification. Inter-genre similarity information is extracted over the mis-classified feature population. Once the inter-genre similarity is modelled, elimination of the inter-genre similarity reduces the inter-genre confusion and improves the identification rates. Inter-genre similarity modelling is further improved with iterative IGS modelling(IIGS) and score modelling for IGS elimination(SMIGS). Experimental results with promising classification improvements are provided. ",Computer Science - Sound ; Computer Science - Artificial Intelligence ; Statistics - Machine Learning ; H.5.5 ; I.5 ; ,"Bagci, Ulas ; Erzin, Engin ; "
http://arxiv.org/abs/0907.3965,P != NP Proof,"  This paper demonstrates that P \not= NP. The way was to generalize the traditional definitions of the classes P and NP, to construct an artificial problem (a generalization to SAT: The XG-SAT, much more difficult than the former) and then to demonstrate that it is in NP but not in P (where the classes P and NP are generalized and called too simply P and NP in this paper, and then it is explained why the traditional classes P and NP should be fixed and replaced by these generalized ones into Theory of Computer Science). The demonstration consists of: 1. Definition of Restricted Type X Program; 2. Definition of the General Extended Problem of Satisfiability of a Boolean Formula - XG-SAT; 3. Generalization to classes P and NP; 4. Demonstration that the XG-SAT is in NP; 5. Demonstration that the XG-SAT is not in P; 6. Demonstration that the Baker-Gill-Solovay Theorem does not refute the proof; 7. Demonstration that the Razborov-Rudich Theorem does not refute the proof; 8. Demonstration that the Aaronson-Wigderson Theorem does not refute the proof. ","Computer Science - Computational Complexity ; 68Q15 (Primary), 68Q17 (Secondary) ; ","Barbosa, André Luiz ; "
http://arxiv.org/abs/0910.2912,Universally Composable Quantum Multi-Party Computation,"  The Universal Composability model (UC) by Canetti (FOCS 2001) allows for secure composition of arbitrary protocols. We present a quantum version of the UC model which enjoys the same compositionality guarantees. We prove that in this model statistically secure oblivious transfer protocols can be constructed from commitments. Furthermore, we show that every statistically classically UC secure protocol is also statistically quantum UC secure. Such implications are not known for other quantum security definitions. As a corollary, we get that quantum UC secure protocols for general multi-party computation can be constructed from commitments. ",Quantum Physics ; Computer Science - Cryptography and Security ; ,"Unruh, Dominique ; "
http://arxiv.org/abs/0910.5577,On the stability of two-chunk file-sharing systems,"  We consider five different peer-to-peer file sharing systems with two chunks, with the aim of finding chunk selection algorithms that have provably stable performance with any input rate and assuming non-altruistic peers who leave the system immediately after downloading the second chunk. We show that many algorithms that first looked promising lead to unstable or oscillating behavior. However, we end up with a system with desirable properties. Most of our rigorous results concern the corresponding deterministic large system limits, but in two simplest cases we provide proofs for the stochastic systems also. ","Computer Science - Operating Systems ; Mathematics - Probability ; 60K25, 68M14 ; ","Norros, Ilkka ; Reittu, Hannu ; Eirola, Timo ; "
http://arxiv.org/abs/0911.1507,MAC Layer Hurdles in BSNs,"  The last few decades have seen considerable research progress in microelectronics and integrated circuits, system-on-chip design, wireless communication, and sensor technology. This progress has enabled the seamless integration of autonomous wireless sensor nodes around a human body to create a Body Sensor Network (BSN). The development of a proactive and ambulatory BSN induces a number of enormous issues and challenges. This paper presents the technical hurdles during the design and implementation of a low-power Medium Access Control (MAC) protocol for in-body and on-body sensor networks. We analyze the performance of IEEE 802.15.4 protocol for the on-body sensor network. We also provide a comprehensive insight into the heterogeneous characteristics of the in-body sensor network. A low-power technique called Pattern-Based Wake-up Table is proposed to handle the normal traffic in a BSN. The proposed technique provides a reliable solution towards low-power communication in the in-body sensor network. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; Khan, Pervez ; Choi, Young-Woo ; Lee, Hyung-Soo ; Kwak, Kyung Sup ; "
http://arxiv.org/abs/0911.1509,On the Development of Low Power MAC Protocol for WBANs,"  Current advances in wireless communication, microelectronics, semiconductor technologies, and intelligent sensors have contributed to the development of unobtrusive WBANs. These networks provide long term health monitoring of patients without any constraint in their normal activities. Traditional MAC protocols do not accommodate the assorted WBAN traffic requirements in a power efficient manner. In this paper, we present a brief discussion on the development process of a low power MAC protocol for WBANs. We observe the behavior of a beacon-enabled IEEE 802.15.4 for on-body sensor networks. We further propose a low power technique called traffic based wakeup mechanism for a WBAN that exploits the traffic patterns of the BAN Nodes to ensure power efficient and reliable communication. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; Khan, Pervez ; Kwak, Kyung Sup ; "
http://arxiv.org/abs/0911.1544,Towards Power Efficient MAC Protocol for In-Body and On-Body Sensor   Networks,"  This paper presents an empirical discussion on the design and implementation of a power-efficient Medium Access Control (MAC) protocol for in-body and on-body sensor networks. We analyze the performance of a beacon-enabled IEEE 802.15.4, PB-TDMA, and S-MAC protocols for on-body sensor networks. We further present a Traffic Based Wakeup Mechanism that utilizes the traffic patterns of the BAN Nodes (BNs) to accommodate the entire BSN traffic. To enable a logical connection between different BNs working on different frequency bands, a method called Bridging function is proposed. The Bridging function integrates all BNs working on different bands into a complete BSN. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; An, Xizhi ; Kwak, Kyung Sup ; "
http://arxiv.org/abs/0911.1546,A Study of Implanted and Wearable Body Sensor Networks,"  Recent advances in intelligent sensors, microelectronics and integrated circuit, system-on-chip design and low power wireless communication introduced the development of miniaturised and autonomous sensor nodes. These tiny sensor nodes can be deployed to develop a proactive Body Sensor Network (BSN). The rapid advancement in ultra low-power RF (radio frequency) technology enables invasive and non-invasive devices to communicate with a remote station. This communication revolutionizes healthcare system by enabling long term health monitoring of a patient and providing real time feedback to the medical experts. In this paper, we present In-body and On-body communication networks with a special focus on the methodologies of wireless communication between implanted medical devices with external monitoring equipment and recent technological growth in both areas. We also discuss open issues and challenges in a BSN. ",Computer Science - Networking and Internet Architecture ; ,"Ullah, Sana ; Higgins, Henry ; Siddiqui, M. Arif ; Kwak, Kyung Sup ; "
http://arxiv.org/abs/0911.2538,Euclidean versus hyperbolic congestion in idealized versus experimental   networks,"  This paper proposes a mathematical justification of the phenomenon of extreme congestion at a very limited number of nodes in very large networks. It is argued that this phenomenon occurs as a combination of the negative curvature property of the network together with minimum length routing. More specifically, it is shown that, in a large n-dimensional hyperbolic ball B of radius R viewed as a roughly similar model of a Gromov hyperbolic network, the proportion of traffic paths transiting through a small ball near the center is independent of the radius R whereas, in a Euclidean ball, the same proportion scales as 1/R^{n-1}. This discrepancy persists for the traffic load, which at the center of the hyperbolic ball scales as the square of the volume, whereas the same traffic load scales as the volume to the power (n+1)/n in the Euclidean ball. This provides a theoretical justification of the experimental exponent discrepancy observed by Narayan and Saniee between traffic loads in Gromov-hyperbolic networks from the Rocketfuel data base and synthetic Euclidean lattice networks. It is further conjectured that for networks that do not enjoy the obvious symmetry of hyperbolic and Euclidean balls, the point of maximum traffic is near the center of mass of the network. ",Computer Science - Networking and Internet Architecture ; ,"Jonckheere, Edmond ; Lou, Mingji ; Bonahon, Francis ; Baryshnikov, Yuliy ; "
http://arxiv.org/abs/0911.2746,Model Selection: Two Fundamental Measures of Coherence and Their   Algorithmic Significance,"  The problem of model selection arises in a number of contexts, such as compressed sensing, subset selection in linear regression, estimation of structures in graphical models, and signal denoising. This paper generalizes the notion of \emph{incoherence} in the existing literature on model selection and introduces two fundamental measures of coherence---termed as the worst-case coherence and the average coherence---among the columns of a design matrix. In particular, it utilizes these two measures of coherence to provide an in-depth analysis of a simple one-step thresholding (OST) algorithm for model selection. One of the key insights offered by the ensuing analysis is that OST is feasible for model selection as long as the design matrix obeys an easily verifiable property. In addition, the paper also characterizes the model-selection performance of OST in terms of the worst-case coherence, \mu, and establishes that OST performs near-optimally in the low signal-to-noise ratio regime for N x C design matrices with \mu = O(N^{-1/2}). Finally, in contrast to some of the existing literature on model selection, the analysis in the paper is nonasymptotic in nature, it does not require knowledge of the true model order, it is applicable to generic (random or deterministic) design matrices, and it neither requires submatrices of the design matrix to have full rank, nor does it assume a statistical prior on the values of the nonzero entries of the data vector. ",Computer Science - Information Theory ; Mathematics - Statistics Theory ; ,"Bajwa, Waheed U. ; Calderbank, Robert ; Jafarpour, Sina ; "
http://arxiv.org/abs/0911.5153,Self-Reference Ultra-Wideband Systems,"  Towards employing low complexity transceivers for signal reception in Ultra-Wideband (UWB) systems, Transmitted Reference (TR) and Differential TR (DTR) schemes have attracted researchers attention. In this letter, we introduce an alternative, less complex scheme, called Self Reference (SR) UWB transceiver, which uses a modified replica of the received signal itself as reference pulse, resulting in double data rates compared to TR schemes. Moreover, SR eliminates the need for delay lines at the receiver side, which constitute a major drawback of the conventional TR and DTR schemes, while it also requires no channel estimations, resulting in lower complexity implementations and power savings. The performance of the SR scheme is investigated in high-frequency (HF) channels, showing that it offers a better or comparable performance to that of DTR, depending on the channel conditions. ",Computer Science - Networking and Internet Architecture ; ,"Lioumpas, Athanasios S. ; "
http://arxiv.org/abs/1001.1435,"JBotSim, a Tool for Fast Prototyping of Distributed Algorithms in   Dynamic Networks","  JBotSim is a java library that offers basic primitives for prototyping, running, and visualizing distributed algorithms in dynamic networks. With JBotSim, one can implement an idea in minutes and interact with it ({\it e.g.}, add, move, or delete nodes) while it is running. JBotSim is well suited to prepare live demonstrations of your algorithms to colleagues or students; it can also be used to evaluate performance at the algorithmic level (number of messages, number of rounds, etc.). Unlike most tools, JBotSim is not an integrated environment. It is a lightweight library to be used in your program. In this paper, we present an overview of its distinctive features and architecture. ","Computer Science - Mathematical Software ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Networking and Internet Architecture ; ","Casteigts, Arnaud ; "
http://arxiv.org/abs/1001.3780,Combinatorial Bounds and Characterizations of Splitting Authentication   Codes,"  We present several generalizations of results for splitting authentication codes by studying the aspect of multi-fold security. As the two primary results, we prove a combinatorial lower bound on the number of encoding rules and a combinatorial characterization of optimal splitting authentication codes that are multi-fold secure against spoofing attacks. The characterization is based on a new type of combinatorial designs, which we introduce and for which basic necessary conditions are given regarding their existence. ",Computer Science - Cryptography and Security ; Computer Science - Information Theory ; G.2.3 ; ,"Huber, Michael ; "
http://arxiv.org/abs/1002.0747,Efficient Bayesian Learning in Social Networks with Gaussian Estimators,"  We consider a group of Bayesian agents who try to estimate a state of the world $\theta$ through interaction on a social network. Each agent $v$ initially receives a private measurement of $\theta$: a number $S_v$ picked from a Gaussian distribution with mean $\theta$ and standard deviation one. Then, in each discrete time iteration, each reveals its estimate of $\theta$ to its neighbors, and, observing its neighbors' actions, updates its belief using Bayes' Law.   This process aggregates information efficiently, in the sense that all the agents converge to the belief that they would have, had they access to all the private measurements. We show that this process is computationally efficient, so that each agent's calculation can be easily carried out. We also show that on any graph the process converges after at most $2N \cdot D$ steps, where $N$ is the number of agents and $D$ is the diameter of the network. Finally, we show that on trees and on distance transitive-graphs the process converges after $D$ steps, and that it preserves privacy, so that agents learn very little about the private signal of most other agents, despite the efficient aggregation of information. Our results extend those in an unpublished manuscript of the first and last authors. ",Statistics - Applications ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Mossel, Elchanan ; Olsman, Noah ; Tamuz, Omer ; "
http://arxiv.org/abs/1003.1628,Having Fun with Lambert W(x) Function,"  This short note presents the Lambert W(x) function and its possible application in the framework of physics related to the Pierre Auger Observatory. The actual numerical implementation in C++ consists of Halley's and Fritsch's iteration with branch-point expansion, asymptotic series and rational fits as initial approximations. ",Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Veberic, Darko ; "
http://arxiv.org/abs/1004.0208,Delay-rate tradeoff in ergodic interference alignment,"  Ergodic interference alignment, as introduced by Nazer et al (NGJV), is a technique that allows high-rate communication in n-user interference networks with fast fading. It works by splitting communication across a pair of fading matrices. However, it comes with the overhead of a long time delay until matchable matrices occur: the delay is q^n^2 for field size q.   In this paper, we outline two new families of schemes, called JAP and JAP-B, that reduce the expected delay, sometimes at the cost of a reduction in rate from the NGJV scheme. In particular, we give examples of good schemes for networks with few users, and show that in large n-user networks, the delay scales like q^T, where T is quadratic in n for a constant per-user rate and T is constant for a constant sum-rate. We also show that half the single-user rate can be achieved while reducing NGJV's delay from q^n^2 to q^(n-1)(n-2).   This extended version includes complete proofs and more details of good schemes for small n. ",Computer Science - Information Theory ; Mathematics - Probability ; ,"Johnson, Oliver ; Aldridge, Matthew ; Piechocki, Robert ; "
http://arxiv.org/abs/1004.4940,FauxCrypt - A Method of Text Obfuscation,"  Warnings have been raised about the steady diminution of privacy. More and more personal information, such as that contained electronic mail, is moving to cloud computing servers where it might be machine-searched and indexed. FauxCrypt is an algorithm for modification of a plaintext document that leaves it generally readable by a person but not readily searched or indexed by machine. The algorithm employs a dictionary substitution of selected words, and an obfuscating transposition of letters in other words. The obfuscation is designed to leave the words understandable, although they are badly spelled. FauxCrypt is free, open source software, with source code available. ",Computer Science - Cryptography and Security ; D.4.6 ; ,"Gualtieri, Devlin M. ; "
http://arxiv.org/abs/1005.2894,Optimal Gradient Clock Synchronization in Dynamic Networks,"  We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at any time. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network for a long time are better synchronized than distant nodes. This property is called gradient clock synchronization.   Gradient clock synchronization has been widely studied in the static setting, where the network topology does not change. We show that the asymptotically optimal bounds obtained for the static case also apply to our highly dynamic setting: if two nodes remain at distance $d$ from each other for sufficiently long, it is possible to upper bound the difference between their clock values by $O(d \log (D / d))$, where $D$ is the diameter of the network. This is known to be optimal even for static networks. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length $d$ appears between two nodes, the time required until the clock skew between the two nodes is reduced to $O(d \log (D / d))$ is $O(D)$, which we prove to be optimal. Finally, the techniques employed for the more intricate analysis of the algorithm for dynamic graphs provide additional insights that are also of interest for the static setting. In particular, we establish self-stabilization of the gradient property within $O(D)$ time. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; F.2.2 ; G.2.2 ; ","Kuhn, Fabian ; Lenzen, Christoph ; Locher, Thomas ; Oshman, Rotem ; "
http://arxiv.org/abs/1005.3010,A Proof for P =? NP Problem,"  The $\textbf{P} =? \textbf{NP}$ problem is an important problem in contemporary mathematics and theoretical computer science. Many proofs have been proposed to this problem. This paper proposes a theoretic proof for $\textbf{P} =? \textbf{NP}$ problem. The central idea of this proof is a recursive definition for Turing machine (shortly TM) that accepts the encoding strings of valid TMs within any given alphabet. As the concepts ""Tao"", ""Yin"" and ""Yang"" described in Chinese philosopy, an infinite sequence of TM, within any given alphabet, is constructed recursively, and it is proven that the sequence includes all valid TMs, and each of them run in polynomial time. Based on these TMs, the class \textbf{D} that includes all decidable languages is defined, and then the class $\textbf{P}$ and $\textbf{NP}$ are defined. By proving $\textbf{P} \subseteq \textbf{NP}$ and $\textbf{NP} \subseteq \textbf{P}$, the result $\textbf{P}=\textbf{NP}$ is proven. ",Computer Science - Computational Complexity ; F.1.3 ; G.2.1 ; ,"Wan, Changlin ; "
http://arxiv.org/abs/1006.0719,Why Gabor Frames? Two Fundamental Measures of Coherence and Their Role   in Model Selection,"  This paper studies non-asymptotic model selection for the general case of arbitrary design matrices and arbitrary nonzero entries of the signal. In this regard, it generalizes the notion of incoherence in the existing literature on model selection and introduces two fundamental measures of coherence---termed as the worst-case coherence and the average coherence---among the columns of a design matrix. It utilizes these two measures of coherence to provide an in-depth analysis of a simple, model-order agnostic one-step thresholding (OST) algorithm for model selection and proves that OST is feasible for exact as well as partial model selection as long as the design matrix obeys an easily verifiable property. One of the key insights offered by the ensuing analysis in this regard is that OST can successfully carry out model selection even when methods based on convex optimization such as the lasso fail due to the rank deficiency of the submatrices of the design matrix. In addition, the paper establishes that if the design matrix has reasonably small worst-case and average coherence then OST performs near-optimally when either (i) the energy of any nonzero entry of the signal is close to the average signal energy per nonzero entry or (ii) the signal-to-noise ratio in the measurement system is not too high. Finally, two other key contributions of the paper are that (i) it provides bounds on the average coherence of Gaussian matrices and Gabor frames, and (ii) it extends the results on model selection using OST to low-complexity, model-order agnostic recovery of sparse signals with arbitrary nonzero entries. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; Statistics - Machine Learning ; ,"Bajwa, Waheed U. ; Calderbank, Robert ; Jafarpour, Sina ; "
http://arxiv.org/abs/1006.1029,Chi-square-based scoring function for categorization of MEDLINE   citations,"  Objectives: Text categorization has been used in biomedical informatics for identifying documents containing relevant topics of interest. We developed a simple method that uses a chi-square-based scoring function to determine the likelihood of MEDLINE citations containing genetic relevant topic. Methods: Our procedure requires construction of a genetic and a nongenetic domain document corpus. We used MeSH descriptors assigned to MEDLINE citations for this categorization task. We compared frequencies of MeSH descriptors between two corpora applying chi-square test. A MeSH descriptor was considered to be a positive indicator if its relative observed frequency in the genetic domain corpus was greater than its relative observed frequency in the nongenetic domain corpus. The output of the proposed method is a list of scores for all the citations, with the highest score given to those citations containing MeSH descriptors typical for the genetic domain. Results: Validation was done on a set of 734 manually annotated MEDLINE citations. It achieved predictive accuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method by comparing it to three machine learning algorithms (support vector machines, decision trees, na\""ive Bayes). Although the differences were not statistically significantly different, results showed that our chi-square scoring performs as good as compared machine learning algorithms. Conclusions: We suggest that the chi-square scoring is an effective solution to help categorize MEDLINE citations. The algorithm is implemented in the BITOLA literature-based discovery support system as a preprocessor for gene symbol disambiguation process. ",Computer Science - Information Retrieval ; Statistics - Applications ; Statistics - Machine Learning ; ,"Kastrin, Andrej ; Peterlin, Borut ; Hristovski, Dimitar ; "
http://arxiv.org/abs/1006.1030,Rasch-based high-dimensionality data reduction and class prediction with   applications to microarray gene expression data,"  Class prediction is an important application of microarray gene expression data analysis. The high-dimensionality of microarray data, where number of genes (variables) is very large compared to the number of samples (obser- vations), makes the application of many prediction techniques (e.g., logistic regression, discriminant analysis) difficult. An efficient way to solve this prob- lem is by using dimension reduction statistical techniques. Increasingly used in psychology-related applications, Rasch model (RM) provides an appealing framework for handling high-dimensional microarray data. In this paper, we study the potential of RM-based modeling in dimensionality reduction with binarized microarray gene expression data and investigate its prediction ac- curacy in the context of class prediction using linear discriminant analysis. Two different publicly available microarray data sets are used to illustrate a general framework of the approach. Performance of the proposed method is assessed by re-randomization scheme using principal component analysis (PCA) as a benchmark method. Our results show that RM-based dimension reduction is as effective as PCA-based dimension reduction. The method is general and can be applied to the other high-dimensional data problems. ",Computer Science - Artificial Intelligence ; Statistics - Applications ; Statistics - Methodology ; Statistics - Machine Learning ; ,"Kastrin, Andrej ; Peterlin, Borut ; "
http://arxiv.org/abs/1006.5892,Computational complexity of reconstruction and isomorphism testing for   designs and line graphs,"  Graphs with high symmetry or regularity are the main source for experimentally hard instances of the notoriously difficult graph isomorphism problem. In this paper, we study the computational complexity of isomorphism testing for line graphs of $t$-$(v,k,\lambda)$ designs. For this class of highly regular graphs, we obtain a worst-case running time of $O(v^{\log v + O(1)})$ for bounded parameters $t,k,\lambda$. In a first step, our approach makes use of the Babai--Luks algorithm to compute canonical forms of $t$-designs. In a second step, we show that $t$-designs can be reconstructed from their line graphs in polynomial-time. The first is algebraic in nature, the second purely combinatorial. For both, profound structural knowledge in design theory is required. Our results extend earlier complexity results about isomorphism testing of graphs generated from Steiner triple systems and block designs. ",Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; F.2.2 ; G.2.2 ; ,"Huber, Michael ; "
http://arxiv.org/abs/1007.5491,The Coarsest Precongruences Respecting Safety and Liveness Properties,"  This paper characterises the coarsest refinement preorders on labelled transition systems that are precongruences for renaming and partially synchronous interleaving operators, and respect all safety, liveness, and conditional liveness properties, respectively. ",Computer Science - Logic in Computer Science ; ,"van Glabbeek, Rob ; "
http://arxiv.org/abs/1008.0851,Identification of Parametric Underspread Linear Systems and   Super-Resolution Radar,"  Identification of time-varying linear systems, which introduce both time-shifts (delays) and frequency-shifts (Doppler-shifts), is a central task in many engineering applications. This paper studies the problem of identification of underspread linear systems (ULSs), whose responses lie within a unit-area region in the delay Doppler space, by probing them with a known input signal. It is shown that sufficiently-underspread parametric linear systems, described by a finite set of delays and Doppler-shifts, are identifiable from a single observation as long as the time bandwidth product of the input signal is proportional to the square of the total number of delay Doppler pairs in the system. In addition, an algorithm is developed that enables identification of parametric ULSs from an input train of pulses in polynomial time by exploiting recent results on sub-Nyquist sampling for time delay estimation and classical results on recovery of frequencies from a sum of complex exponentials. Finally, application of these results to super-resolution target detection using radar is discussed. Specifically, it is shown that the proposed procedure allows to distinguish between multiple targets with very close proximity in the delay Doppler space, resulting in a resolution that substantially exceeds that of standard matched-filtering based techniques without introducing leakage effects inherent in recently proposed compressed sensing-based radar methods. ",Computer Science - Information Theory ; ,"Bajwa, Waheed U. ; Gedalyahu, Kfir ; Eldar, Yonina C. ; "
http://arxiv.org/abs/1009.5894,Some Theorems on the Algorithmic Approach to Probability Theory and   Information Theory,"  This is a 1971 dissertation. Only its extended abstract was published at the time. While some results appeared in other publications, a number of details remained unpublished and may still have relevance. ",Computer Science - Information Theory ; ,"Levin, Leonid A. ; "
http://arxiv.org/abs/1011.0774,"Leaders, Followers, and Community Detectio","  Communities in social networks or graphs are sets of well-connected, overlapping vertices. The effectiveness of a community detection algorithm is determined by accuracy in finding the ground-truth communities and ability to scale with the size of the data. In this work, we provide three contributions. First, we show that a popular measure of accuracy known as the F1 score, which is between 0 and 1, with 1 being perfect detection, has an information lower bound is 0.5. We provide a trivial algorithm that produces communities with an F1 score of 0.5 for any graph! Somewhat surprisingly, we find that popular algorithms such as modularity optimization, BigClam and CESNA have F1 scores less than 0.5 for the popular IMDB graph. To rectify this, as the second contribution we propose a generative model for community formation, the sequential community graph, which is motivated by the formation of social networks. Third, motivated by our generative model, we propose the leader-follower algorithm (LFA). We prove that it recovers all communities for sequential community graphs by establishing a structural result that sequential community graphs are chordal. For a large number of popular social networks, it recovers communities with a much higher F1 score than other popular algorithms. For the IMDB graph, it obtains an F1 score of 0.81. We also propose a modification to the LFA called the fast leader-follower algorithm (FLFA) which in addition to being highly accurate, is also fast, with a scaling that is almost linear in the network size. ",Statistics - Machine Learning ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Parthasarathy, Dhruv ; Shah, Devavrat ; Zaman, Tauhid ; "
http://arxiv.org/abs/1012.4019,Constructing elliptic curve isogenies in quantum subexponential time,"  Given two elliptic curves over a finite field having the same cardinality and endomorphism ring, it is known that the curves admit an isogeny between them, but finding such an isogeny is believed to be computationally difficult. The fastest known classical algorithm takes exponential time, and prior to our work no faster quantum algorithm was known. Recently, public-key cryptosystems based on the presumed hardness of this problem have been proposed as candidates for post-quantum cryptography. In this paper, we give a subexponential-time quantum algorithm for constructing isogenies, assuming the Generalized Riemann Hypothesis (but with no other assumptions). Our algorithm is based on a reduction to a hidden shift problem, together with a new subexponential-time algorithm for evaluating isogenies from kernel ideals (under only GRH), and represents the first nontrivial application of Kuperberg's quantum algorithm for the hidden shift problem. This result suggests that isogeny-based cryptosystems may be uncompetitive with more mainstream quantum-resistant cryptosystems such as lattice-based cryptosystems. ",Quantum Physics ; Computer Science - Computational Complexity ; Mathematics - Number Theory ; ,"Childs, Andrew M. ; Jao, David ; Soukharev, Vladimir ; "
http://arxiv.org/abs/1012.4290,Bit recycling for scaling random number generators,"  Many Random Number Generators (RNG) are available nowadays; they are divided in two categories, hardware RNG, that provide ""true"" random numbers, and algorithmic RNG, that generate pseudo random numbers (PRNG). Both types usually generate random numbers $(X_n)$ as independent uniform samples in a range $0,\cdots,2^{b-1}$, with $b = 8, 16, 32$ or $b = 64$. In applications, it is instead sometimes desirable to draw random numbers as independent uniform samples $(Y_n)$ in a range $1, \cdots, M$, where moreover M may change between drawings. Transforming the sequence $(X_n)$ to $(Y_n)$ is sometimes known as scaling. We discuss different methods for scaling the RNG, both in term of mathematical efficiency and of computational speed. ",Computer Science - Information Theory ; Mathematics - Numerical Analysis ; Mathematics - Probability ; ,"Mennucci, Andrea C. G. ; "
http://arxiv.org/abs/1101.1169,Almost Settling the Hardness of Noncommutative Determinant,"  In this paper, we study the complexity of computing the determinant of a matrix over a non-commutative algebra. In particular, we ask the question, ""over which algebras, is the determinant easier to compute than the permanent?"" Towards resolving this question, we show the following hardness and easiness of noncommutative determinant computation.   * [Hardness] Computing the determinant of an n \times n matrix whose entries are themselves 2 \times 2 matrices over a field is as hard as computing the permanent over the field. This extends the recent result of Arvind and Srinivasan, who proved a similar result which however required the entries to be of linear dimension.   * [Easiness] Determinant of an n \times n matrix whose entries are themselves d \times d upper triangular matrices can be computed in poly(n^d) time.   Combining the above with the decomposition theorem of finite dimensional algebras (in particular exploiting the simple structure of 2 \times 2 matrix algebras), we can extend the above hardness and easiness statements to more general algebras as follows. Let A be a finite dimensional algebra over a finite field with radical R(A).   * [Hardness] If the quotient A/R(A) is non-commutative, then computing the determinant over the algebra A is as hard as computing the permanent.   * [Easiness] If the quotient A/R(A) is commutative and furthermore, R(A) has nilpotency index d (i.e., the smallest d such that R(A)d = 0), then there exists a poly(n^d)-time algorithm that computes determinants over the algebra A.   In particular, for any constant dimensional algebra A over a finite field, since the nilpotency index of R(A) is at most a constant, we have the following dichotomy theorem: if A/R(A) is commutative, then efficient determinant computation is feasible and otherwise determinant is as hard as permanent. ",Computer Science - Computational Complexity ; ,"Chien, Steve ; Harsha, Prahladh ; Sinclair, Alistair ; Srinivasan, Srikanth ; "
http://arxiv.org/abs/1101.1477,Asynchronous Code-Division Random Access Using Convex Optimization,"  Many applications in cellular systems and sensor networks involve a random subset of a large number of users asynchronously reporting activity to a base station. This paper examines the problem of multiuser detection (MUD) in random access channels for such applications. Traditional orthogonal signaling ignores the random nature of user activity in this problem and limits the total number of users to be on the order of the number of signal space dimensions. Contention-based schemes, on the other hand, suffer from delays caused by colliding transmissions and the hidden node problem. In contrast, this paper presents a novel pairing of an asynchronous non-orthogonal code-division random access scheme with a convex optimization-based MUD algorithm that overcomes the issues associated with orthogonal signaling and contention-based methods. Two key distinguishing features of the proposed MUD algorithm are that it does not require knowledge of the delay or channel state information of every user and it has polynomial-time computational complexity. The main analytical contribution of this paper is the relationship between the performance of the proposed MUD algorithm in the presence of arbitrary or random delays and two simple metrics of the set of user codewords. The study of these metrics is then focused on two specific sets of codewords, random binary codewords and specially constructed algebraic codewords, for asynchronous random access. The ensuing analysis confirms that the proposed scheme together with either of these two codeword sets significantly outperforms the orthogonal signaling-based random access in terms of the total number of users in the system. ",Computer Science - Information Theory ; ,"Applebaum, Lorne ; Bajwa, Waheed U. ; Duarte, Marco F. ; Calderbank, Robert ; "
http://arxiv.org/abs/1101.1640,Restarting Automata with Auxiliary Symbols and Small Lookahead,"  We present a study on lookahead hierarchies for restarting automata with auxiliary symbols and small lookahead. In particular, we show that there are just two different classes of languages recognised RRWW automata, through the restriction of lookahead size. We also show that the respective (left-) monotone restarting automaton models characterise the context-free languages and that the respective right-left-monotone restarting automata characterise the linear languages both with just lookahead length 2. ",Computer Science - Formal Languages and Automata Theory ; ,"Schluter, Natalie ; "
http://arxiv.org/abs/1101.1934,Bit-wise Unequal Error Protection for Variable Length Block Codes with   Feedback,"  The bit-wise unequal error protection problem, for the case when the number of groups of bits $\ell$ is fixed, is considered for variable length block codes with feedback. An encoding scheme based on fixed length block codes with erasures is used to establish inner bounds to the achievable performance for finite expected decoding time. A new technique for bounding the performance of variable length block codes is used to establish outer bounds to the performance for a given expected decoding time. The inner and the outer bounds match one another asymptotically and characterize the achievable region of rate-exponent vectors, completely. The single message message-wise unequal error protection problem for variable length block codes with feedback is also solved as a necessary step on the way. ",Computer Science - Information Theory ; ,"Nakiboglu, Baris ; Gorantla, Siva K. ; Zheng, Lizhong ; Coleman, Todd P. ; "
http://arxiv.org/abs/1101.5460,A Human-Centric Approach to Group-Based Context-Awareness,"  The emerging need for qualitative approaches in context-aware information processing calls for proper modeling of context information and efficient handling of its inherent uncertainty resulted from human interpretation and usage. Many of the current approaches to context-awareness either lack a solid theoretical basis for modeling or ignore important requirements such as modularity, high-order uncertainty management and group-based context-awareness. Therefore, their real-world application and extendability remains limited. In this paper, we present f-Context as a service-based context-awareness framework, based on language-action perspective (LAP) theory for modeling. Then we identify some of the complex, informational parts of context which contain high-order uncertainties due to differences between members of the group in defining them. An agent-based perceptual computer architecture is proposed for implementing f-Context that uses computing with words (CWW) for handling uncertainty. The feasibility of f-Context is analyzed using a realistic scenario involving a group of mobile users. We believe that the proposed approach can open the door to future research on context-awareness by offering a theoretical foundation based on human communication, and a service-based layered architecture which exploits CWW for context-aware, group-based and platform-independent access to information systems. ",Computer Science - Artificial Intelligence ; Computer Science - Human-Computer Interaction ; ,"Ghadiri, Nasser ; Baraani-Dastjerdi, Ahmad ; Ghasem-Aghaee, Nasser ; Nematbakhsh, Mohammad A. ; "
http://arxiv.org/abs/1102.0969,On the Complexity of Newman's Community Finding Approach for Biological   and Social Networks,"  Given a graph of interactions, a module (also called a community or cluster) is a subset of nodes whose fitness is a function of the statistical significance of the pairwise interactions of nodes in the module. The topic of this paper is a model-based community finding approach, commonly referred to as modularity clustering, that was originally proposed by Newman and has subsequently been extremely popular in practice. Various heuristic methods are currently employed for finding the optimal solution. However, the exact computational complexity of this approach is still largely unknown.   To this end, we initiate a systematic study of the computational complexity of modularity clustering. Due to the specific quadratic nature of the modularity function, it is necessary to study its value on sparse graphs and dense graphs separately. Our main results include a (1+\eps)-inapproximability for dense graphs and a logarithmic approximation for sparse graphs. We make use of several combinatorial properties of modularity to get these results. These are the first non-trivial approximability results beyond the previously known NP-hardness results. ","Physics - Physics and Society ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Computer Science - Social and Information Networks ; 68Q25, 68R01, 05C85 ; F.2.2 ; G.2.2 ; ","DasGupta, Bhaskar ; Desai, Devendra ; "
http://arxiv.org/abs/1102.4802,A generalization of heterochromatic graphs,"  In 2006, Suzuki, and Akbari & Alipour independently presented a necessary and sufficient condition for edge-colored graphs to have a heterochromatic spanning tree, where a heterochromatic spanning tree is a spanning tree whose edges have distinct colors. In this paper, we propose $f$-chromatic graphs as a generalization of heterochromatic graphs. An edge-colored graph is $f$-chromatic if each color $c$ appears on at most $f(c)$ edges. We also present a necessary and sufficient condition for edge-colored graphs to have an $f$-chromatic spanning forest with exactly $m$ components. Moreover, using this criterion, we show that a $g$-chromatic graph $G$ of order $n$ with $|E(G)|>\binom{n-m}{2}$ has an $f$-chromatic spanning forest with exactly $m$ ($1 \le m \le n-1$) components if $g(c) \le \frac{|E(G)|}{n-m}f(c)$ for any color $c$. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C05, 05C15 ; ","Suzuki, Kazuhiro ; "
http://arxiv.org/abs/1103.1091,A generalization of Hopcroft-Karp algorithm for semi-matchings and   covers in bipartite graphs (Maximum semi-matching problem in bipartite   graphs),"  An $(f,g)$-semi-matching in a bipartite graph $G=(U \cup V,E)$ is a set of edges $M \subseteq E$ such that each vertex $u\in U$ is incident with at most $f(u)$ edges of $M$, and each vertex $v\in V$ is incident with at most $g(v)$ edges of $M$. In this paper we give an algorithm that for a graph with $n$ vertices and $m$ edges, $n\le m$, constructs a maximum $(f,g)$-semi-matching in running time $O(m\cdot \min (\sqrt{\sum_{u\in U}f(u)}, \sqrt{\sum_{v\in V}g(v)}))$. Using the reduction of [5], our result on maximum $(f,g)$-semi-matching problem directly implies an algorithm for the optimal semi-matching problem with running time $O(\sqrt{n}m \log n)$. ",Computer Science - Data Structures and Algorithms ; ,"Katrenic, Ján ; Semanisin, Gabriel ; "
http://arxiv.org/abs/1103.1951,Constructive proof of the existence of equilibrium in competitive   economy with sequentially locally non-constant excess demand functions,"  We present a constructive proof of the existence of an equilibrium in a competitive economy with sequentially locally non-constant excess demand functions. And we will show that the existence of such an equilibrium implies Sperner's lemma. Since the existence of an equilibrium is derived from the existence an approximate fixed point of uniformly continuous functions, which is derived from Sperner's lemma, the existence of an equilibrium in a competitive economy with sequentially locally non-constant excess demand functions is equivalent to Sperner's lemma. ",Mathematics - Logic ; Computer Science - Computer Science and Game Theory ; ,"Tanaka, Yasuhito ; "
http://arxiv.org/abs/1103.1980,Constructive proof of the existence of Nash Equilibrium in a finite   strategic game with sequentially locally non-constant payoff functions by   Sperner's lemma,  Using Sperner's lemma for modified partition of a simplex we will constructively prove the existence of a Nash equilibrium in a finite strategic game with sequentially locally non-constant payoff functions. We follow the Bishop style constructive mathematics. ,Mathematics - Logic ; Computer Science - Computer Science and Game Theory ; ,"Tanaka, Yasuhito ; "
http://arxiv.org/abs/1104.0471,Quantum Bayesian implementation,"  Bayesian implementation concerns decision making problems when agents have incomplete information. This paper proposes that the traditional sufficient conditions for Bayesian implementation shall be amended by virtue of a quantum Bayesian mechanism. In addition, by using an algorithmic Bayesian mechanism, this amendment holds in the macro world. ","Physics - Data Analysis, Statistics and Probability ; Computer Science - Computer Science and Game Theory ; ","Wu, Haoyang ; "
http://arxiv.org/abs/1104.0746,"Quantifier Elimination over Finite Fields Using Gr\""obner Bases","  We give an algebraic quantifier elimination algorithm for the first-order theory over any given finite field using Gr\""obner basis methods. The algorithm relies on the strong Nullstellensatz and properties of elimination ideals over finite fields. We analyze the theoretical complexity of the algorithm and show its application in the formal analysis of a biological controller model. ",Computer Science - Symbolic Computation ; Computer Science - Logic in Computer Science ; ,"Gao, Sicun ; Platzer, André ; Clarke, Edmund M. ; "
http://arxiv.org/abs/1104.2373,Hybrid Deterministic-Stochastic Methods for Data Fitting,"  Many structured data-fitting applications require the solution of an optimization problem involving a sum over a potentially large number of measurements. Incremental gradient algorithms offer inexpensive iterations by sampling a subset of the terms in the sum. These methods can make great progress initially, but often slow as they approach a solution. In contrast, full-gradient methods achieve steady convergence at the expense of evaluating the full objective and gradient on each iteration. We explore hybrid methods that exhibit the benefits of both approaches. Rate-of-convergence analysis shows that by controlling the sample size in an incremental gradient algorithm, it is possible to maintain the steady convergence rates of full-gradient methods. We detail a practical quasi-Newton implementation based on this approach. Numerical experiments illustrate its potential benefits. ",Computer Science - Numerical Analysis ; Computer Science - Systems and Control ; Mathematics - Optimization and Control ; Statistics - Machine Learning ; ,"Friedlander, Michael P. ; Schmidt, Mark ; "
http://arxiv.org/abs/1104.4465,Randomness and Differentiability,"  We characterize some major algorithmic randomness notions via differentiability of effective functions.   (1) As the main result we show that a real number z in [0,1] is computably random if and only if each nondecreasing computable function [0,1]->R is differentiable at z.   (2) We prove that a real number z in [0,1] is weakly 2-random if and only if each almost everywhere differentiable computable function [0,1]->R is differentiable at z.   (3) Recasting in classical language results dating from 1975 of the constructivist Demuth, we show that a real z is ML random if and only if every computable function of bounded variation is differentiable at z, and similarly for absolutely continuous functions.   We also use our analytic methods to show that computable randomness of a real is base invariant, and to derive other preservation results for randomness notions. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D32, 03F60, 26A27, 26A48, 26A45 ; ","Brattka, Vasco ; Miller, Joseph S. ; Nies, André ; "
http://arxiv.org/abs/1104.4987,An improved bound on the number of point-surface incidences in three   dimensions,"  We show that $m$ points and $n$ smooth algebraic surfaces of bounded degree in $\mathbb{R}^3$ satisfying suitable nondegeneracy conditions can have at most $O(m^{\frac{2k}{3k-1}}n^{\frac{3k-3}{3k-1}}+m+n)$ incidences, provided that any collection of $k$ points have at most O(1) surfaces passing through all of them, for some $k\geq 3$. In the case where the surfaces are spheres and no three spheres meet in a common circle, this implies there are $O((mn)^{3/4} + m +n)$ point-sphere incidences. This is a slight improvement over the previous bound of $O((mn)^{3/4} \beta(m,n)+ m +n)$ for $\beta(m,n)$ an (explicit) very slowly growing function. We obtain this bound by using the discrete polynomial ham sandwich theorem to cut $\mathbb{R}^3$ into open cells adapted to the set of points, and within each cell of the decomposition we apply a Turan-type theorem to obtain crude control on the number of point-surface incidences. We then perform a second polynomial ham sandwich decomposition on the irreducible components of the variety defined by the first decomposition. As an application, we obtain a new bound on the maximum number of unit distances amongst $m$ points in $\mathbb{R}^3$. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Zahl, Joshua ; "
http://arxiv.org/abs/1104.5286,Doubly Robust Smoothing of Dynamical Processes via Outlier Sparsity   Constraints,"  Coping with outliers contaminating dynamical processes is of major importance in various applications because mismatches from nominal models are not uncommon in practice. In this context, the present paper develops novel fixed-lag and fixed-interval smoothing algorithms that are robust to outliers simultaneously present in the measurements {\it and} in the state dynamics. Outliers are handled through auxiliary unknown variables that are jointly estimated along with the state based on the least-squares criterion that is regularized with the $\ell_1$-norm of the outliers in order to effect sparsity control. The resultant iterative estimators rely on coordinate descent and the alternating direction method of multipliers, are expressed in closed form per iteration, and are provably convergent. Additional attractive features of the novel doubly robust smoother include: i) ability to handle both types of outliers; ii) universality to unknown nominal noise and outlier distributions; iii) flexibility to encompass maximum a posteriori optimal estimators with reliable performance under nominal conditions; and iv) improved performance relative to competing alternatives at comparable complexity, as corroborated via simulated tests. ",Computer Science - Systems and Control ; Mathematics - Optimization and Control ; Statistics - Applications ; ,"Farahmand, Shahrokh ; Giannakis, Georgios B. ; Angelosante, Daniele ; "
http://arxiv.org/abs/1104.5288,Tracking Target Signal Strengths on a Grid using Sparsity,"  Multi-target tracking is mainly challenged by the nonlinearity present in the measurement equation, and the difficulty in fast and accurate data association. To overcome these challenges, the present paper introduces a grid-based model in which the state captures target signal strengths on a known spatial grid (TSSG). This model leads to \emph{linear} state and measurement equations, which bypass data association and can afford state estimation via sparsity-aware Kalman filtering (KF). Leveraging the grid-induced sparsity of the novel model, two types of sparsity-cognizant TSSG-KF trackers are developed: one effects sparsity through $\ell_1$-norm regularization, and the other invokes sparsity as an extra measurement. Iterative extended KF and Gauss-Newton algorithms are developed for reduced-complexity tracking, along with accurate error covariance updates for assessing performance of the resultant sparsity-aware state estimators. Based on TSSG state estimates, more informative target position and track estimates can be obtained in a follow-up step, ensuring that track association and position estimation errors do not propagate back into TSSG state estimates. The novel TSSG trackers do not require knowing the number of targets or their signal strengths, and exhibit considerably lower complexity than the benchmark hidden Markov model filter, especially for a large number of targets. Numerical simulations demonstrate that sparsity-cognizant trackers enjoy improved root mean-square error performance at reduced complexity when compared to their sparsity-agnostic counterparts. ",Computer Science - Systems and Control ; Mathematics - Optimization and Control ; Statistics - Applications ; ,"Farahmand, Shahrokh ; Giannakis, Georgios B. ; Leus, Geert ; Tian, Zhi ; "
http://arxiv.org/abs/1105.1302,A Modified Cross Correlation Algorithm for Reference-free Image   Alignment of Non-Circular Projections in Single-Particle Electron Microscopy,"  In this paper we propose a modified cross correlation method to align images from the same class in single-particle electron microscopy of highly non-spherical structures. In this new method, First we coarsely align projection images, and then re-align the resulting images using the cross correlation (CC) method. The coarse alignment is obtained by matching the centers of mass and the principal axes of the images. The distribution of misalignment in this coarse alignment can be quantified based on the statistical properties of the additive background noise. As a consequence, the search space for re-alignment in the cross correlation method can be reduced to achieve better alignment. In order to overcome problems associated with false peaks in the cross correlations function, we use artificially blurred images for the early stage of the iterative cross correlation method and segment the intermediate class average from every iteration step. These two additional manipulations combined with the reduced search space size in the cross correlation method yield better alignments for low signal-to-noise ratio images than both classical cross correlation and maximum likelihood(ML) methods. ",Quantitative Biology - Quantitative Methods ; Computer Science - Computer Vision and Pattern Recognition ; Mathematics - Numerical Analysis ; ,"Park, Wooram ; Chirikjian, Gregory S. ; "
http://arxiv.org/abs/1106.1445,From Classical to Quantum Shannon Theory,"  The aim of this book is to develop ""from the ground up"" many of the major, exciting, pre- and post-millenium developments in the general area of study known as quantum Shannon theory. As such, we spend a significant amount of time on quantum mechanics for quantum information theory (Part II), we give a careful study of the important unit protocols of teleportation, super-dense coding, and entanglement distribution (Part III), and we develop many of the tools necessary for understanding information transmission or compression (Part IV). Parts V and VI are the culmination of this book, where all of the tools developed come into play for understanding many of the important results in quantum Shannon theory. ",Quantum Physics ; Computer Science - Information Theory ; ,"Wilde, Mark M. ; "
http://arxiv.org/abs/1106.2327,A framework for coupled deformation-diffusion analysis with application   to degradation/healing,"  This paper deals with the formulation and numerical implementation of a fully coupled continuum model for deformation-diffusion in linearized elastic solids. The mathematical model takes into account the effect of the deformation on the diffusion process, and the affect of the transport of an inert chemical species on the deformation of the solid. We then present a robust computational framework for solving the proposed mathematical model, which consists of coupled non-linear partial differential equations. It should be noted that many popular numerical formulations may produce unphysical negative values for the concentration, particularly, when the diffusion process is anisotropic. The violation of the non-negative constraint by these numerical formulations is not mere numerical noise. In the proposed computational framework we employ a novel numerical formulation that will ensure that the concentration of the diffusant be always non-negative, which is one of the main contributions of this paper. Representative numerical examples are presented to show the robustness, convergence, and performance of the proposed computational framework. Another contribution of this paper is to systematically study the affect of transport of the diffusant on the deformation of the solid and vice-versa, and their implication in modeling degradation/healing of materials. We show that the coupled response is both qualitatively and quantitatively different from the uncoupled response. ","Computer Science - Numerical Analysis ; Computer Science - Computational Engineering, Finance, and Science ; Mathematics - Numerical Analysis ; Physics - Computational Physics ; ","Mudunuru, M. K. ; Nakshatrala, K. B. ; "
http://arxiv.org/abs/1106.2441,An f-chromatic spanning forest of edge-colored complete bipartite graphs,"  In 2001, Brualdi and Hollingsworth proved that an edge-colored balanced complete bipartite graph Kn,n with a color set C = {1,2,3,..., 2n-1} has a heterochromatic spanning tree if the number of edges colored with colors in R is more than |R|^2 /4 for any non-empty subset R \subseteq C, where a heterochromatic spanning tree is a spanning tree whose edges have distinct colors, namely, any color appears at most once. In 2010, Suzuki generalized heterochromatic graphs to f-chromatic graphs, where any color c appears at most f(c). Moreover, he presented a necessary and sufficient condition for graphs to have an f-chromatic spanning forest with exactly w components. In this paper, using this necessary and sufficient condition, we generalize the Brualdi-Hollingsworth theorem above. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C05, 05C15 ; ","Suzuki, Kazuhiro ; "
http://arxiv.org/abs/1106.5130,Some Properties of R\'{e}nyi Entropy over Countably Infinite Alphabets,"  In this paper we study certain properties of R\'{e}nyi entropy functionals $H_\alpha(\mathcal{P})$ on the space of probability distributions over $\mathbb{Z}_+$. Primarily, continuity and convergence issues are addressed. Some properties shown parallel those known in the finite alphabet case, while others illustrate a quite different behaviour of R\'enyi entropy in the infinite case. In particular, it is shown that, for any distribution $\mathcal P$ and any $r\in[0,\infty]$, there exists a sequence of distributions $\mathcal{P}_n$ converging to $\mathcal{P}$ with respect to the total variation distance, such that $\lim_{n\to\infty}\lim_{\alpha\to{1+}} H_\alpha(\mathcal{P}_n) = \lim_{\alpha\to{1+}}\lim_{n\to\infty} H_\alpha(\mathcal{P}_n) + r$. ",Computer Science - Information Theory ; 94A17 ; H.1.1 ; ,"Kovačević, Mladen ; Stanojević, Ivan ; Šenk, Vojin ; "
http://arxiv.org/abs/1107.0088,Sparse Sums of Positive Semidefinite Matrices,"  Recently there has been much interest in ""sparsifying"" sums of rank one matrices: modifying the coefficients such that only a few are nonzero, while approximately preserving the matrix that results from the sum. Results of this sort have found applications in many different areas, including sparsifying graphs. In this paper we consider the more general problem of sparsifying sums of positive semidefinite matrices that have arbitrary rank.   We give several algorithms for solving this problem. The first algorithm is based on the method of Batson, Spielman and Srivastava (2009). The second algorithm is based on the matrix multiplicative weights update method of Arora and Kale (2007). We also highlight an interesting connection between these two algorithms.   Our algorithms have numerous applications. We show how they can be used to construct graph sparsifiers with auxiliary constraints, sparsifiers of hypergraphs, and sparse solutions to semidefinite programs. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Computer Science - Numerical Analysis ; Mathematics - Combinatorics ; ,"Silva, Marcel K. de Carli ; Harvey, Nicholas J. A. ; Sato, Cristiane M. ; "
http://arxiv.org/abs/1107.2465,An Efficient Algorithm for Maximum-Entropy Extension of Block-Circulant   Covariance Matrices,"  This paper deals with maximum entropy completion of partially specified block-circulant matrices. Since positive definite symmetric circulants happen to be covariance matrices of stationary periodic processes, in particular of stationary reciprocal processes, this problem has applications in signal processing, in particular to image modeling. In fact it is strictly related to maximum likelihood estimation of bilateral AR-type representations of acausal signals subject to certain conditional independence constraints. The maximum entropy completion problem for block-circulant matrices has recently been solved by the authors, although leaving open the problem of an efficient computation of the solution. In this paper, we provide an effcient algorithm for computing its solution which compares very favourably with existing algorithms designed for positive definite matrix extension problems. The proposed algorithm benefits from the analysis of the relationship between our problem and the band-extension problem for block-Toeplitz matrices also developed in this paper. ",Mathematics - Optimization and Control ; Computer Science - Information Theory ; Computer Science - Systems and Control ; ,"Carli, Francesca P. ; Ferrante, Augusto ; Pavon, Michele ; Picci, Giorgio ; "
http://arxiv.org/abs/1108.1915,Noise effects in the quantum search algorithm from the computational   complexity point of view,  We analyse the resilience of the quantum search algorithm in the presence of quantum noise modelled as trace preserving completely positive maps. We study the influence of noise on computational complexity of the quantum search algorithm. We show that only for small amounts of noise the quantum search algorithm is still more efficient than any classical algorithm. ,Quantum Physics ; Computer Science - Computational Complexity ; ,"Gawron, Piotr ; Klamka, Jerzy ; Winiarczyk, Ryszard ; "
http://arxiv.org/abs/1109.0345,Planar and Poly-Arc Lombardi Drawings,"  In Lombardi drawings of graphs, edges are represented as circular arcs, and the edges incident on vertices have perfect angular resolution. However, not every graph has a Lombardi drawing, and not every planar graph has a planar Lombardi drawing. We introduce k-Lombardi drawings, in which each edge may be drawn with k circular arcs, noting that every graph has a smooth 2-Lombardi drawing. We show that every planar graph has a smooth planar 3-Lombardi drawing and further investigate topics connecting planarity and Lombardi drawings. ","Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; 05C10, 68R10 ; G.2.2 ; F.2.2 ; ","Duncan, Christian A. ; Eppstein, David ; Goodrich, Michael T. ; Kobourov, Stephen G. ; Löffler, Maarten ; "
http://arxiv.org/abs/1109.2162,The Complexity of the Empire Colouring Problem,"  We investigate the computational complexity of the empire colouring problem (as defined by Percy Heawood in 1890) for maps containing empires formed by exactly $r > 1$ countries each. We prove that the problem can be solved in polynomial time using $s$ colours on maps whose underlying adjacency graph has no induced subgraph of average degree larger than $s/r$. However, if $s \geq 3$, the problem is NP-hard even if the graph is a forest of paths of arbitrary lengths (for any $r \geq 2$, provided $s < 2r - \sqrt(2r + 1/4+ 3/2)$. Furthermore we obtain a complete characterization of the problem's complexity for the case when the input graph is a tree, whereas our result for arbitrary planar graphs fall just short of a similar dichotomy. Specifically, we prove that the empire colouring problem is NP-hard for trees, for any $r \geq 2$, if $3 \leq s \leq 2r-1$ (and polynomial time solvable otherwise). For arbitrary planar graphs we prove NP-hardness if $s<7$ for $r=2$, and $s < 6r-3$, for $r \geq 3$. The result for planar graphs also proves the NP-hardness of colouring with less than 7 colours graphs of thickness two and less than $6r-3$ colours graphs of thickness $r \geq 3$. ",Computer Science - Computational Complexity ; ,"McGrae, Andrew R. A. ; Zito, Michele ; "
http://arxiv.org/abs/1109.2984,A Statistically Modelling Method for Performance Limits in Sensor   Localization,"  In this paper, we study performance limits of sensor localization from a novel perspective. Specifically, we consider the Cramer-Rao Lower Bound (CRLB) in single-hop sensor localization using measurements from received signal strength (RSS), time of arrival (TOA) and bearing, respectively, but differently from the existing work, we statistically analyze the trace of the associated CRLB matrix (i.e. as a scalar metric for performance limits of sensor localization) by assuming anchor locations are random. By the Central Limit Theorems for $U$-statistics, we show that as the number of the anchors increases, this scalar metric is asymptotically normal in the RSS/bearing case, and converges to a random variable which is an affine transformation of a chi-square random variable of degree 2 in the TOA case. Moreover, we provide formulas quantitatively describing the relationship among the mean and standard deviation of the scalar metric, the number of the anchors, the parameters of communication channels, the noise statistics in measurements and the spatial distribution of the anchors. These formulas, though asymptotic in the number of the anchors, in many cases turn out to be remarkably accurate in predicting performance limits, even if the number is small. Simulations are carried out to confirm our results. ",Computer Science - Systems and Control ; Mathematics - Optimization and Control ; ,"Huang, Baoqi ; Li, Tao ; Anderson, Brian D. O. ; Yu, Changbin ; "
http://arxiv.org/abs/1110.0685,Energy Aware Scheduling for Weighted Completion Time and Weighted   Tardiness,"  The ever increasing adoption of mobile devices with limited energy storage capacity, on the one hand, and more awareness of the environmental impact of massive data centres and server pools, on the other hand, have both led to an increased interest in energy management algorithms.   The main contribution of this paper is to present several new constant factor approximation algorithms for energy aware scheduling problems where the objective is to minimize weighted completion time plus the cost of the energy consumed, in the one machine non-preemptive setting, while allowing release dates and deadlines.Unlike previous known algorithms these new algorithms can handle general job-dependent energy cost functions, extending the application of these algorithms to settings outside the typical CPU-energy one. These new settings include problems where in addition, or instead, of energy costs we also have maintenance costs, wear and tear, replacement costs, etc., which in general depend on the speed at which the machine runs but also depend on the types of jobs processed. Our algorithms also extend to approximating weighted tardiness plus energy cost, an inherently more difficult problem that has not been addressed in the literature. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Carrasco, Rodrigo A. ; Iyengar, Garud ; Stein, Cliff ; "
http://arxiv.org/abs/1110.0895,Robust inversion via semistochastic dimensionality reduction,"  We consider a class of inverse problems where it is possible to aggregate the results of multiple experiments. This class includes problems where the forward model is the solution operator to linear ODEs or PDEs. The tremendous size of such problems motivates dimensionality reduction techniques based on randomly mixing experiments. These techniques break down, however, when robust data-fitting formulations are used, which are essential in cases of missing data, unusually large errors, and systematic features in the data unexplained by the forward model. We survey robust methods within a statistical framework, and propose a semistochastic optimization approach that allows dimensionality reduction. The efficacy of the methods are demonstrated for a large-scale seismic inverse problem using the robust Student's t-distribution, where a useful synthetic velocity model is recovered in the extreme scenario of 60% data missing at random. The semistochastic approach achieves this recovery using 20% of the effort required by a direct robust approach. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Numerical Analysis ; ","Aravkin, Aleksandr ; Friedlander, Michael P. ; van Leeuwen, Tristan ; "
http://arxiv.org/abs/1110.2053,"Steps Towards a Theory of Visual Information: Active Perception,   Signal-to-Symbol Conversion and the Interplay Between Sensing and Control","  This manuscript describes the elements of a theory of information tailored to control and decision tasks and specifically to visual data. The concept of Actionable Information is described, that relates to a notion of information championed by J. Gibson, and a notion of ""complete information"" that relates to the minimal sufficient statistics of a complete representation. It is shown that the ""actionable information gap"" between the two can be reduced by exercising control on the sensing process. Thus, senging, control and information are inextricably tied. This has consequences in the so-called ""signal-to-symbol barrier"" problem, as well as in the analysis and design of active sensing systems. It has ramifications in vision-based control, navigation, 3-D reconstruction and rendering, as well as detection, localization, recognition and categorization of objects and scenes in live video.   This manuscript has been developed from a set of lecture notes for a summer course at the First International Computer Vision Summer School (ICVSS) in Scicli, Italy, in July of 2008. They were later expanded and amended for subsequent lectures in the same School in July 2009. Starting on November 1, 2009, they were further expanded for a special topics course, CS269, taught at UCLA in the Spring term of 2010. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Soatto, Stefano ; "
http://arxiv.org/abs/1111.0235,New Methods for Handling Singular Sample Covariance Matrices,"  The estimation of a covariance matrix from an insufficient amount of data is one of the most common problems in fields as diverse as multivariate statistics, wireless communications, signal processing, biology, learning theory and finance. In a joint work of Marzetta, Tucci and Simon, a new approach to handle singular covariance matrices was suggested. The main idea was to use dimensionality reduction in conjunction with an average over the Stiefel manifold. In this paper we continue with this research and we consider some new approaches to solve this problem. One of the methods is called the Ewens estimator and uses a randomization of the sample covariance matrix over all the permutation matrices with respect to the Ewens measure. The techniques used to attack this problem are broad and run from random matrix theory to combinatorics. ","Mathematics - Probability ; Computer Science - Information Theory ; Mathematics - Statistics Theory ; 15B52, 60B20 ; ","Tucci, Gabriel H. ; Wang, Ke ; "
http://arxiv.org/abs/1111.0284,A topological interpretation of the walk distances,"  The walk distances in graphs have no direct interpretation in terms of walk weights, since they are introduced via the \emph{logarithms} of walk weights. Only in the limiting cases where the logarithms vanish such representations follow straightforwardly. The interpretation proposed in this paper rests on the identity $\ln\det B=\tr\ln B$ applied to the cofactors of the matrix $I-tA,$ where $A$ is the weighted adjacency matrix of a weighted multigraph and $t$ is a sufficiently small positive parameter. In addition, this interpretation is based on the power series expansion of the logarithm of a matrix. Kasteleyn (1967) was probably the first to apply the foregoing approach to expanding the determinant of $I-A$. We show that using a certain linear transformation the same approach can be extended to the cofactors of $I-tA,$ which provides a topological interpretation of the walk distances. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Computer Science - Social and Information Networks ; Mathematics - Metric Geometry ; 05C12, 05C50, 51K05, 15A09, 15A15 ; ","Chebotarev, Pavel ; Deza, Michel ; "
http://arxiv.org/abs/1111.2480,The Distance Function on a Computable Graph,"  We apply the techniques of computable model theory to the distance function of a graph. This task leads us to adapt the definitions of several truth-table reducibilities so that they apply to functions as well as to sets, and we prove assorted theorems about the new reducibilities and about functions which have nonincreasing computable approximations. Finally, we show that the spectrum of the distance function can consist of an arbitrary single btt-degree which is approximable from above, or of all such btt-degrees at once, or of the bT-degrees of exactly those functions approximable from above in at most n steps. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Calvert, Wesley ; Miller, Russell ; Reimann, Jennifer Chubb ; "
http://arxiv.org/abs/1111.3048,On a Connection Between Small Set Expansions and Modularity Clustering   in Social Networks,"  In this paper we explore a connection between two seemingly different problems from two different domains: the small-set expansion problem studied in unique games conjecture, and a popular community finding approach for social networks known as the modularity clustering approach. We show that a sub-exponential time algorithm for the small-set expansion problem leads to a sub-exponential time constant factor approximation for some hard input instances of the modularity clustering problem. ","Computer Science - Social and Information Networks ; Computer Science - Computational Complexity ; Physics - Physics and Society ; 68Q25, 68W25 ; F.2.2 ; J.4 ; ","DasGupta, Bhaskar ; Desai, Devendra ; "
http://arxiv.org/abs/1111.4662,Traffic distributions and independence: permutation invariant random   matrices and the three notions of independence,"  Voiculescu's notion of asymptotic free independence is known for a large class of random matrices including independent unitary invariant matrices. This notion is extended for independent random matrices invariant in law by conjugation by permutation matrices. This fact leads naturally to an extension of free probability, formalized under the notions of traffic probability. We first establish this construction for random matrices. We define the traffic distribution of random matrices, which is richer than the *-distribution of free probability. The knowledge of the individual traffic distributions of independent permutation invariant families of matrices is sufficient to compute the limiting distribution of the join family. Under a factorization assumption, we call traffic independence the asymptotic rule that plays the role of independence with respect to traffic distributions. Wigner matrices, Haar unitary matrices and uniform permutation matrices converge in traffic distributions, a fact which yields new results on the limiting *-distributions of several matrices we can construct from them. Then we define the abstract traffic spaces as non commutative probability spaces with more structure. We prove that at an algebraic level, traffic independence in some sense unifies the three canonical notions of tensor, free and Boolean independence. A central limiting theorem is stated in this context, interpolating between the tensor, free and Boolean central limit theorems. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; Mathematics - Operator Algebras ; ,"Male, Camille ; "
http://arxiv.org/abs/1111.7013,A Taxation Policy for Maximizing Social Welfare in Networks: A General   Framework,"  We present a simple tatonnement process based on a decomposition method which is simple to implement and achieves the maximal social welfare, under the assumption that the utility function of each [price-taking] individual will be his own private information and need not be known by the designer. At each iteration, very little information needs to be exchanged among the individuals in order to achieve the optimal allocation. Furthermore, the given tatonnement process is always balanced at equilibrium and off equilibrium. ",Mathematics - Optimization and Control ; Computer Science - Computer Science and Game Theory ; ,"Kakhbod, Ali ; Koo, Joseph ; Teneketzis, Demosthenis ; "
http://arxiv.org/abs/1112.0857,I/O efficient bisimulation partitioning on very large directed acyclic   graphs,"  In this paper we introduce the first efficient external-memory algorithm to compute the bisimilarity equivalence classes of a directed acyclic graph (DAG). DAGs are commonly used to model data in a wide variety of practical applications, ranging from XML documents and data provenance models, to web taxonomies and scientific workflows. In the study of efficient reasoning over massive graphs, the notion of node bisimilarity plays a central role. For example, grouping together bisimilar nodes in an XML data set is the first step in many sophisticated approaches to building indexing data structures for efficient XPath query evaluation. To date, however, only internal-memory bisimulation algorithms have been investigated. As the size of real-world DAG data sets often exceeds available main memory, storage in external memory becomes necessary. Hence, there is a practical need for an efficient approach to computing bisimulation in external memory.   Our general algorithm has a worst-case IO-complexity of O(Sort(|N| + |E|)), where |N| and |E| are the numbers of nodes and edges, resp., in the data graph and Sort(n) is the number of accesses to external memory needed to sort an input of size n. We also study specializations of this algorithm to common variations of bisimulation for tree-structured XML data sets. We empirically verify efficient performance of the algorithms on graphs and XML documents having billions of nodes and edges, and find that the algorithms can process such graphs efficiently even when very limited internal memory is available. The proposed algorithms are simple enough for practical implementation and use, and open the door for further study of external-memory bisimulation algorithms. To this end, the full open-source C++ implementation has been made freely available. ",Computer Science - Data Structures and Algorithms ; Computer Science - Databases ; ,"Hellings, Jelle ; Fletcher, George H. L. ; Haverkort, Herman ; "
http://arxiv.org/abs/1112.2275,On Problems as Hard as CNFSAT,"  The field of exact exponential time algorithms for NP-hard problems has thrived over the last decade. While exhaustive search remains asymptotically the fastest known algorithm for some basic problems, difficult and non-trivial exponential time algorithms have been found for a myriad of problems, including Graph Coloring, Hamiltonian Path, Dominating Set and 3-CNF-Sat. In some instances, improving these algorithms further seems to be out of reach. The CNF-Sat problem is the canonical example of a problem for which the trivial exhaustive search algorithm runs in time O(2^n), where n is the number of variables in the input formula. While there exist non-trivial algorithms for CNF-Sat that run in time o(2^n), no algorithm was able to improve the growth rate 2 to a smaller constant, and hence it is natural to conjecture that 2 is the optimal growth rate. The strong exponential time hypothesis (SETH) by Impagliazzo and Paturi [JCSS 2001] goes a little bit further and asserts that, for every epsilon<1, there is a (large) integer k such that that k-CNF-Sat cannot be computed in time 2^{epsilon n}.   In this paper, we show that, for every epsilon < 1, the problems Hitting Set, Set Splitting, and NAE-Sat cannot be computed in time O(2^{epsilon n}) unless SETH fails. Here n is the number of elements or variables in the input. For these problems, we actually get an equivalence to SETH in a certain sense. We conjecture that SETH implies a similar statement for Set Cover, and prove that, under this assumption, the fastest known algorithms for Steinter Tree, Connected Vertex Cover, Set Partitioning, and the pseudo-polynomial time algorithm for Subset Sum cannot be significantly improved. Finally, we justify our assumption about the hardness of Set Cover by showing that the parity of the number of set covers cannot be computed in time O(2^{epsilon n}) for any epsilon<1 unless SETH fails. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; ,"Cygan, Marek ; Dell, Holger ; Lokshtanov, Daniel ; Marx, Daniel ; Nederlof, Jesper ; Okamoto, Yoshio ; Paturi, Ramamohan ; Saurabh, Saket ; Wahlstrom, Magnus ; "
http://arxiv.org/abs/1201.0490,Scikit-learn: Machine Learning in Python,"  Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org. ",Computer Science - Machine Learning ; Computer Science - Mathematical Software ; ,"Pedregosa, Fabian ; Varoquaux, Gaël ; Gramfort, Alexandre ; Michel, Vincent ; Thirion, Bertrand ; Grisel, Olivier ; Blondel, Mathieu ; Müller, Andreas ; Nothman, Joel ; Louppe, Gilles ; Prettenhofer, Peter ; Weiss, Ron ; Dubourg, Vincent ; Vanderplas, Jake ; Passos, Alexandre ; Cournapeau, David ; Brucher, Matthieu ; Perrot, Matthieu ; Duchesnay, Édouard ; "
http://arxiv.org/abs/1201.2845,Competition through selective inhibitory synchrony,"  Models of cortical neuronal circuits commonly depend on inhibitory feedback to control gain, provide signal normalization, and to selectively amplify signals using winner-take-all (WTA) dynamics. Such models generally assume that excitatory and inhibitory neurons are able to interact easily, because their axons and dendrites are co-localized in the same small volume. However, quantitative neuroanatomical studies of the dimensions of axonal and dendritic trees of neurons in the neocortex show that this co-localization assumption is not valid. In this paper we describe a simple modification to the WTA circuit design that permits the effects of distributed inhibitory neurons to be coupled through synchronization, and so allows a single WTA to be distributed widely in cortical space, well beyond the arborization of any single inhibitory neuron, and even across different cortical areas. We prove by non-linear contraction analysis, and demonstrate by simulation that distributed WTA sub-systems combined by such inhibitory synchrony are inherently stable. We show analytically that synchronization is substantially faster than winner selection. This circuit mechanism allows networks of independent WTAs to fully or partially compete with each other. ",Quantitative Biology - Neurons and Cognition ; Computer Science - Neural and Evolutionary Computing ; ,"Rutishauser, Ueli ; Slotine, Jean-Jacques ; Douglas, Rodney J. ; "
http://arxiv.org/abs/1201.3416,Verifying Real-time Commit Protocols Using Dense-time Model Checking   Technology,"  The timed-based automata model, introduced by Alur and Dill, provides a useful formalism for describing real-time systems. Over the last two decades, several dense-time model checking tools have been developed based on that model. The paper considers the verification of real-time distributed commit protocols using dense-time model checking technology. More precisely, we model and verify the well-known timed two phase commit protocol in three different state-of-the-art real-time model checkers: UPPAAL, Rabbit, and RED, and compare the results. ",Computer Science - Software Engineering ; ,"Al-Bataineh, Omar I. ; Reynolds, Mark ; "
http://arxiv.org/abs/1201.5921,An iterative algorithm for parametrization of shortest length shift   registers over finite rings,"  The construction of shortest feedback shift registers for a finite sequence S_1,...,S_N is considered over the finite ring Z_{p^r}. A novel algorithm is presented that yields a parametrization of all shortest feedback shift registers for the sequence of numbers S_1,...,S_N, thus solving an open problem in the literature. The algorithm iteratively processes each number, starting with S_1, and constructs at each step a particular type of minimal Gr\""obner basis. The construction involves a simple update rule at each step which leads to computational efficiency. It is shown that the algorithm simultaneously computes a similar parametrization for the reciprocal sequence S_N,...,S_1. ","Computer Science - Information Theory ; Computer Science - Symbolic Computation ; 94A55, 11T71 ; ","Kuijper, M. ; Pinto, R. ; "
http://arxiv.org/abs/1201.6371,Standard decomposition of expansive ergodically supported dynamics,"  In this work we introduce the notion of weak quasigroups, that are quasigroup operations defined almost everywhere on some set. Then we prove that the topological entropy and the ergodic period of an invertible expansive ergodically supported dynamical system $(X,T)$ with the shadowing property establishes a sufficient criterion for the existence of quasigroup operations defined almost everywhere outside of universally null sets and for which $T$ is an automorphism. Furthermore, we find a decomposition of the dynamics of $T$ in terms of $T$-invariant weak topological subquasigroups. ","Mathematics - Dynamical Systems ; Computer Science - Information Theory ; Mathematics - Group Theory ; 20N05, 17C10, 94A55, 68P30 ; ","Sobottka, Marcelo ; "
http://arxiv.org/abs/1202.3538,Refinement Modal Logic,"  In this paper we present {\em refinement modal logic}. A refinement is like a bisimulation, except that from the three relational requirements only `atoms' and `back' need to be satisfied. Our logic contains a new operator 'all' in addition to the standard modalities 'box' for each agent. The operator 'all' acts as a quantifier over the set of all refinements of a given model. As a variation on a bisimulation quantifier, this refinement operator or refinement quantifier 'all' can be seen as quantifying over a variable not occurring in the formula bound by it. The logic combines the simplicity of multi-agent modal logic with some powers of monadic second-order quantification. We present a sound and complete axiomatization of multi-agent refinement modal logic. We also present an extension of the logic to the modal mu-calculus, and an axiomatization for the single-agent version of this logic. Examples and applications are also discussed: to software verification and design (the set of agents can also be seen as a set of actions), and to dynamic epistemic logic. We further give detailed results on the complexity of satisfiability, and on succinctness. ",Computer Science - Logic in Computer Science ; Computer Science - Artificial Intelligence ; ,"Bozzelli, Laura ; van Ditmarsch, Hans ; French, Tim ; Hales, James ; Pinchinat, Sophie ; "
http://arxiv.org/abs/1202.3733,Lipschitz Parametrization of Probabilistic Graphical Models,"  We show that the log-likelihood of several probabilistic graphical models is Lipschitz continuous with respect to the lp-norm of the parameters. We discuss several implications of Lipschitz parametrization. We present an upper bound of the Kullback-Leibler divergence that allows understanding methods that penalize the lp-norm of differences of parameters as the minimization of that upper bound. The expected log-likelihood is lower bounded by the negative lp-norm, which allows understanding the generalization ability of probabilistic models. The exponential of the negative lp-norm is involved in the lower bound of the Bayes error rate, which shows that it is reasonable to use parameters as features in algorithms that rely on metric spaces (e.g. classification, dimensionality reduction, clustering). Our results do not rely on specific algorithms for learning the structure or parameters. We show preliminary results for activity recognition and temporal segmentation. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Honorio, Jean ; "
http://arxiv.org/abs/1202.4707,A para-model agent for dynamical systems,"  Consider a dynamical system $u \mapsto x, \dot{x} = f_{nl}(x,u)$ where $f_{nl}$ is a nonlinear (convex or nonconvex) function, or a combination of nonlinear functions that can eventually switch. We present, in this preliminary work, a generalization of the standard model-free control, that can either control the dynamical system, given an output reference trajectory, or optimize the dynamical system as a derivative-free optimization based ""extremum-seeking"" procedure. Multiple applications are presented and the robustness of the proposed method is studied in simulation. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Michel, Loïc ; "
http://arxiv.org/abs/1202.4910,Distributed Private Heavy Hitters,"  In this paper, we give efficient algorithms and lower bounds for solving the heavy hitters problem while preserving differential privacy in the fully distributed local model. In this model, there are n parties, each of which possesses a single element from a universe of size N. The heavy hitters problem is to find the identity of the most common element shared amongst the n parties. In the local model, there is no trusted database administrator, and so the algorithm must interact with each of the $n$ parties separately, using a differentially private protocol. We give tight information-theoretic upper and lower bounds on the accuracy to which this problem can be solved in the local model (giving a separation between the local model and the more common centralized model of privacy), as well as computationally efficient algorithms even in the case where the data universe N may be exponentially large. ",Computer Science - Data Structures and Algorithms ; Computer Science - Cryptography and Security ; Computer Science - Databases ; ,"Hsu, Justin ; Khanna, Sanjeev ; Roth, Aaron ; "
http://arxiv.org/abs/1202.4961,Strongly universal string hashing is fast,"  We present fast strongly universal string hashing families: they can process data at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that these families---though they require a large buffer of random numbers---are often faster than popular hash functions with weaker theoretical guarantees. Moreover, conventional wisdom is that hash functions with fewer multiplications are faster. Yet we find that they may fail to be faster due to operation pipelining. We present experimental results on several processors including low-powered processors. Our tests include hash functions designed for processors with the Carry-Less Multiplication (CLMUL) instruction set. We also prove, using accessible proofs, the strong universality of our families. ",Computer Science - Databases ; Computer Science - Data Structures and Algorithms ; ,"Kaser, Owen ; Lemire, Daniel ; "
http://arxiv.org/abs/1203.3341,"A Comparison of the Embedding Method to Multi-Parametric Programming,   Mixed-Integer Programming, Gradient-Descent, and Hybrid Minimum Principle   Based Methods","  In recent years, the embedding approach for solving switched optimal control problems has been developed in a series of papers. However, the embedding approach, which advantageously converts the hybrid optimal control problem to a classical nonlinear optimization, has not been extensively compared to alternative solution approaches. The goal of this paper is thus to compare the embedding approach to multi-parametric programming, mixed-integer programming (e.g., CPLEX), and gradient-descent based methods in the context of five recently published examples: a spring-mass system, moving-target tracking for a mobile robot, two-tank filling, DC-DC boost converter, and skid-steered vehicle. A sixth example, an autonomous switched 11-region linear system, is used to compare a hybrid minimum principle method and traditional numerical programming. For a given performance index for each case, cost and solution times are presented. It is shown that there are numerical advantages of the embedding approach: lower performance index cost (except in some instances when autonomous switches are present), generally faster solution time, and convergence to a solution when other methods may fail. In addition, the embedding method requires no ad hoc assumptions (e.g., predetermined mode sequences) or specialized control models. Theoretical advantages of the embedding approach over the other methods are also described: guaranteed existence of a solution under mild conditions, convexity of the embedded hybrid optimization problem (under the customary conditions on the performance index), solvability with traditional techniques (e.g., sequential quadratic programming) avoiding the combinatorial complexity in the number of modes/discrete variables of mixed-integer programming, applicability to affine nonlinear systems, and no need to explicitly assign discrete/mode variables to autonomous switches. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Meyer, Richard ; Žefran, Miloš ; DeCarlo, Raymond A. ; "
http://arxiv.org/abs/1203.4600,A Szemeredi-Trotter type theorem in $\mathbb{R}^4$,"  We show that $m$ points and $n$ two-dimensional algebraic surfaces in $\mathbb{R}^4$ can have at most $O(m^{\frac{k}{2k-1}}n^{\frac{2k-2}{2k-1}}+m+n)$ incidences, provided that the algebraic surfaces behave like pseudoflats with $k$ degrees of freedom, and that $m\leq n^{\frac{2k+2}{3k}}$. As a special case, we obtain a Szemer\'edi-Trotter type theorem for 2--planes in $\mathbb{R}^4$, provided $m\leq n$ and the planes intersect transversely. As a further special case, we obtain a Szemer\'edi-Trotter type theorem for complex lines in $\mathbb{C}^2$ with no restrictions on $m$ and $n$ (this theorem was originally proved by T\'oth using a different method). As a third special case, we obtain a Szemer\'edi-Trotter type theorem for complex unit circles in $\mathbb{C}^2$. We obtain our results by combining several tools, including a two-level analogue of the discrete polynomial partitioning theorem and the crossing lemma. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Zahl, Joshua ; "
http://arxiv.org/abs/1203.5184,A Universal Model of Commuting Networks,"  We test a recently proposed model of commuting networks on 80 case studies from different regions of the world (Europe and United-States) and with geographic units of different sizes (municipality, county, region). The model takes as input the number of commuters coming in and out of each geographic unit and generates the matrix of commuting flows betwen the geographic units. We show that the single parameter of the model, which rules the compromise between the influence of the distance and job opportunities, follows a universal law that depends only on the average surface of the geographic units. We verified that the law derived from a part of the case studies yields accurate results on other case studies. We also show that our model significantly outperforms the two other approaches proposing a universal commuting model (Balcan et al. (2009); Simini et al. (2012)), particularly when the geographic units are small (e.g. municipalities). ",Mathematics - Statistics Theory ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Lenormand, Maxime ; Huet, Sylvie ; Gargiulo, Floriana ; Deffuant, Guillaume ; "
http://arxiv.org/abs/1203.5188,Semi-Automatically Extracting FAQs to Improve Accessibility of Software   Development Knowledge,"  Frequently asked questions (FAQs) are a popular way to document software development knowledge. As creating such documents is expensive, this paper presents an approach for automatically extracting FAQs from sources of software development discussion, such as mailing lists and Internet forums, by combining techniques of text mining and natural language processing. We apply the approach to popular mailing lists and carry out a survey among software developers to show that it is able to extract high-quality FAQs that may be further improved by experts. ",Computer Science - Software Engineering ; Computer Science - Computation and Language ; Computer Science - Information Retrieval ; ,"Henß, Stefan ; Monperrus, Martin ; Mezini, Mira ; "
http://arxiv.org/abs/1203.5414,"Clique problem, cutting plane proofs and communication complexity","  Motivated by its relation to the length of cutting plane proofs for the Maximum Biclique problem, we consider the following communication game on a given graph G, known to both players. Let K be the maximal number of vertices in a complete bipartite subgraph of G, which is not necessarily an induced subgraph if G is not bipartite. Alice gets a set A of vertices, and Bob gets a disjoint set B of vertices such that |A|+|B|>K. The goal is to find a nonedge of G between A and B. We show that O(\log n) bits of communication are enough for every n-vertex graph. ",Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; ,"Jukna, S. ; "
http://arxiv.org/abs/1203.5706,Effective de Rham Cohomology - The General Case,"  Grothendieck has proved that each class in the de Rham cohomology of a smooth complex affine variety can be represented by a differential form with polynomial coefficients. After having proved a single exponential bound for the degrees of these forms in the case of a hypersurface, here we generalize this result to arbitrary codimension. More precisely, we show that the p-th de Rham cohomology of a smooth affine variety of dimension m and degree D can be represented by differential forms of degree (pD)^{O(pm)}. This result is relevant for the algorithmic computation of the cohomology, but is also motivated by questions in the theory of ordinary differential equations related to the infinitesimal Hilbert 16th problem. ","Mathematics - Algebraic Geometry ; Computer Science - Computational Complexity ; Mathematics - Commutative Algebra ; 14Q20, 14Q15, 68W30, 34C07 ; ","Scheiblechner, Peter ; "
http://arxiv.org/abs/1203.6152,The FO^2 alternation hierarchy is decidable,"  We consider the two-variable fragment FO^2[<] of first-order logic over finite words. Numerous characterizations of this class are known. Th\'erien and Wilke have shown that it is decidable whether a given regular language is definable in FO^2[<]. From a practical point of view, as shown by Weis, FO^2[<] is interesting since its satisfiability problem is in NP. Restricting the number of quantifier alternations yields an infinite hierarchy inside the class of FO^2[<]-definable languages. We show that each level of this hierarchy is decidable. For this purpose, we relate each level of the hierarchy with a decidable variety of finite monoids. Our result implies that there are many different ways of climbing up the FO^2[<]-quantifier alternation hierarchy: deterministic and co-deterministic products, Mal'cev products with definite and reverse definite semigroups, iterated block products with J-trivial monoids, and some inductively defined omega-term identities. A combinatorial tool in the process of ascension is that of condensed rankers, a refinement of the rankers of Weis and Immerman and the turtle programs of Schwentick, Th\'erien, and Vollmer. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; ,"Kufleitner, Manfred ; Weil, Pascal ; "
http://arxiv.org/abs/1203.6286,On the Easiest and Hardest Fitness Functions,"  The hardness of fitness functions is an important research topic in the field of evolutionary computation. In theory, the study can help understanding the ability of evolutionary algorithms. In practice, the study may provide a guideline to the design of benchmarks. The aim of this paper is to answer the following research questions: Given a fitness function class, which functions are the easiest with respect to an evolutionary algorithm? Which are the hardest? How are these functions constructed? The paper provides theoretical answers to these questions. The easiest and hardest fitness functions are constructed for an elitist (1+1) evolutionary algorithm to maximise a class of fitness functions with the same optima. It is demonstrated that the unimodal functions are the easiest and deceptive functions are the hardest in terms of the time-fitness landscape. The paper also reveals that the easiest fitness function to one algorithm may become the hardest to another algorithm, and vice versa. ",Computer Science - Neural and Evolutionary Computing ; ,"He, Jun ; Chen, Tianshi ; Yao, Xin ; "
http://arxiv.org/abs/1203.6566,New Combinatorial Construction Techniques for Low-Density Parity-Check   Codes and Systematic Repeat-Accumulate Codes,"  This paper presents several new construction techniques for low-density parity-check (LDPC) and systematic repeat-accumulate (RA) codes. Based on specific classes of combinatorial designs, the improved code design focuses on high-rate structured codes with constant column weights 3 and higher. The proposed codes are efficiently encodable and exhibit good structural properties. Experimental results on decoding performance with the sum-product algorithm show that the novel codes offer substantial practical application potential, for instance, in high-speed applications in magnetic recording and optical communications channels. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Gruner, Alexander ; Huber, Michael ; "
http://arxiv.org/abs/1204.0480,Deducing Security Goals From Shape Analysis Sentences,"  Guttman presented a model-theoretic approach to establishing security goals in the context of Strand Space theory. In his approach, a run of the Cryptographic Protocol Shapes Analyzer (CPSA) produces models that determine if a goal is satisfied. This paper presents a method for extracting a sentence that completely characterizes a run of CPSA. Logical deduction can then be used to determine if a goal is satisfied. This method has been implemented and is available to all. ",Computer Science - Cryptography and Security ; Computer Science - Logic in Computer Science ; ,"Ramsdell, John D. ; "
http://arxiv.org/abs/1204.0839,A Constrained Random Demodulator for Sub-Nyquist Sampling,"  This paper presents a significant modification to the Random Demodulator (RD) of Tropp et al. for sub-Nyquist sampling of frequency-sparse signals. The modification, termed constrained random demodulator, involves replacing the random waveform, essential to the operation of the RD, with a constrained random waveform that has limits on its switching rate because fast switching waveforms may be hard to generate cleanly. The result is a relaxation on the hardware requirements with a slight, but manageable, decrease in the recovery guarantees. The paper also establishes the importance of properly choosing the statistics of the constrained random waveform. If the power spectrum of the random waveform matches the distribution on the tones of the input signal (i.e., the distribution is proportional to the power spectrum), then recovery of the input signal tones is improved. The theoretical guarantees provided in the paper are validated through extensive numerical simulations and phase transition plots. ",Computer Science - Information Theory ; ,"Harms, Andrew ; Bajwa, Waheed U. ; Calderbank, Robert ; "
http://arxiv.org/abs/1204.1160,Opinion formation in time-varying social networks: The case of the   naming game,"  We study the dynamics of the naming game as an opinion formation model on time-varying social networks. This agent-based model captures the essential features of the agreement dynamics by means of a memory-based negotiation process. Our study focuses on the impact of time-varying properties of the social network of the agents on the naming game dynamics. In particular, we perform a computational exploration of this model using simulations on top of real networks. We investigate the outcomes of the dynamics on two different types of time-varying data - (i) the networks vary on a day-to-day basis and (ii) the networks vary within very short intervals of time (20 seconds). In the first case, we find that networks with strong community structure hinder the system from reaching global agreement; the evolution of the naming game in these networks maintains clusters of coexisting opinions indefinitely leading to metastability. In the second case, we investigate the evolution of the naming game in perfect synchronization with the time evolution of the underlying social network shedding new light on the traditional emergent properties of the game that differ largely from what has been reported in the existing literature. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Maity, Suman Kalyan ; Manoj, T. Venkat ; Mukherjee, Animesh ; "
http://arxiv.org/abs/1204.1846,Approximate Revenue Maximization with Multiple Items,"  Maximizing the revenue from selling _more than one_ good (or item) to a single buyer is a notoriously difficult problem, in stark contrast to the one-good case. For two goods, we show that simple ""one-dimensional"" mechanisms, such as selling the goods separately, _guarantee_ at least 73% of the optimal revenue when the valuations of the two goods are independent and identically distributed, and at least $50\%$ when they are independent. For the case of $k>2$ independent goods, we show that selling them separately guarantees at least a $c/\log^2 k$ fraction of the optimal revenue; and, for independent and identically distributed goods, we show that selling them as one bundle guarantees at least a $c/\log k$ fraction of the optimal revenue. Additional results compare the revenues from the two simple mechanisms of selling the goods separately and bundled, identify situations where bundling is optimal, and extend the analysis to multiple buyers. ",Computer Science - Computer Science and Game Theory ; ,"Hart, Sergiu ; Nisan, Noam ; "
http://arxiv.org/abs/1204.1868,User-based key frame detection in social web video,"  Video search results and suggested videos on web sites are represented with a video thumbnail, which is manually selected by the video up-loader among three randomly generated ones (e.g., YouTube). In contrast, we present a grounded user-based approach for automatically detecting interesting key-frames within a video through aggregated users' replay interactions with the video player. Previous research has focused on content-based systems that have the benefit of analyzing a video without user interactions, but they are monolithic, because the resulting video thumbnails are the same regardless of the user preferences. We constructed a user interest function, which is based on aggregate video replays, and analyzed hundreds of user interactions. We found that the local maximum of the replaying activity stands for the semantics of information rich videos, such as lecture, and how-to. The concept of user-based key-frame detection could be applied to any video on the web, in order to generate a user-based and dynamic video thumbnail in search results. ",Computer Science - Multimedia ; Computer Science - Human-Computer Interaction ; Computer Science - Information Retrieval ; ,"Chorianopoulos, Konstantinos ; "
http://arxiv.org/abs/1204.2606,Privacy via the Johnson-Lindenstrauss Transform,"  Suppose that party A collects private information about its users, where each user's data is represented as a bit vector. Suppose that party B has a proprietary data mining algorithm that requires estimating the distance between users, such as clustering or nearest neighbors. We ask if it is possible for party A to publish some information about each user so that B can estimate the distance between users without being able to infer any private bit of a user. Our method involves projecting each user's representation into a random, lower-dimensional space via a sparse Johnson-Lindenstrauss transform and then adding Gaussian noise to each entry of the lower-dimensional representation. We show that the method preserves differential privacy---where the more privacy is desired, the larger the variance of the Gaussian noise. Further, we show how to approximate the true distances between users via only the lower-dimensional, perturbed data. Finally, we consider other perturbation methods such as randomized response and draw comparisons to sketch-based methods. While the goal of releasing user-specific data to third parties is more broad than preserving distances, this work shows that distance computations with privacy is an achievable goal. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computers and Society ; Computer Science - Databases ; Computer Science - Social and Information Networks ; K.4.1 ; F.2 ; H.3.5 ; G.3 ; I.5.3 ; H.3.3 ; H.2.8 ; E.1 ; G.1.3 ; ,"Kenthapadi, Krishnaram ; Korolova, Aleksandra ; Mironov, Ilya ; Mishra, Nina ; "
http://arxiv.org/abs/1204.2727,The Cost of Perfection for Matchings in Graphs,"  Perfect matchings and maximum weight matchings are two fundamental combinatorial structures. We consider the ratio between the maximum weight of a perfect matching and the maximum weight of a general matching. Motivated by the computer graphics application in triangle meshes, where we seek to convert a triangulation into a quadrangulation by merging pairs of adjacent triangles, we focus mainly on bridgeless cubic graphs. First, we characterize graphs that attain the extreme ratios. Second, we present a lower bound for all bridgeless cubic graphs. Third, we present upper bounds for subclasses of bridgeless cubic graphs, most of which are shown to be tight. Additionally, we present tight bounds for the class of regular bipartite graphs. ",Computer Science - Discrete Mathematics ; ,"Brazil, Emilio Vital ; da Fonseca, Guilherme D. ; de Figueiredo, Celina ; Sasaki, Diana ; "
http://arxiv.org/abs/1204.3850,Simple Agents Learn to Find Their Way: An Introduction on Mapping   Polygons,"  This paper gives an introduction to the problem of mapping simple polygons with autonomous agents. We focus on minimalistic agents that move from vertex to vertex along straight lines inside a polygon, using their sensors to gather local observations at each vertex. Our attention revolves around the question whether a given configuration of sensors and movement capabilities of the agents allows them to capture enough data in order to draw conclusions regarding the global layout of the polygon. In particular, we study the problem of reconstructing the visibility graph of a simple polygon by an agent moving either inside or on the boundary of the polygon. Our aim is to provide insight about the algorithmic challenges faced by an agent trying to map a polygon. We present an overview of techniques for solving this problem with agents that are equipped with simple sensorial capabilities. We illustrate these techniques on examples with sensors that mea- sure angles between lines of sight or identify the previous location. We give an overview over related problems in combinatorial geometry as well as graph exploration. ",Computer Science - Computational Geometry ; ,"Chalopin, Jérémie ; Das, Shantanu ; Disser, Yann ; Mihalák, Matúš ; Widmayer, Peter ; "
http://arxiv.org/abs/1204.6321,Efficient Video Indexing on the Web: A System that Leverages User   Interactions with a Video Player,"  In this paper, we propose a user-based video indexing method, that automatically generates thumbnails of the most important scenes of an online video stream, by analyzing users' interactions with a web video player. As a test bench to verify our idea we have extended the YouTube video player into the VideoSkip system. In addition, VideoSkip uses a web-database (Google Application Engine) to keep a record of some important parameters, such as the timing of basic user actions (play, pause, skip). Moreover, we implemented an algorithm that selects representative thumbnails. Finally, we populated the system with data from an experiment with nine users. We found that the VideoSkip system indexes video content by leveraging implicit users interactions, such as pause and thirty seconds skip. Our early findings point toward improvements of the web video player and its thumbnail generation technique. The VideSkip system could compliment content-based algorithms, in order to achieve efficient video-indexing in difficult videos, such as lectures or sports. ",Computer Science - Multimedia ; Computer Science - Digital Libraries ; Computer Science - Human-Computer Interaction ; Computer Science - Information Retrieval ; ,"Leftheriotis, Ioannis ; Gkonela, Chrysoula ; Chorianopoulos, Konstantinos ; "
http://arxiv.org/abs/1204.6445,A Complete Dichotomy Rises from the Capture of Vanishing Signatures,"  We prove a complexity dichotomy theorem for Holant problems over an arbitrary set of complex-valued symmetric constraint functions F on Boolean variables. This extends and unifies all previous dichotomies for Holant problems on symmetric constraint functions (taking values without a finite modulus). We define and characterize all symmetric vanishing signatures. They turned out to be essential to the complete classification of Holant problems. The dichotomy theorem has an explicit tractability criterion expressible in terms of holographic transformations. A Holant problem defined by a set of constraint functions F is solvable in polynomial time if it satisfies this tractability criterion, and is #P-hard otherwise. The tractability criterion can be intuitively stated as follows: A set F is tractable if (1) every function in F has arity at most two, or (2) F is transformable to an affine type, or (3) F is transformable to a product type, or (4) F is vanishing, combined with the right type of binary functions, or (5) F belongs to a special category of vanishing type Fibonacci gates. The proof of this theorem utilizes many previous dichotomy theorems on Holant problems and Boolean #CSP. Holographic transformations play an indispensable role as both a proof technique and in the statement of the tractability criterion. ",Computer Science - Computational Complexity ; 68Q17 ; F.1.3 ; G.2.1 ; ,"Cai, Jin-Yi ; Guo, Heng ; Williams, Tyson ; "
http://arxiv.org/abs/1205.3576,Dexpler: Converting Android Dalvik Bytecode to Jimple for Static   Analysis with Soot,"  This paper introduces Dexpler, a software package which converts Dalvik bytecode to Jimple. Dexpler is built on top of Dedexer and Soot. As Jimple is Soot's main internal rep- resentation of code, the Dalvik bytecode can be manipu- lated with any Jimple based tool, for instance for performing point-to or flow analysis. ",Computer Science - Software Engineering ; ,"Bartel, Alexandre ; Klein, Jacques ; Monperrus, Martin ; Traon, Yves Le ; "
http://arxiv.org/abs/1205.4874,Perfect Secrecy Systems Immune to Spoofing Attacks,"  We present novel perfect secrecy systems that provide immunity to spoofing attacks under equiprobable source probability distributions. On the theoretical side, relying on an existence result for $t$-designs by Teirlinck, our construction method constructively generates systems that can reach an arbitrary high level of security. On the practical side, we obtain, via cyclic difference families, very efficient constructions of new optimal systems that are onefold secure against spoofing. Moreover, we construct, by means of $t$-designs for large values of $t$, the first near-optimal systems that are 5- and 6-fold secure as well as further systems with a feasible number of keys that are 7-fold secure against spoofing. We apply our results furthermore to a recently extended authentication model, where the opponent has access to a verification oracle. We obtain this way novel perfect secrecy systems with immunity to spoofing in the verification oracle model. ",Computer Science - Cryptography and Security ; Computer Science - Information Theory ; ,"Huber, Michael ; "
http://arxiv.org/abs/1205.5770,Randomized Extended Kaczmarz for Solving Least-Squares,  We present a randomized iterative algorithm that exponentially converges in expectation to the minimum Euclidean norm least squares solution of a given linear system of equations. The expected number of arithmetic operations required to obtain an estimate of given accuracy is proportional to the square condition number of the system multiplied by the number of non-zeros entries of the input matrix. The proposed algorithm is an extension of the randomized Kaczmarz method that was analyzed by Strohmer and Vershynin. ,Mathematics - Numerical Analysis ; Computer Science - Data Structures and Algorithms ; ,"Zouzias, Anastasios ; Freris, Nikolaos ; "
http://arxiv.org/abs/1205.6363,What Should Developers Be Aware Of? An Empirical Study on the Directives   of API Documentation,  Application Programming Interfaces (API) are exposed to developers in order to reuse software libraries. API directives are natural-language statements in API documentation that make developers aware of constraints and guidelines related to the usage of an API. This paper presents the design and the results of an empirical study on the directives of API documentation of object-oriented libraries. Its main contribution is to propose and extensively discuss a taxonomy of 23 kinds of API directives. ,Computer Science - Software Engineering ; ,"Monperrus, Martin ; Eichberg, Michael ; Tekes, Elif ; Mezini, Mira ; "
http://arxiv.org/abs/1206.1775,Exponential Time Complexity of the Permanent and the Tutte Polynomial,"  We show conditional lower bounds for well-studied #P-hard problems:   (a) The number of satisfying assignments of a 2-CNF formula with n variables cannot be counted in time exp(o(n)), and the same is true for computing the number of all independent sets in an n-vertex graph.   (b) The permanent of an n x n matrix with entries 0 and 1 cannot be computed in time exp(o(n)).   (c) The Tutte polynomial of an n-vertex multigraph cannot be computed in time exp(o(n)) at most evaluation points (x,y) in the case of multigraphs, and it cannot be computed in time exp(o(n/polylog n)) in the case of simple graphs.   Our lower bounds are relative to (variants of) the Exponential Time Hypothesis (ETH), which says that the satisfiability of n-variable 3-CNF formulas cannot be decided in time exp(o(n)). We relax this hypothesis by introducing its counting version #ETH, namely that the satisfying assignments cannot be counted in time exp(o(n)). In order to use #ETH for our lower bounds, we transfer the sparsification lemma for d-CNF formulas to the counting setting. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; F.2.1 ; G.2.1 ; ,"Dell, Holger ; Husfeldt, Thore ; Marx, Dániel ; Taslaman, Nina ; Wáhlen, Martin ; "
http://arxiv.org/abs/1206.3431,Computability and analysis: the legacy of Alan Turing,  We discuss the legacy of Alan Turing and his impact on computability and analysis. ,Mathematics - Logic ; Computer Science - Logic in Computer Science ; Mathematics - History and Overview ; ,"Avigad, Jeremy ; Brattka, Vasco ; "
http://arxiv.org/abs/1206.3862,Total coloring of 1-toroidal graphs of maximum degree at least 11 and no   adjacent triangles,"  A {\em total coloring} of a graph $G$ is an assignment of colors to the vertices and the edges of $G$ such that every pair of adjacent/incident elements receive distinct colors. The {\em total chromatic number} of a graph $G$, denoted by $\chiup''(G)$, is the minimum number of colors in a total coloring of $G$. The well-known Total Coloring Conjecture (TCC) says that every graph with maximum degree $\Delta$ admits a total coloring with at most $\Delta + 2$ colors. A graph is {\em $1$-toroidal} if it can be drawn in torus such that every edge crosses at most one other edge. In this paper, we investigate the total coloring of $1$-toroidal graphs, and prove that the TCC holds for the $1$-toroidal graphs with maximum degree at least~$11$ and some restrictions on the triangles. Consequently, if $G$ is a $1$-toroidal graph with maximum degree $\Delta$ at least~$11$ and without adjacent triangles, then $G$ admits a total coloring with at most $\Delta + 2$ colors. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Wang, Tao ; "
http://arxiv.org/abs/1206.4627,Convergence Rates of Biased Stochastic Optimization for Learning Sparse   Ising Models,"  We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Honorio, Jean ; "
http://arxiv.org/abs/1206.4656,Machine Learning that Matters,"  Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; Statistics - Machine Learning ; ,"Wagstaff, Kiri ; "
http://arxiv.org/abs/1206.4809,Connected Choice and the Brouwer Fixed Point Theorem,"  We study the computational content of the Brouwer Fixed Point Theorem in the Weihrauch lattice. Connected choice is the operation that finds a point in a non-empty connected closed set given by negative information. One of our main results is that for any fixed dimension the Brouwer Fixed Point Theorem of that dimension is computably equivalent to connected choice of the Euclidean unit cube of the same dimension. Another main result is that connected choice is complete for dimension greater than or equal to two in the sense that it is computably equivalent to Weak K\H{o}nig's Lemma. While we can present two independent proofs for dimension three and upwards that are either based on a simple geometric construction or a combinatorial argument, the proof for dimension two is based on a more involved inverse limit construction. The connected choice operation in dimension one is known to be equivalent to the Intermediate Value Theorem; we prove that this problem is not idempotent in contrast to the case of dimension two and upwards. We also prove that Lipschitz continuity with Lipschitz constants strictly larger than one does not simplify finding fixed points. Finally, we prove that finding a connectedness component of a closed subset of the Euclidean unit cube of any dimension greater or equal to one is equivalent to Weak K\H{o}nig's Lemma. In order to describe these results, we introduce a representation of closed subsets of the unit cube by trees of rational complexes. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D30, 03D78, 03F60, 37C25 ; ","Brattka, Vasco ; Roux, Stéphane Le ; Miller, Joseph S. ; Pauly, Arno ; "
http://arxiv.org/abs/1206.5336,Near-Optimal Online Multiselection in Internal and External Memory,"  We introduce an online version of the multiselection problem, in which q selection queries are requested on an unsorted array of n elements. We provide the first online algorithm that is 1-competitive with Kaligosi et al. [ICALP 2005] in terms of comparison complexity. Our algorithm also supports online search queries efficiently.   We then extend our algorithm to the dynamic setting, while retaining online functionality, by supporting arbitrary insertions and deletions on the array. Assuming that the insertion of an element is immediately preceded by a search for that element, we show that our dynamic online algorithm performs an optimal number of comparisons, up to lower order terms and an additive O(n) term.   For the external memory model, we describe the first online multiselection algorithm that is O(1)-competitive. This result improves upon the work of Sibeyn [Journal of Algorithms 2006] when q > m, where m is the number of blocks that can be stored in main memory. We also extend it to support searches, insertions, and deletions of elements efficiently. ",Computer Science - Data Structures and Algorithms ; ,"Barbay, Jérémy ; Gupta, Ankur ; Rao, S. Srinivasa ; Sorenson, Jonathan ; "
http://arxiv.org/abs/1208.0713,On logical hierarchies within FO^2-definable languages,"  We consider the class of languages defined in the 2-variable fragment of the first-order logic of the linear order. Many interesting characterizations of this class are known, as well as the fact that restricting the number of quantifier alternations yields an infinite hierarchy whose levels are varieties of languages (and hence admit an algebraic characterization). Using this algebraic approach, we show that the quantifier alternation hierarchy inside FO^{2}[<] is decidable within one unit. For this purpose, we relate each level of the hierarchy with decidable varieties of languages, which can be defined in terms of iterated deterministic and co-deterministic products. A crucial notion in this process is that of condensed rankers, a refinement of the rankers of Weis and Immerman and the turtle languages of Schwentick, Th\'erien and Vollmer. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; F.4.3 ; F.4.1 ; ,"Kufleitner, Manfred ; Weil, Pascal ; "
http://arxiv.org/abs/1208.3124,On the computation of zone and double zone diagrams,"  Classical objects in computational geometry are defined by explicit relations. Several years ago the pioneering works of T. Asano, J. Matousek and T. Tokuyama introduced ""implicit computational geometry"", in which the geometric objects are defined by implicit relations involving sets. An important member in this family is called ""a zone diagram"". The implicit nature of zone diagrams implies, as already observed in the original works, that their computation is a challenging task. In a continuous setting this task has been addressed (briefly) only by these authors in the Euclidean plane with point sites. We discuss the possibility to compute zone diagrams in a wide class of spaces and also shed new light on their computation in the original setting. The class of spaces, which is introduced here, includes, in particular, Euclidean spheres and finite dimensional strictly convex normed spaces. Sites of a general form are allowed and it is shown that a generalization of the iterative method suggested by Asano, Matousek and Tokuyama converges to a double zone diagram, another implicit geometric object whose existence is known in general. Occasionally a zone diagram can be obtained from this procedure. The actual (approximate) computation of the iterations is based on a simple algorithm which enables the approximate computation of Voronoi diagrams in a general setting. Our analysis also yields a few byproducts of independent interest, such as certain topological properties of Voronoi cells (e.g., that in the considered setting their boundaries cannot be ""fat""). ","Computer Science - Computational Geometry ; Mathematics - Functional Analysis ; Mathematics - Metric Geometry ; 68U05, 47H10, 51M05, 53C22, 46B20, 65D18, 51N05 ; F.2.2 ; G.0 ; I.3.5 ; ","Reem, Daniel ; "
http://arxiv.org/abs/1208.3251,Toward Resource-Optimal Consensus over the Wireless Medium,"  We carry out a comprehensive study of the resource cost of averaging consensus in wireless networks. Most previous approaches suppose a graphical network, which abstracts away crucial features of the wireless medium, and measure resource consumption only in terms of the total number of transmissions required to achieve consensus. Under a path-loss dominated model, we study the resource requirements of consensus with respect to three wireless-appropriate metrics: total transmit energy, elapsed time, and time-bandwidth product. First we characterize the performance of several popular gossip algorithms, showing that they may be order-optimal with respect to transmit energy but are strictly suboptimal with respect to elapsed time and time-bandwidth product. Further, we propose a new consensus scheme, termed hierarchical averaging, and show that it is nearly order-optimal with respect to all three metrics. Finally, we examine the effects of quantization, showing that hierarchical averaging provides a nearly order-optimal tradeoff between resource consumption and quantization error. ",Computer Science - Information Theory ; ,"Nokleby, Matthew ; Bajwa, Waheed U. ; Calderbank, Robert ; Aazhang, Behnaam ; "
http://arxiv.org/abs/1208.6408,Java Source-code Clustering: Unifying Syntactic and Semantic Features,"  This is a companion draft to paper 'Software Clustering: Unifying Syntactic and Semantic Features', in proceedings of the 19th Working Conference on Reverse Engineering (WCRE 2012). It discusses the clustering process in detail, which appeared in the paper in an abridged form. It also contains certain additional process steps which were not covered in the WCRE paper. The clustering process is described for applications with Java source-code. However, as argued in the WCRE paper, it can be seamlessly adapted to many other programming paradigms. ",Computer Science - Software Engineering ; D.2.7 ; ,"Misra, Janardan ; Kaulgud, Vikrant ; Titus, Gary ; KM, Annervaz ; Sengupta, Shubhashis ; "
http://arxiv.org/abs/1209.0521,Efficient EM Training of Gaussian Mixtures with Missing Data,"  In data-mining applications, we are frequently faced with a large fraction of missing entries in the data matrix, which is problematic for most discriminant machine learning algorithms. A solution that we explore in this paper is the use of a generative model (a mixture of Gaussians) to compute the conditional expectation of the missing variables given the observed variables. Since training a Gaussian mixture with many different patterns of missing values can be computationally very expensive, we introduce a spanning-tree based algorithm that significantly speeds up training in these conditions. We also observe that good results can be obtained by using the generative model to fill-in the missing values for a separate discriminant learning algorithm. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Delalleau, Olivier ; Courville, Aaron ; Bengio, Yoshua ; "
http://arxiv.org/abs/1209.0735,Lambert W Function for Applications in Physics,"  The Lambert W(x) function and its possible applications in physics are presented. The actual numerical implementation in C++ consists of Halley's and Fritsch's iterations with initial approximations based on branch-point expansion, asymptotic series, rational fits, and continued-logarithm recursion. ",Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; Physics - Computational Physics ; ,"Veberic, Darko ; "
http://arxiv.org/abs/1209.1711,Programming Languages for Scientific Computing,"  Scientific computation is a discipline that combines numerical analysis, physical understanding, algorithm development, and structured programming. Several yottacycles per year on the world's largest computers are spent simulating problems as diverse as weather prediction, the properties of material composites, the behavior of biomolecules in solution, and the quantum nature of chemical compounds. This article is intended to review specfic languages features and their use in computational science. We will review the strengths and weaknesses of different programming styles, with examples taken from widely used scientific codes. ","Computer Science - Programming Languages ; Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Mathematical Software ; ","Knepley, Matthew G. ; "
http://arxiv.org/abs/1209.5527,Strategic Learning and the Topology of Social Networks,"  We consider a group of strategic agents who must each repeatedly take one of two possible actions. They learn which of the two actions is preferable from initial private signals, and by observing the actions of their neighbors in a social network.   We show that the question of whether or not the agents learn efficiently depends on the topology of the social network. In particular, we identify a geometric ""egalitarianism"" condition on the social network that guarantees learning in infinite networks, or learning with high probability in large finite networks, in any equilibrium. We also give examples of non-egalitarian networks with equilibria in which learning fails. ",Computer Science - Computer Science and Game Theory ; Economics - Theoretical Economics ; Mathematics - Probability ; ,"Mossel, Elchanan ; Sly, Allan ; Tamuz, Omer ; "
http://arxiv.org/abs/1209.5785,Coupling Data Transmission for Multiple-Access Communications,"  We consider a signaling format where the information to be communicated from one or multiple transmitters to a receiver is modulated via a superposition of independent data streams. Each data stream is formed by error-correction encoding, constellation mapping, replication and permutation of symbols, and application of signature sequences. The relations between the data bits and modulation symbols transmitted over the channel can be represented by a sparse graph. In the case where the modulated data streams are transmitted with time offsets the receiver observes spatial coupling of the individual graphs into a graph chain enabling efficient demodulation/decoding. We prove that a two-stage demodulation/decoding method, in which iterative demodulation based on symbol estimation and interference cancellation is followed by parallel error correction decoding, achieves capacity on the additive white Gaussian noise (AWGN) channel asymptotically. We compare the performance of the two-stage receiver to the receiver which utilizes hard feedback between the error-correction encoders and the iterative demodulator. ",Computer Science - Information Theory ; ,"Truhachev, Dmitri ; Schlegel, Christian ; "
http://arxiv.org/abs/1209.6626,On Newton-Raphson iteration for multiplicative inverses modulo prime   powers,"  We study algorithms for the fast computation of modular inverses. Newton-Raphson iteration over $p$-adic numbers gives a recurrence relation computing modular inverse modulo $p^m$, that is logarithmic in $m$. We solve the recurrence to obtain an explicit formula for the inverse. Then we study different implementation variants of this iteration and show that our explicit formula is interesting for small exponent values but slower or large exponent, say of more than $700$ bits. Overall we thus propose a hybrid combination of our explicit formula and the best asymptotic variants. This hybrid combination yields then a constant factor improvement, also for large exponents. ",Computer Science - Symbolic Computation ; Computer Science - Mathematical Software ; ,"Dumas, Jean-Guillaume ; "
http://arxiv.org/abs/1210.2246,An empirical study to order citation statistics between subject fields,"  An empirical study is conducted to compare citations per publication, statistics and observed Hirsch indexes between subject fields using summary statistics of countries. No distributional assumptions are made and ratios are calculated. These ratios can be used to make approximate comparisons between researchers of different subject fields with respect to the Hirsch index. ",Computer Science - Digital Libraries ; Physics - Physics and Society ; 62P99 ; ,"van Zyl, J. Martin ; van der Merwe, Sean ; "
http://arxiv.org/abs/1210.2440,"Group Model Selection Using Marginal Correlations: The Good, the Bad and   the Ugly","  Group model selection is the problem of determining a small subset of groups of predictors (e.g., the expression data of genes) that are responsible for majority of the variation in a response variable (e.g., the malignancy of a tumor). This paper focuses on group model selection in high-dimensional linear models, in which the number of predictors far exceeds the number of samples of the response variable. Existing works on high-dimensional group model selection either require the number of samples of the response variable to be significantly larger than the total number of predictors contributing to the response or impose restrictive statistical priors on the predictors and/or nonzero regression coefficients. This paper provides comprehensive understanding of a low-complexity approach to group model selection that avoids some of these limitations. The proposed approach, termed Group Thresholding (GroTh), is based on thresholding of marginal correlations of groups of predictors with the response variable and is reminiscent of existing thresholding-based approaches in the literature. The most important contribution of the paper in this regard is relating the performance of GroTh to a polynomial-time verifiable property of the predictors for the general case of arbitrary (random or deterministic) predictors and arbitrary nonzero regression coefficients. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; Statistics - Machine Learning ; ,"Bajwa, Waheed U. ; Mixon, Dustin G. ; "
http://arxiv.org/abs/1210.2540,"On the Automorphism Group of a Binary Self-dual [120, 60, 24] Code","  We prove that an automorphism of order 3 of a putative binary self-dual [120, 60, 24] code C has no fixed points. Moreover, the order of the automorphism group of C divides 2^a.3.5.7.19.23.29 where a is a nonegative integer. Automorphisms of odd composite order r may occur only for r=15, 57 or r=115 with corresponding cycle structures 15-(0,0,8;0), 57-(2,0,2;0) or 115-(1,0,1;0), respectively. In case that all involutions act fixed point freely we have |Aut(C)|<=920, and Aut(C) is solvable if it contains an element of prime order p>=7. Moreover, the alternating group A_5 is the only non-abelian composition factor which may occur. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Mathematics - Group Theory ; ,"Bouyuklieva, Stefka ; de la Cruz, Javier ; Willems, Wolfgang ; "
http://arxiv.org/abs/1210.4663,A PRQ Search Method for Probabilistic Objects,"  This article proposes an PQR search method for probabilistic objects. The main idea of our method is to use a strategy called \textit{pre-approximation} that can reduce the initial problem to a highly simplified version, implying that it makes the rest of steps easy to tackle. In particular, this strategy itself is pretty simple and easy to implement. Furthermore, motivated by the cost analysis, we further optimize our solution. The optimizations are mainly based on two insights: (\romannumeral 1) the number of \textit{effective subdivision}s is no more than 1; and (\romannumeral 2) an entity with the larger \textit{span} is more likely to subdivide a single region. We demonstrate the effectiveness and efficiency of our proposed approaches through extensive experiments under various experimental settings. ",Computer Science - Databases ; Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; H.2.8 ; H.3.3 ; G.3 ; ,"Wang, Jack ; "
http://arxiv.org/abs/1210.4959,Halving Lines and Their Underlying Graphs,"  In this paper we study underlying graphs corresponding to a set of halving lines. We establish many properties of such graphs. In addition, we tighten the upper bound for the number of halving lines. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C30 ; ,"Khovanova, Tanya ; Yang, Dai ; "
http://arxiv.org/abs/1210.5065,Realizability algebras III: some examples,"  We use the technique of ""classical realizability"" to build new models of ZF + DC in which R is not well ordered. This gives new relative consistency results, probably not obtainable by forcing. This gives also a new method to get programs from proofs of arithmetical formulas with dependent choice. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03E35 ; F.4.1 ; ,"Krivine, Jean-Louis ; "
http://arxiv.org/abs/1210.7341,Subset Codes for Packet Networks,"  In this paper, we present a coding-theoretic framework for message transmission over packet-switched networks. Network is modeled as a channel which can induce packet errors, deletions, insertions, and out of order delivery of packets. The proposed approach can be viewed as an extension of the one introduced by Koetter and Kschischang for networks based on random linear network coding. Namely, while their framework is based on subspace codes and designed for networks in which network nodes perform random linear combining of the packets, ours is based on the so-called subset codes, and is designed for networks employing routing in network nodes. ",Computer Science - Information Theory ; 94B60 ; ,"Kovačević, Mladen ; Vukobratović, Dejan ; "
http://arxiv.org/abs/1210.7638,Finding Efficient Region in The Plane with Line segments,"  Let $\mathscr O$ be a set of $n$ disjoint obstacles in $\mathbb{R}^2$, $\mathscr M$ be a moving object. Let $s$ and $l$ denote the starting point and maximum path length of the moving object $\mathscr M$, respectively. Given a point $p$ in ${R}^2$, we say the point $p$ is achievable for $\mathscr M$ such that $\pi(s,p)\leq l$, where $\pi(\cdot)$ denotes the shortest path length in the presence of obstacles. One is to find a region $\mathscr R$ such that, for any point $p\in \mathbb{R}^2$, if it is achievable for $\mathscr M$, then $p\in \mathscr R$; otherwise, $p\notin \mathscr R$. In this paper, we restrict our attention to the case of line-segment obstacles. To tackle this problem, we develop three algorithms. We first present a simpler-version algorithm for the sake of intuition. Its basic idea is to reduce our problem to computing the union of a set of circular visibility regions (CVRs). This algorithm takes $O(n^3)$ time. By analysing its dominant steps, we break through its bottleneck by using the short path map (SPM) technique to obtain those circles (unavailable beforehand), yielding an $O(n^2\log n)$ algorithm. Owing to the finding above, the third algorithm also uses the SPM technique. It however, does not continue to construct the CVRs. Instead, it directly traverses each region of the SPM to trace the boundaries, the final algorithm obtains $O(n\log n)$ complexity. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Geometry ; F.2 ; I.1.2 ; ,"Wang, Jack ; "
http://arxiv.org/abs/1211.0071,Randomness and Non-determinism,"  Exponentiation makes the difference between the bit-size of this line and the number (<< 2^{300}) of particles in the known Universe. The expulsion of exponential time algorithms from Computer Theory in the 60's broke its umbilical cord from Mathematical Logic. It created a deep gap between deterministic computation and -- formerly its unremarkable tools -- randomness and non-determinism. Little did we learn in the past decades about the power of either of these two basic ""freedoms"" of computation, but some vague pattern is emerging in relationships between them. The pattern of similar techniques instrumental for quite different results in this area seems even more interesting. Ideas like multilinear and low-degree multivariate polynomials, Fourier transformation over low-periodic groups seem very illuminating. The talk surveyed some recent results. One of them, given in a stronger form than previously published, is described below. ",Computer Science - Computational Complexity ; Computer Science - Cryptography and Security ; Computer Science - Information Theory ; ,"Levin, Leonid A. ; "
http://arxiv.org/abs/1211.0589,Sharp Bounds on Random Walk Eigenvalues via Spectral Embedding,"  Spectral embedding of graphs uses the top k non-trivial eigenvectors of the random walk matrix to embed the graph into R^k. The primary use of this embedding has been for practical spectral clustering algorithms [SM00,NJW02]. Recently, spectral embedding was studied from a theoretical perspective to prove higher order variants of Cheeger's inequality [LOT12,LRTV12].   We use spectral embedding to provide a unifying framework for bounding all the eigenvalues of graphs. For example, we show that for any finite graph with n vertices and all k >= 2, the k-th largest eigenvalue is at most 1-Omega(k^3/n^3), which extends the only other such result known, which is for k=2 only and is due to [LO81]. This upper bound improves to 1-Omega(k^2/n^2) if the graph is regular. We generalize these results, and we provide sharp bounds on the spectral measure of various classes of graphs, including vertex-transitive graphs and infinite graphs, in terms of specific graph parameters like the volume growth.   As a consequence, using the entire spectrum, we provide (improved) upper bounds on the return probabilities and mixing time of random walks with considerably shorter and more direct proofs. Our work introduces spectral embedding as a new tool in analyzing reversible Markov chains. Furthermore, building on [Lyo05], we design a local algorithm to approximate the number of spanning trees of massive graphs. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Spectral Theory ; ,"Lyons, Russell ; Gharan, Shayan Oveis ; "
http://arxiv.org/abs/1211.0729,A Simple Algorithm for Computing BOCP,"  In this article, we devise a concise algorithm for computing BOCP. Our method is simple, easy-to-implement but without loss of efficiency. Given two circular-arc polygons with $m$ and $n$ edges respectively, our method runs in $O(m+n+(l+k)\log l)$ time, using $O(m+n+k)$ space, where $k$ is the number of intersections, and $l$ is the number of {edge}s. Our algorithm has the power to approximate to linear complexity when $k$ and $l$ are small. The superiority of the proposed algorithm is also validated through empirical study. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Geometry ; Computer Science - Graphics ; E.1 ; I.3.5 ; I.3.6 ; ,"Wang, Jack ; "
http://arxiv.org/abs/1211.0877,Differential Privacy for the Analyst via Private Equilibrium Computation,"  We give new mechanisms for answering exponentially many queries from multiple analysts on a private database, while protecting differential privacy both for the individuals in the database and for the analysts. That is, our mechanism's answer to each query is nearly insensitive to changes in the queries asked by other analysts. Our mechanism is the first to offer differential privacy on the joint distribution over analysts' answers, providing privacy for data analysts even if the other data analysts collude or register multiple accounts. In some settings, we are able to achieve nearly optimal error rates (even compared to mechanisms which do not offer analyst privacy), and we are able to extend our techniques to handle non-linear queries. Our analysis is based on a novel view of the private query-release problem as a two-player zero-sum game, which may be of independent interest. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; ,"Hsu, Justin ; Roth, Aaron ; Ullman, Jonathan ; "
http://arxiv.org/abs/1211.2384,Strong Bounds for Evolution in Undirected Graphs,"  This work studies the generalized Moran process, as introduced by Lieberman et al. [Nature, 433:312-316, 2005]. We introduce the parameterized notions of selective amplifiers and selective suppressors of evolution, i.e. of networks (graphs) with many ""strong starts"" and many ""weak starts"" for the mutant, respectively. We first prove the existence of strong selective amplifiers and of (quite) strong selective suppressors. Furthermore we provide strong upper bounds and almost tight lower bounds (by proving the ""Thermal Theorem"") for the traditional notion of fixation probability of Lieberman et al., i.e. assuming a random initial placement of the mutant. ",Computer Science - Data Structures and Algorithms ; Mathematics - Probability ; G.3 ; E.1 ; ,"Mertzios, George B. ; Spirakis, Paul G. ; "
http://arxiv.org/abs/1211.2662,Recognizing Interval Bigraphs by Forbidden Patterns,  Let H be a connected bipartite graph with n nodes and m edges. We give an O(nm) time algorithm to decide whether H is an interval bigraph. The best known algorithm has time complexity O(nm^6(m + n) \log n) and it was developed in 1997 [18]. Our approach is based on an ordering characterization of interval bigraphs introduced by Hell and Huang [13]. We transform the problem of finding the desired ordering to choosing strong components of a pair-digraph without creating conflicts. We make use of the structure of the pair-digraph as well as decomposition of bigraph H based on the special components of the pair-digraph. This way we make explicit what the difficult cases are and gain efficiency by isolating such situations. ,Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Rafiey, Arash ; "
http://arxiv.org/abs/1211.3234,Computational topology and normal surfaces: Theoretical and experimental   complexity bounds,"  In three-dimensional computational topology, the theory of normal surfaces is a tool of great theoretical and practical significance. Although this theory typically leads to exponential time algorithms, very little is known about how these algorithms perform in ""typical"" scenarios, or how far the best known theoretical bounds are from the real worst-case scenarios. Here we study the combinatorial and algebraic complexity of normal surfaces from both the theoretical and experimental viewpoints. Theoretically, we obtain new exponential lower bounds on the worst-case complexities in a variety of settings that are important for practical computation. Experimentally, we study the worst-case and average-case complexities over a comprehensive body of roughly three billion input triangulations. Many of our lower bounds are the first known exponential lower bounds in these settings, and experimental evidence suggests that many of our theoretical lower bounds on worst-case growth rates may indeed be asymptotically tight. ","Mathematics - Geometric Topology ; Computer Science - Computational Geometry ; Mathematics - Combinatorics ; 68Q17 (Primary) 68Q15, 68Q15, 57Q35, 57Q35 (Secondary) ; ","Burton, Benjamin A. ; Paixão, João ; Spreer, Jonathan ; "
http://arxiv.org/abs/1211.4892,Confusion of Tagged Perturbations in Forward Automatic Differentiation   of Higher-Order Functions,"  Forward Automatic Differentiation (AD) is a technique for augmenting programs to compute derivatives. The essence of Forward AD is to attach perturbations to each number, and propagate these through the computation. When derivatives are nested, the distinct derivative calculations, and their associated perturbations, must be distinguished. This is typically accomplished by creating a unique tag for each derivative calculation, tagging the perturbations, and overloading the arithmetic operators. We exhibit a subtle bug, present in fielded implementations, in which perturbations are confused despite the tagging machinery. The essence of the bug is this: each invocation of a derivative creates a unique tag but a unique tag is needed for each derivative calculation. When taking derivatives of higher-order functions, these need not correspond! The derivative of a higher-order function $f$ that returns a function $g$ will be a function $f'$ that returns a function $\bar{g}$ that performs a derivative calculation. A single invocation of $f'$ will create a single fresh tag but that same tag will be used for each derivative calculation resulting from an invocation of $\bar{g}$. This situation arises when taking derivatives of curried functions. Two potential solutions are presented, and their serious deficiencies discussed. One requires eta expansion to delay the creation of fresh tags from the invocation of $f'$ to the invocation of $\bar{g}$, which can be difficult or even impossible in some circumstances. The other requires $f'$ to wrap $\bar{g}$ with tag renaming, which is difficult to implement without violating the desirable complexity properties of forward AD. ",Computer Science - Symbolic Computation ; Computer Science - Mathematical Software ; Mathematics - Differential Geometry ; ,"Manzyuk, Oleksandr ; Pearlmutter, Barak A. ; Radul, Alexey Andreyevich ; Rush, David R. ; Siskind, Jeffrey Mark ; "
http://arxiv.org/abs/1211.5608,Blind Deconvolution using Convex Programming,"  We consider the problem of recovering two unknown vectors, $\boldsymbol{w}$ and $\boldsymbol{x}$, of length $L$ from their circular convolution. We make the structural assumption that the two vectors are members of known subspaces, one with dimension $N$ and the other with dimension $K$. Although the observed convolution is nonlinear in both $\boldsymbol{w}$ and $\boldsymbol{x}$, it is linear in the rank-1 matrix formed by their outer product $\boldsymbol{w}\boldsymbol{x}^*$. This observation allows us to recast the deconvolution problem as low-rank matrix recovery problem from linear measurements, whose natural convex relaxation is a nuclear norm minimization program.   We prove the effectiveness of this relaxation by showing that for ""generic"" signals, the program can deconvolve $\boldsymbol{w}$ and $\boldsymbol{x}$ exactly when the maximum of $N$ and $K$ is almost on the order of $L$. That is, we show that if $\boldsymbol{x}$ is drawn from a random subspace of dimension $N$, and $\boldsymbol{w}$ is a vector in a subspace of dimension $K$ whose basis vectors are ""spread out"" in the frequency domain, then nuclear norm minimization recovers $\boldsymbol{w}\boldsymbol{x}^*$ without error.   We discuss this result in the context of blind channel estimation in communications. If we have a message of length $N$ which we code using a random $L\times N$ coding matrix, and the encoded message travels through an unknown linear time-invariant channel of maximum length $K$, then the receiver can recover both the channel response and the message when $L\gtrsim N+K$, to within constant and log factors. ",Computer Science - Information Theory ; ,"Ahmed, Ali ; Recht, Benjamin ; Romberg, Justin ; "
http://arxiv.org/abs/1211.5773,Circuit complexity and Problem structure in Hamming space,"  This paper describes about relation between circuit complexity and accept inputs structure in Hamming space by using almost all monotone circuit that emulate deterministic Turing machine (DTM).   Circuit family that emulate DTM are almost all monotone circuit family except some NOT-gate which connect input variables (like negation normal form (NNF)). Therefore, we can analyze DTM limitation by using this NNF Circuit family.   NNF circuit have symmetry of OR-gate input line, so NNF circuit cannot identify from OR-gate output line which of OR-gate input line is 1. So NNF circuit family cannot compute sandwich structure effectively (Sandwich structure is two accept inputs that sandwich reject inputs in Hamming space). NNF circuit have to use unique AND-gate to identify each different vector of sandwich structure. That is, we can measure problem complexity by counting different vectors.   Some decision problem have characteristic in sandwich structure. Different vectors of Negate HornSAT problem are at most constant length because we can delete constant part of each negative literal in Horn clauses by using definite clauses. Therefore, number of these different vector is at most polynomial size. The other hand, we can design high complexity problem with almost perfct nonlinear (APN) function. ",Computer Science - Computational Complexity ; ,"Kobayashi, Koji ; "
http://arxiv.org/abs/1211.6468,"Using Isabelle to verify special relativity, with application to   hypercomputation theory","  Logicians at the R\'enyi Mathematical Institute in Budapest have spent several years developing versions of relativity theory (special, general, and other variants) based wholly on first order logic, and have argued in favour of the physical decidability, via exploitation of cosmological phenomena, of formally undecidable questions such as the Halting Problem and the consistency of set theory.   The Hungarian theories are very extensive, and their associated proofs are intuitively very satisfying, but this brings its own risks since intuition can sometimes be misleading. As part of a joint project, researchers at Sheffield have recently started generating rigorous machine-verified versions of the Hungarian proofs, so as to demonstrate the soundness of their work. In this paper, we explain the background to the project and demonstrate an Isabelle proof of the theorem ""No inertial observer can travel faster than light"".   This approach to physical theories and physical computability has several pay-offs: (a) we can be certain our intuition hasn't led us astray (or if it has, we can identify where this has happened); (b) we can identify which axioms are specifically required in the proof of each theorem and to what extent those axioms can be weakened (the fewer assumptions we make up-front, the stronger the results); and (c) we can identify whether new formal proof techniques and tactics are needed when tackling physical as opposed to mathematical theories. ",Computer Science - Logic in Computer Science ; General Relativity and Quantum Cosmology ; F.4.1 ; J.2 ; ,"Stannett, Mike ; Németi, István ; "
http://arxiv.org/abs/1211.6470,A new class of SETI beacons that contain information (22-aug-2010),"  In the cm-wavelength range, an extraterrestrial electromagnetic narrow band (sine wave) beacon is an excellent choice to get alien attention across interstellar distances because 1) it is not strongly affected by interstellar / interplanetary dispersion or scattering, and 2) searching for narrowband signals is computationally efficient (scales as Ns log(Ns) where Ns = number of voltage samples). Here we consider a special case wideband signal where two or more delayed copies of the same signal are transmitted over the same frequency and bandwidth, with the result that ISM dispersion and scattering cancel out during the detection stage. Such a signal is both a good beacon (easy to find) and carries arbitrarily large information rate (limited only by the atmospheric transparency to about 10 GHz). The discovery process uses an autocorrelation algorithm, and we outline a compute scheme where the beacon discovery search can be accomplished with only 2x the processing of a conventional sine wave search, and discuss signal to background response for sighting the beacon. Once the beacon is discovered, the focus turns to information extraction. Information extraction requires similar processing as for generic wideband signal searches, but since we have already identified the beacon, the efficiency of information extraction is negligible. ",Astrophysics - Instrumentation and Methods for Astrophysics ; Computer Science - Other Computer Science ; ,"Harp, G. R. ; Ackermann, R. F. ; Blair, Samantha K. ; Arbunich, J. ; Backus, P. R. ; Tarter, J. C. ; Team, the ATA ; "
http://arxiv.org/abs/1212.1095,The projector algorithm: a simple parallel algorithm for computing   Voronoi diagrams and Delaunay graphs,"  The Voronoi diagram is a certain geometric data structure which has numerous applications in various scientific and technological fields. The theory of algorithms for computing 2D Euclidean Voronoi diagrams of point sites is rich and useful, with several different and important algorithms. However, this theory has been quite steady during the last few decades in the sense that no essentially new algorithms have entered the game. In addition, most of the known algorithms are serial in nature and hence cast inherent difficulties on the possibility to compute the diagram in parallel. In this paper we present the projector algorithm: a new and simple algorithm which enables the (combinatorial) computation of 2D Voronoi diagrams. The algorithm is significantly different from previous ones and some of the involved concepts in it are in the spirit of linear programming and optics. Parallel implementation is naturally supported since each Voronoi cell can be computed independently of the other cells. A new combinatorial structure for representing the cells (and any convex polytope) is described along the way and the computation of the induced Delaunay graph is obtained almost automatically. ","Computer Science - Computational Geometry ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; 68U05, 68W10, 65D18, 68W40, 52B05 ; D.1.3 ; D.3.2 ; F.1.2 ; F.2.2 ; G.1.0 ; G.2.1 ; G.4 ; I.1.2 ; I.3.5 ; ","Reem, Daniel ; "
http://arxiv.org/abs/1212.1149,Threshold Digraphs,"  A digraph whose degree sequence has a unique vertex labeled realization is called threshold. In this paper we present several characterizations of threshold digraphs and their degree sequences, and show these characterizations to be equivalent. One of the characterizations is new, and allows for a shorter proof of the equivalence of the two known characterizations as well as proving the final characterization which appears without proof in the literature. Using this result, we obtain a new, short proof of the Fulkerson-Chen theorem on degree sequences of general digraphs. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C20 ; ,"Cloteaux, Brian ; LaMar, M. Drew ; Moseman, Elizabeth ; Shook, James ; "
http://arxiv.org/abs/1212.1710,"The information and its observer: external and internal information   processes, information cooperation, and the origin of the observer intellect","  In observing interactive processes, conversion of observed uncertainty to observer certainty is natural phenomenon creating Yes-No actions of information Bit and its information observer. Observer emerges from interacting random field of Kolmogorov probabilities which link Kolmogorov 0-1 law probabilities and Bayesian probabilities observing Markov diffusion process by probabilistic 0-1 impulses. Each No-0 action cuts maximum of impulse minimal entropy while following Yes-1 action transfers maxim between impulses performing dual principle of converting process entropy to information. Merging Yes-No actions generate microprocess within bordered impulse producing Bit with free information when the microprocess probability approaches 1. Interacting bits memorizes each impulse free information which attracts multiple Bits moving macroprocess that self-joins triplet macrounits. Each memorized information binds reversible microprocess within impulse with irreversible macroprocess. The observation automatically converts cutting entropy to information macrounits. Macrounits logically self-organize information networks IN encoding the units in geometrical structures enclosing triplet code. Multiple IN binds their ending triplets which encloses observer information cognition and intelligence. The observer cognition assembles common units through multiple attraction and resonances at forming IN triplet hierarchy which accept only units that recognizes each IN node. Maximal number of accepted triplet levels in multiple IN measures the observer maximum comparative information intelligence. The observation process carries probabilistic and certain wave function which self-organizes the space hierarchical structures.These information regularities create integral logic and intelligence self-operating multiple IN up to physical reality matter. ","Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Computer Science - Information Theory ; 58J65, 60J65, 93B52, 93E02, 93E15, 93E30 ; H.1.1 ; ","Lerner, Vladimir S. ; "
http://arxiv.org/abs/1212.6751,Computably Categorical Fields via Fermat's Last Theorem,"  We construct a computable, computably categorical field of infinite transcendence degree over the rational numbers, using the Fermat polynomials and assorted results from algebraic geometry. We also show that this field has an intrinsically computable (infinite) transcendence basis. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03C57 (Primary), 03D45, 12L05 (Secondary) ; ","Miller, Russell ; Schoutens, Hans ; "
http://arxiv.org/abs/1212.6879,On two conjectures of Maurer concerning basis graphs of matroids,"  We characterize 2-dimensional complexes associated canonically with basis graphs of matroids as simply connected triangle-square complexes satisfying some local conditions. This proves a version of a (disproved) conjecture by Stephen Maurer (Conjecture 3 of S. Maurer, Matroid basis graphs I, JCTB 14 (1973), 216-240). We also establish Conjecture 1 from the same paper about the redundancy of the conditions in the characterization of basis graphs. We indicate positive-curvature-like aspects of the local properties of the studied complexes. We characterize similarly the corresponding 2-dimensional complexes of even $\Delta$-matroids. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05B35, 05C12, 57M10, 57M20 ; ","Chalopin, Jérémie ; Chepoi, Victor ; Osajda, Damian ; "
http://arxiv.org/abs/1301.1027,On online energy harvesting in multiple access communication systems,"  We investigate performance limits of a multiple access communication system with energy harvesting nodes where the utility function is taken to be the long-term average sum-throughput. We assume a causal structure for energy arrivals and study the problem in the continuous time regime. For this setting, we first characterize a storage dam model that captures the dynamics of a battery with energy harvesting and variable transmission power. Using this model, we next establish an upper bound on the throughput problem as a function of battery capacity. We also formulate a non-linear optimization problem to determine optimal achievable power policies for transmitters. Applying a calculus of variation technique, we then derive Euler-Lagrange equations as necessary conditions for optimum power policies in terms of a system of coupled partial integro-differential equations (PIDEs). Based on a Gauss-Seidel algorithm, we devise an iterative algorithm to solve these equations. We also propose a fixed-point algorithm for the symmetric multiple access setting in which the statistical descriptions of energy harvesters are identical. Along with the analysis and to support our iterative algorithms, comprehensive numerical results are also obtained. ",Computer Science - Information Theory ; ,"Khuzani, Masoud Badiei ; Mitran, Patrick ; "
http://arxiv.org/abs/1301.1071,Direct QR factorizations for tall-and-skinny matrices in MapReduce   architectures,"  The QR factorization and the SVD are two fundamental matrix decompositions with applications throughout scientific computing and data analysis. For matrices with many more rows than columns, so-called ""tall-and-skinny matrices,"" there is a numerically stable, efficient, communication-avoiding algorithm for computing the QR factorization. It has been used in traditional high performance computing and grid computing environments. For MapReduce environments, existing methods to compute the QR decomposition use a numerically unstable approach that relies on indirectly computing the Q factor. In the best case, these methods require only two passes over the data. In this paper, we describe how to compute a stable tall-and-skinny QR factorization on a MapReduce architecture in only slightly more than 2 passes over the data. We can compute the SVD with only a small change and no difference in performance. We present a performance comparison between our new direct TSQR method, a standard unstable implementation for MapReduce (Cholesky QR), and the classic stable algorithm implemented for MapReduce (Householder QR). We find that our new stable method has a large performance advantage over the Householder QR method. This holds both in a theoretical performance model as well as in an actual implementation. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Numerical Analysis ; ","Benson, Austin R. ; Gleich, David F. ; Demmel, James ; "
http://arxiv.org/abs/1301.1107,Spectral Condition-Number Estimation of Large Sparse Matrices,"  We describe a randomized Krylov-subspace method for estimating the spectral condition number of a real matrix A or indicating that it is numerically rank deficient. The main difficulty in estimating the condition number is the estimation of the smallest singular value \sigma_{\min} of A. Our method estimates this value by solving a consistent linear least-squares problem with a known solution using a specific Krylov-subspace method called LSQR. In this method, the forward error tends to concentrate in the direction of a right singular vector corresponding to \sigma_{\min}. Extensive experiments show that the method is able to estimate well the condition number of a wide array of matrices. It can sometimes estimate the condition number when running a dense SVD would be impractical due to the computational cost or the memory requirements. The method uses very little memory (it inherits this property from LSQR) and it works equally well on square and rectangular matrices. ",Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Avron, Haim ; Druinsky, Alex ; Toledo, Sivan ; "
http://arxiv.org/abs/1301.2158,Artificial Intelligence Framework for Simulating Clinical   Decision-Making: A Markov Decision Process Approach,"  In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This serves two potential functions: 1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and 2) the basis for clinical artificial intelligence - an AI that can think like a doctor. This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans. This framework was evaluated using real patient data from an electronic health record. Such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a 30-35% increase in patient outcomes. Tweaking certain model parameters further enhances this advantage, obtaining roughly 50% more improvement for roughly half the costs. Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine. ",Computer Science - Artificial Intelligence ; Statistics - Machine Learning ; ,"Bennett, Casey C. ; Hauser, Kris ; "
http://arxiv.org/abs/1301.2959,New elements for a network (including brain) general theory during   learning period,"  This study deals with the evolution of the so called 'intelligent' networks (insect society without leader, cells of an organism, brain,...) during their learning period. First we summarize briefly the Version 2 (published in French), whose the main characteristics are: 1) A network connected to its environment is considered as immersed into an information field created by this environment which so dictates to it the learning constraints. 2) The used formalism draws one's inspiration from the one of the Quantum field theory (Principle of stationary action, gauge fields, invariance by symmetry transformations,...). 3) We obtain Lagrange equations whose solutions describe the network evolution during the whole learning period. 4) Then, while proceeding with the same formalism inspiration, we suggest other study ways capable of evolving the knowledge in the considered scope. In a second part, after a reminder of the points to be improved, we exhibit the Version 5 which brings, we think, relevant improvements. Indeed: 5) We consider the weighted averages of the variables; this introduces probabilities. 6) We define two observables (L average of information flux and A activity of the network) which could be measured and so be compared with experimental results. 7) We find that L , weighted average of information flows, is an invariant. 8) Finally, we propose two expressions for the conactance, from which we deduce the corresponding Lagrange equations which have to be solved to know the evolution of the considered weighted averages. But, at the present stage, we think that we can progress only by carrying out experiments (see projects like Human brain project) and discovering invariants, symmetries which would allow us, like in Physics, to classify networks and above all to understand better the connections between them. Indeed, and that is what we propose among the future research ways, the underlying problem is to understand how, after their learning period, several networks can connect together to produce, in the brain case for instance, what we call mental states. ",Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Computer Science - Neural and Evolutionary Computing ; Nonlinear Sciences - Chaotic Dynamics ; ,"Piniello, Jean ; "
http://arxiv.org/abs/1301.3605,Feature Learning in Deep Neural Networks - Studies on Speech Recognition   Tasks,"  Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization. ",Computer Science - Machine Learning ; Computer Science - Computation and Language ; Computer Science - Neural and Evolutionary Computing ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Yu, Dong ; Seltzer, Michael L. ; Li, Jinyu ; Huang, Jui-Ting ; Seide, Frank ; "
http://arxiv.org/abs/1301.5055,"Nested Recursions, Simultaneous Parameters and Tree Superpositions","  We apply a tree-based methodology to solve new, very broadly defined families of nested recursions of the general form R(n)=sum_{i=1}^k R(n-a_i-sum_{j=1}^p R(n-b_{ij})), where a_i are integers, b_{ij} are natural numbers, and k,p are natural numbers that we use to denote ""arity"" and ""order,"" respectively, and with some specified initial conditions. The key idea of the tree-based solution method is to associate such recursions with infinite labelled trees in a natural way so that the solution to the recursions solves a counting question relating to the corresponding trees. We characterize certain recursion families within R(n) by introducing ""simultaneous parameters"" that appear both within the recursion itself and that also specify structural properties of the corresponding tree. First, we extend and unify recently discovered results concerning two families of arity k=2, order p=1 recursions. Next, we investigate the solution of nested recursion families by taking linear combinations of solution sequence frequencies for simpler nested recursions, which correspond to superpositions of the associated trees; this leads us to identify and solve two new recursion families for arity k=2 and general order p. Finally, we extend these results to general arity k>2. We conclude with several related open problems. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 11B37, 05C05 (Primary) 05A15, 05A19 (Secondary) ; ","Isgur, Abraham ; Kuznetsov, Vitaly ; Rahman, Mustazee ; Tanny, Stephen ; "
http://arxiv.org/abs/1301.5293,Approximately counting semismooth integers,"  An integer $n$ is $(y,z)$-semismooth if $n=pm$ where $m$ is an integer with all prime divisors $\le y$ and $p$ is 1 or a prime $\le z$. arge quantities of semismooth integers are utilized in modern integer factoring algorithms, such as the number field sieve, that incorporate the so-called large prime variant. Thus, it is useful for factoring practitioners to be able to estimate the value of $\Psi(x,y,z)$, the number of $(y,z)$-semismooth integers up to $x$, so that they can better set algorithm parameters and minimize running times, which could be weeks or months on a cluster supercomputer. In this paper, we explore several algorithms to approximate $\Psi(x,y,z)$ using a generalization of Buchstab's identity with numeric integration. ","Computer Science - Data Structures and Algorithms ; Mathematics - Number Theory ; 11y16, 11y05, 11a51 ; F.2.1 ; I.1.2 ; ","Bach, Eric ; Sorenson, Jonathan ; "
http://arxiv.org/abs/1301.5522,On Gaussian Half-Duplex Relay Networks,"  This paper considers Gaussian relay networks where a source transmits a message to a sink terminal with the help of one or more relay nodes. The relays work in half-duplex mode, in the sense that they can not transmit and receive at the same time. For the case of one relay, the generalized Degrees-of-Freedom is characterized first and then it is shown that capacity can be achieved to within a constant gap regardless of the actual value of the channel parameters. Different achievable schemes are presented with either deterministic or random switch for the relay node. It is shown that random switch in general achieves higher rates than deterministic switch. For the case of K relays, it is shown that the generalized Degrees-of-Freedom can be obtained by solving a linear program and that capacity can be achieved to within a constant gap of K/2log(4K). This gap may be further decreased by considering more structured networks such as, for example, the diamond network. ",Computer Science - Information Theory ; ,"Cardone, Martina ; Tuninetti, Daniela ; Knopp, Raymond ; Salim, Umer ; "
http://arxiv.org/abs/1301.7023,The Capacity of Adaptive Group Testing,"  We define capacity for group testing problems and deduce bounds for the capacity of a variety of noisy models, based on the capacity of equivalent noisy communication channels. For noiseless adaptive group testing we prove an information-theoretic lower bound which tightens a bound of Chan et al. This can be combined with a performance analysis of a version of Hwang's adaptive group testing algorithm, in order to deduce the capacity of noiseless and erasure group testing models. ",Computer Science - Information Theory ; ,"Baldassini, Leonardo ; Johnson, Oliver ; Aldridge, Matthew ; "
http://arxiv.org/abs/1302.1211,Quantum Lyapunov Control Based on the Average Value of an Imaginary   Mechanical Quantity,"  The convergence of closed quantum systems in the degenerate cases to the desired target state by using the quantum Lyapunov control based on the average value of an imaginary mechanical quantity is studied. On the basis of the existing methods which can only ensure the single-control Hamiltonian systems converge toward a set, we design the control laws to make the multi-control Hamiltonian systems converge to the desired target state. The convergence of the control system is proved, and the convergence to the desired target state is analyzed. How to make these conditions of convergence to the target state to be satisfied is proved or analyzed. Finally, numerical simulations for a three level system in the degenrate case transfering form an initial eigenstate to a target superposition state are studied to verify the effectiveness of the proposed control method. ",Computer Science - Systems and Control ; Mathematical Physics ; ,"Cong, Shuang ; Meng, Fangfang ; Kuang, Sen ; "
http://arxiv.org/abs/1302.2279,Expressing Second-order Sentences in Intuitionistic Dependence Logic,"  Intuitionistic dependence logic was introduced by Abramsky and Vaananen (2009) as a variant of dependence logic under a general construction of Hodges' (trump) team semantics. It was proven that there is a translation from intuitionistic dependence logic sentences into second order logic sentences. In this paper, we prove that the other direction is also true, therefore intuitionistic dependence logic is equivalent to second order logic on the level of sentences. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Yang, Fan ; "
http://arxiv.org/abs/1302.4118,Target Estimation in Colocated MIMO Radar via Matrix Completion,"  We consider a colocated MIMO radar scenario, in which the receive antennas forward their measurements to a fusion center. Based on the received data, the fusion center formulates a matrix which is then used for target parameter estimation. When the receive antennas sample the target returns at Nyquist rate, and assuming that there are more receive antennas than targets, the data matrix at the fusion center is low-rank. When each receive antenna sends to the fusion center only a small number of samples, along with the sample index, the receive data matrix has missing elements, corresponding to the samples that were not forwarded. Under certain conditions, matrix completion techniques can be applied to recover the full receive data matrix, which can then be used in conjunction with array processing techniques, e.g., MUSIC, to obtain target information. Numerical results indicate that good target recovery can be achieved with occupancy of the receive data matrix as low as 50%. ",Computer Science - Information Theory ; Statistics - Applications ; ,"Sun, Shunqiao ; Petropulu, Athina P. ; Bajwa, Waheed U. ; "
http://arxiv.org/abs/1302.4808,Verifying the Consistency of Remote Untrusted Services with   Conflict-Free Operations,"  A group of mutually trusting clients outsources a computation service to a remote server, which they do not fully trust and that may be subject to attacks. The clients do not communicate with each other and would like to verify the correctness of the remote computation and the consistency of the server's responses. This paper presents the Conflict-free Operation verification Protocol (COP) that ensures linearizability when the server is correct and preserves fork-linearizability in any other case. All clients that observe each other's operations are consistent, in the sense that their own operations and those operations of other clients that they see are linearizable. If the server forks two clients by hiding an operation, these clients never again see operations of each other. COP supports wait-free client operations in the sense that when executed with a correct server, non-conflicting operations can run without waiting for other clients, allowing more parallelism than earlier protocols. A conflict arises when an operation causes a subsequent operation to produce a different output value for the client who runs it. The paper gives a precise model for the guarantees of COP and includes a formal analysis that these are achieved. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Cachin, Christian ; Ohrimenko, Olga ; "
http://arxiv.org/abs/1302.5906,Achieving AWGN Channel Capacity With Lattice Gaussian Coding,"  We propose a new coding scheme using only one lattice that achieves the $\frac{1}{2}\log(1+\SNR)$ capacity of the additive white Gaussian noise (AWGN) channel with lattice decoding, when the signal-to-noise ratio $\SNR>e-1$. The scheme applies a discrete Gaussian distribution over an AWGN-good lattice, but otherwise does not require a shaping lattice or dither. Thus, it significantly simplifies the default lattice coding scheme of Erez and Zamir which involves a quantization-good lattice as well as an AWGN-good lattice. Using the flatness factor, we show that the error probability of the proposed scheme under minimum mean-square error (MMSE) lattice decoding is almost the same as that of Erez and Zamir, for any rate up to the AWGN channel capacity. We introduce the notion of good constellations, which carry almost the same mutual information as that of continuous Gaussian inputs. We also address the implementation of Gaussian shaping for the proposed lattice Gaussian coding scheme. ",Computer Science - Information Theory ; ,"Ling, Cong ; Belfiore, Jean-Claude ; "
http://arxiv.org/abs/1302.6325,"A Note on ""A polynomial-time algorithm for global value numbering""","  Global Value Numbering(GVN) is a popular method for detecting redundant computations. A polynomial time algorithm for GVN is presented by Gulwani and Necula(2006). Here we present two limitations of this GVN algorithm due to which detection of certain kinds of redundancies can not be done using this algorithm. The first one is concerning the use of this algorithm in detecting some instances of the classical global common subexpressions, and the second is concerning its use in the detection of some redundancies that a local value numbering algorithm will detect. We suggest improvements that enable the algorithm to detect these kinds of redundancies as well. ",Computer Science - Programming Languages ; Computer Science - Logic in Computer Science ; ,"Nabeezath, Saleena ; Paleri, Vineeth ; "
http://arxiv.org/abs/1303.0730,Diagonalizing by Fixed-Points,"  A universal schema for diagonalization was popularized by N. S. Yanofsky (2003) in which the existence of a (diagonolized-out and contradictory) object implies the existence of a fixed-point for a certain function. It was shown that many self-referential paradoxes and diagonally proved theorems can fit in that schema. Here, we fit more theorems in the universal schema of diagonalization, such as Euclid's theorem on the infinitude of the primes and new proofs of Boolos (1997) for Cantor's theorem on the non-equinumerosity of a set with its powerset. Then, in Linear Temporal Logic, we show the non-existence of a fixed-point in this logic whose proof resembles the argument of Yablo's paradox. Thus, Yablo's paradox turns for the first time into a genuine mathematico-logical theorem in the framework of Linear Temporal Logic. Again the diagonal schema of the paper is used in this proof, and also it is shown that G. Priest's inclosure schema (1997) can fit in our universal diagonal/fixed-point schema. We also show the existence of dominating (Ackermann-like) functions (which dominate a given countable set of functions---like primitive recursives) using the schema. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 18A10, 18A15, 03B44, 03A05 ; ","Karimi, Ahmad ; Salehi, Saeed ; "
http://arxiv.org/abs/1303.0926,Injectivity w.r.t. Distribution of Elements in the Compressed Sequences   Derived from Primitive Sequences over $Z/p^eZ$,"  Let $p\geq3$ be a prime and $e\geq2$ an integer. Let $\sigma(x)$ be a primitive polynomial of degree $n$ over $Z/p^eZ$ and $G'(\sigma(x),p^e)$ the set of primitive linear recurring sequences generated by $\sigma(x)$. A compressing map $\varphi$ on $Z/p^eZ$ naturally induces a map $\hat{\varphi}$ on $G'(\sigma(x),p^e)$. For a subset $D$ of the image of $\varphi$,$\hat{\varphi}$ is called to be injective w.r.t. $D$-uniformity if the distribution of elements of $D$ in the compressed sequence implies all information of the original primitive sequence. In this correspondence, for at least $1-2(p-1)/(p^n-1)$ of primitive polynomials of degree $n$, a clear criterion on $\varphi$ is obtained to decide whether $\hat{\varphi}$ is injective w.r.t. $D$-uniformity, and the majority of maps on $Z/p^eZ$ induce injective maps on $G'(\sigma(x),p^e)$. Furthermore, a sufficient condition on $\varphi$ is given to ensure injectivity of $\hat{\varphi}$ w.r.t. $D$-uniformity. It follows from the sufficient condition that if $\sigma(x)$ is strongly primitive and the compressing map $\varphi(x)=f(x_{e-1})$, where $f(x_{e-1})$ is a permutation polynomial over $\mathbb{F}_{p}$, then $\hat{\varphi}$ is injective w.r.t. $D$-uniformity for $\emptyset\neq D\subset\mathbb{F}_{p}$. Moreover, we give three specific families of compressing maps which induce injective maps on $G'(\sigma(x),p^e)$. ","Computer Science - Information Theory ; 11T71, 11B50 ; ","Wang, Lin ; Hu, Zhi ; "
http://arxiv.org/abs/1303.2054,Mining Representative Unsubstituted Graph Patterns Using Prior   Similarity Matrix,"  One of the most powerful techniques to study protein structures is to look for recurrent fragments (also called substructures or spatial motifs), then use them as patterns to characterize the proteins under study. An emergent trend consists in parsing proteins three-dimensional (3D) structures into graphs of amino acids. Hence, the search of recurrent spatial motifs is formulated as a process of frequent subgraph discovery where each subgraph represents a spatial motif. In this scope, several efficient approaches for frequent subgraph discovery have been proposed in the literature. However, the set of discovered frequent subgraphs is too large to be efficiently analyzed and explored in any further process. In this paper, we propose a novel pattern selection approach that shrinks the large number of discovered frequent subgraphs by selecting the representative ones. Existing pattern selection approaches do not exploit the domain knowledge. Yet, in our approach we incorporate the evolutionary information of amino acids defined in the substitution matrices in order to select the representative subgraphs. We show the effectiveness of our approach on a number of real datasets. The results issued from our experiments show that our approach is able to considerably decrease the number of motifs while enhancing their interestingness. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Machine Learning ; ","Dhifli, Wajdi ; Saidi, Rabie ; Nguifo, Engelbert Mephu ; "
http://arxiv.org/abs/1303.3235,On the Entropy of Couplings,"  In this paper, some general properties of Shannon information measures are investigated over sets of probability distributions with restricted marginals. Certain optimization problems associated with these functionals are shown to be NP-hard, and their special cases are found to be essentially information-theoretic restatements of well-known computational problems, such as the SUBSET SUM and the 3-PARTITION. The notion of minimum entropy coupling is introduced and its relevance is demonstrated in information-theoretic, computational, and statistical contexts. Finally, a family of pseudometrics (on the space of discrete probability distributions) defined by these couplings is studied, in particular their relation to the total variation distance, and a new characterization of the conditional entropy is given. ","Computer Science - Information Theory ; 94A17, 60E99, 68Q17 ; ","Kovačević, Mladen ; Stanojević, Ivan ; Šenk, Vojin ; "
http://arxiv.org/abs/1303.5613,Network Detection Theory and Performance,"  Network detection is an important capability in many areas of applied research in which data can be represented as a graph of entities and relationships. Oftentimes the object of interest is a relatively small subgraph in an enormous, potentially uninteresting background. This aspect characterizes network detection as a ""big data"" problem. Graph partitioning and network discovery have been major research areas over the last ten years, driven by interest in internet search, cyber security, social networks, and criminal or terrorist activities. The specific problem of network discovery is addressed as a special case of graph partitioning in which membership in a small subgraph of interest must be determined. Algebraic graph theory is used as the basis to analyze and compare different network detection methods. A new Bayesian network detection framework is introduced that partitions the graph based on prior information and direct observations. The new approach, called space-time threat propagation, is proved to maximize the probability of detection and is therefore optimum in the Neyman-Pearson sense. This optimality criterion is compared to spectral community detection approaches which divide the global graph into subsets or communities with optimal connectivity properties. We also explore a new generative stochastic model for covert networks and analyze using receiver operating characteristics the detection performance of both classes of optimal detection techniques. ",Computer Science - Social and Information Networks ; Computer Science - Machine Learning ; Mathematics - Statistics Theory ; Physics - Physics and Society ; Statistics - Machine Learning ; ,"Smith, Steven T. ; Senne, Kenneth D. ; Philips, Scott ; Kao, Edward K. ; Bernstein, Garrett ; "
http://arxiv.org/abs/1303.5678,Interference alignment for the MIMO interference channel,"  We study vector space interference alignment for the MIMO interference channel with no time or frequency diversity, and no symbol extensions. We prove both necessary and sufficient conditions for alignment. In particular, we characterize the feasibility of alignment for the symmetric three-user channel where all users transmit along d dimensions, all transmitters have M antennas and all receivers have N antennas, as well as feasibility of alignment for the fully symmetric (M=N) channel with an arbitrary number of users.   An implication of our results is that the total degrees of freedom available in a K-user interference channel, using only spatial diversity from the multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees of freedom shown to be possible by Cadambe and Jafar with arbitrarily large time or frequency diversity.   Moving beyond the question of feasibility, we additionally discuss computation of the number of solutions using Schubert calculus in cases where there are a finite number of solutions. ",Computer Science - Information Theory ; ,"Bresler, Guy ; Cartwright, Dustin ; Tse, David ; "
http://arxiv.org/abs/1303.7037,Parameterized Complexity of Discrete Morse Theory,"  Optimal Morse matchings reveal essential structures of cell complexes which lead to powerful tools to study discrete geometrical objects, in particular discrete 3-manifolds. However, such matchings are known to be NP-hard to compute on 3-manifolds, through a reduction to the erasability problem.   Here, we refine the study of the complexity of problems related to discrete Morse theory in terms of parameterized complexity. On the one hand we prove that the erasability problem is W[P]-complete on the natural parameter. On the other hand we propose an algorithm for computing optimal Morse matchings on triangulations of 3-manifolds which is fixed-parameter tractable in the treewidth of the bipartite graph representing the adjacency of the 1- and 2-simplexes. This algorithm also shows fixed parameter tractability for problems such as erasability and maximum alternating cycle-free matching. We further show that these results are also true when the treewidth of the dual graph of the triangulated 3-manifold is bounded. Finally, we investigate the respective treewidths of simplicial and generalized triangulations of 3-manifolds. ","Computer Science - Computational Geometry ; Computer Science - Computational Complexity ; Mathematics - Geometric Topology ; 68Q17, 68Q15, 57Q15, 58E05, 68R01 ; ","Burton, Benjamin A. ; Lewiner, Thomas ; Paixão, João ; Spreer, Jonathan ; "
http://arxiv.org/abs/1304.0912,Structures Without Scattered-Automatic Presentation,"  Bruyere and Carton lifted the notion of finite automata reading infinite words to finite automata reading words with shape an arbitrary linear order L. Automata on finite words can be used to represent infinite structures, the so-called word-automatic structures. Analogously, for a linear order L there is the class of L-automatic structures. In this paper we prove the following limitations on the class of L-automatic structures for a fixed L of finite condensation rank 1+\alpha. Firstly, no scattered linear order with finite condensation rank above \omega^(\alpha+1) is L-\alpha-automatic. In particular, every L-automatic ordinal is below \omega^\omega^(\alpha+1). Secondly, we provide bounds on the (ordinal) height of well-founded order trees that are L-automatic. If \alpha is finite or L is an ordinal, the height of such a tree is bounded by \omega^{\alpha+1}. Finally, we separate the class of tree-automatic structures from that of L-automatic structures for any ordinal L: the countable atomless boolean algebra is known to be tree-automatic, but we show that it is not L-automatic. ",Computer Science - Formal Languages and Automata Theory ; Mathematics - Logic ; ,"Kartzow, Alexander ; Schlicht, Philipp ; "
http://arxiv.org/abs/1304.1572,Stable and Informative Spectral Signatures for Graph Matching,"  In this paper, we consider the approximate weighted graph matching problem and introduce stable and informative first and second order compatibility terms suitable for inclusion into the popular integer quadratic program formulation. Our approach relies on a rigorous analysis of stability of spectral signatures based on the graph Laplacian. In the case of the first order term, we derive an objective function that measures both the stability and informativeness of a given spectral signature. By optimizing this objective, we design new spectral node signatures tuned to a specific graph to be matched. We also introduce the pairwise heat kernel distance as a stable second order compatibility term; we justify its plausibility by showing that in a certain limiting case it converges to the classical adjacency matrix-based second order compatibility function. We have tested our approach on a set of synthetic graphs, the widely-used CMU house sequence, and a set of real images. These experiments show the superior performance of our first and second order compatibility terms as compared with the commonly used ones. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Hu, Nan ; Rustamov, Raif M. ; Guibas, Leonidas ; "
http://arxiv.org/abs/1304.2503,Simulating the Smart Grid,"  Major challenges for the transition of power systems do not only tackle power electronics but also communication technology, power market economy and user acceptance studies. Simulation is an important research method therein, as it helps to avoid costly failures. A common smart grid simulation platform is still missing. We introduce a conceptual model of agents in multiple flow networks. Flow networks extend the depth of established power flow analysis through use of networks of information flow and financial transactions. We use this model as a basis for comparing different power system simulators. Furthermore, a quantitative comparison of simulators is done to facilitate the decision for a suitable tool in comprehensive smart grid simulation. ",Computer Science - Systems and Control ; ,"Pöchacker, Manfred ; Sobe, Anita ; Elmenreich, Wilfried ; "
http://arxiv.org/abs/1304.2816,Asymptotic Behaviour and Ratios of Complexity in Cellular Automata,"  We study the asymptotic behaviour of symbolic computing systems, notably one-dimensional cellular automata (CA), in order to ascertain whether and at what rate the number of complex versus simple rules dominate the rule space for increasing neighbourhood range and number of symbols (or colours), and how different behaviour is distributed in the spaces of different cellular automata formalisms. Using two different measures, Shannon's block entropy and Kolmogorov complexity, the latter approximated by two different methods (lossless compressibility and block decomposition), we arrive at the same trend of larger complex behavioural fractions. We also advance a notion of asymptotic and limit behaviour for individual rules, both over initial conditions and runtimes, and we provide a formalisation of Wolfram's classification as a limit function in terms of Kolmogorov complexity. ",Nonlinear Sciences - Cellular Automata and Lattice Gases ; Computer Science - Computational Complexity ; ,"Zenil, Hector ; "
http://arxiv.org/abs/1304.3944,Smart Microgrids: Overview and Outlook,"  The idea of changing our energy system from a hierarchical design into a set of nearly independent microgrids becomes feasible with the availability of small renewable energy generators. The smart microgrid concept comes with several challenges in research and engineering targeting load balancing, pricing, consumer integration and home automation. In this paper we first provide an overview on these challenges and present approaches that target the problems identified. While there exist promising algorithms for the particular field, we see a missing integration which specifically targets smart microgrids. Therefore, we propose an architecture that integrates the presented approaches and defines interfaces between the identified components such as generators, storage, smart and \dq{dumb} devices. ",Computer Science - Emerging Technologies ; Computer Science - Computers and Society ; Computer Science - Systems and Control ; ,"Sobe, Anita ; Elmenreich, Wilfried ; "
http://arxiv.org/abs/1304.4964,Newton-Based Optimization for Kullback-Leibler Nonnegative Tensor   Factorizations,"  Tensor factorizations with nonnegative constraints have found application in analyzing data from cyber traffic, social networks, and other areas. We consider application data best described as being generated by a Poisson process (e.g., count data), which leads to sparse tensors that can be modeled by sparse factor matrices. In this paper we investigate efficient techniques for computing an appropriate canonical polyadic tensor factorization based on the Kullback-Leibler divergence function. We propose novel subproblem solvers within the standard alternating block variable approach. Our new methods exploit structure and reformulate the optimization problem as small independent subproblems. We employ bound-constrained Newton and quasi-Newton methods. We compare our algorithms against other codes, demonstrating superior speed for high accuracy results and the ability to quickly find sparse solutions. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; ,"Hansen, Samantha ; Plantenga, Todd ; Kolda, Tamara G. ; "
http://arxiv.org/abs/1304.5591,Parameterized Complexity of 1-Planarity,"  We consider the problem of finding a 1-planar drawing for a general graph, where a 1-planar drawing is a drawing in which each edge participates in at most one crossing. Since this problem is known to be NP-hard we investigate the parameterized complexity of the problem with respect to the vertex cover number, tree-depth, and cyclomatic number. For these parameters we construct fixed-parameter tractable algorithms. However, the problem remains NP-complete for graphs of bounded bandwidth, pathwidth, or treewidth. ",Computer Science - Data Structures and Algorithms ; ,"Bannister, Michael J. ; Cabello, Sergio ; Eppstein, David ; "
http://arxiv.org/abs/1304.6116,"Selling Multiple Correlated Goods: Revenue Maximization and Menu-Size   Complexity (old title: ""The Menu-Size Complexity of Auctions"")","  We consider the well known, and notoriously difficult, problem of a single revenue-maximizing seller selling two or more heterogeneous goods to a single buyer whose private values for the goods are drawn from a (possibly correlated) known distribution, and whose valuation is additive over the goods. We show that when there are two (or more) goods, _simple mechanisms_ -- such as selling the goods separately or as a bundle -- _may yield only a negligible fraction of the optimal revenue_. This resolves the open problem of Briest, Chawla, Kleinberg, and Weinberg (JET 2015) who prove the result for at least three goods in the related setup of a unit-demand buyer. We also introduce the menu size as a simple measure of the complexity of mechanisms, and show that the revenue may increase polynomially with _menu size_ and that no bounded menu size can ensure any positive fraction of the optimal revenue. The menu size also turns out to ""pin down"" the revenue properties of deterministic mechanisms. ",Computer Science - Computer Science and Game Theory ; ,"Hart, Sergiu ; Nisan, Noam ; "
http://arxiv.org/abs/1304.6896,Strongly light subgraphs in the 1-planar graphs with minimum degree 7,"  A graph is {\em $1$-planar} if it can be drawn in the plane such that every edge crosses at most one other edge. A connected graph $H$ is {\em strongly light} in a family of graphs $\mathfrak{G}$, if there exists a constant $\lambda$, such that every graph $G$ in $\mathfrak{G}$ contains a subgraph $K$ isomorphic to $H$ with $\deg_{G}(v) \leq \lambda$ for all $v \in V(K)$. In this paper, we present some strongly light subgraphs in the family of $1$-planar graphs with minimum degree~$7$. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C10 ; ,"Wang, Tao ; "
http://arxiv.org/abs/1304.7480,The Ergodic Capacity of the Multiple Access Channel Under Distributed   Scheduling - Order Optimality of Linear Receivers,"  Consider the problem of a Multiple-Input Multiple-Output (MIMO) Multiple-Access Channel (MAC) at the limit of large number of users. Clearly, in practical scenarios, only a small subset of the users can be scheduled to utilize the channel simultaneously. Thus, a problem of user selection arises. However, since solutions which collect Channel State Information (CSI) from all users and decide on the best subset to transmit in each slot do not scale when the number of users is large, distributed algorithms for user selection are advantageous.   In this paper, we analyse a distributed user selection algorithm, which selects a group of users to transmit without coordinating between users and without all users sending CSI to the base station. This threshold-based algorithm is analysed for both Zero-Forcing (ZF) and Minimum Mean Square Error (MMSE) receivers, and its expected sum-rate in the limit of large number of users is investigated. It is shown that for large number of users it achieves the same scaling laws as the optimal centralized scheme. ",Computer Science - Information Theory ; ,"Kampeas, Joseph ; Cohen, Asaf ; Gurewitz, Omer ; "
http://arxiv.org/abs/1305.0750,Multi-Sided Boundary Labeling,"  In the Boundary Labeling problem, we are given a set of $n$ points, referred to as sites, inside an axis-parallel rectangle $R$, and a set of $n$ pairwise disjoint rectangular labels that are attached to $R$ from the outside. The task is to connect the sites to the labels by non-intersecting rectilinear paths, so-called leaders, with at most one bend.   In this paper, we study the Multi-Sided Boundary Labeling problem, with labels lying on at least two sides of the enclosing rectangle. We present a polynomial-time algorithm that computes a crossing-free leader layout if one exists. So far, such an algorithm has only been known for the cases in which labels lie on one side or on two opposite sides of $R$ (here a crossing-free solution always exists). The case where labels may lie on adjacent sides is more difficult. We present efficient algorithms for testing the existence of a crossing-free leader layout that labels all sites and also for maximizing the number of labeled sites in a crossing-free leader layout. For two-sided boundary labeling with adjacent sides, we further show how to minimize the total leader length in a crossing-free layout. ",Computer Science - Computational Geometry ; ,"Kindermann, Philipp ; Niedermann, Benjamin ; Rutter, Ignaz ; Schaefer, Marcus ; Schulz, André ; Wolff, Alexander ; "
http://arxiv.org/abs/1305.2386,Disappointment in Social Choice Protocols,"  Social choice theory is a theoretical framework for analysis of combining individual preferences, interests, or welfare to reach a collective decision or social welfare in some sense. We introduce a new criterion for social choice protocols called social disappointment. Social disappointment happens when the outcome of a voting system occurs for those alternatives which are at the end of at least half of individual preference profiles. Here we introduce some protocols that prevent social disappointment and prove an impossibility theorem based on this key concept. ",Computer Science - Multiagent Systems ; 91B14 ; ,"Javidian, Mohammad Ali ; Ramezanian, Rasoul ; "
http://arxiv.org/abs/1305.2494,Computing Solution Operators of Boundary-value Problems for Some Linear   Hyperbolic Systems of PDEs,"  We discuss possibilities of application of Numerical Analysis methods to proving computability, in the sense of the TTE approach, of solution operators of boundary-value problems for systems of PDEs. We prove computability of the solution operator for a symmetric hyperbolic system with computable real coefficients and dissipative boundary conditions, and of the Cauchy problem for the same system (we also prove computable dependence on the coefficients) in a cube $Q\subseteq\mathbb R^m$. Such systems describe a wide variety of physical processes (e.g. elasticity, acoustics, Maxwell equations). Moreover, many boundary-value problems for the wave equation also can be reduced to this case, thus we partially answer a question raised in Weihrauch and Zhong (2002). Compared with most of other existing methods of proving computability for PDEs, this method does not require existence of explicit solution formulas and is thus applicable to a broader class of (systems of) equations. ","Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; 03D78, 58J45, 65M06, 65M25 ; F.1.1 ; G.1.8 ; ","Selivanova, Svetlana ; Selivanov, Victor ; "
http://arxiv.org/abs/1305.4732,Enabling Self-Powered Autonomous Wireless Sensors with New-Generation   I2C-RFID Chips,"  A self-powered autonomous RFID device with sensing and computing capabilities is presented in this paper. Powered by an RF energy-harvesting circuit enhanced by a DC-DC voltage booster in silicon-on-insulator (SOI) technology, the device relies on a microcontroller and a new generation I2C-RFID chip to wirelessly deliver sensor data to standard RFID EPC Class-1 Generation-2 (Gen2) readers. When the RF power received from the interrogating reader is -14 dBm or higher, the device, fabricated on an FR4 substrate using low-cost discrete components, is able to produce 2.4-V DC voltage to power its circuitry. The experimental results demonstrate the effectiveness of the device to perform reliable sensor data transmissions up to 5 meters in fully-passive mode. To the best of our knowledge, this represents the longest read range ever reported for passive UHF RFID sensors compliant with the EPC Gen2 standard. ",Computer Science - Other Computer Science ; ,"De Donno, D. ; Catarinucci, L. ; Tarricone, L. ; "
http://arxiv.org/abs/1305.4874,The Query Complexity of Correlated Equilibria,"  We consider the complexity of finding a correlated equilibrium of an $n$-player game in a model that allows the algorithm to make queries on players' payoffs at pure strategy profiles. Randomized regret-based dynamics are known to yield an approximate correlated equilibrium efficiently, namely, in time that is polynomial in the number of players $n$. Here we show that both randomization and approximation are necessary: no efficient deterministic algorithm can reach even an approximate correlated equilibrium, and no efficient randomized algorithm can reach an exact correlated equilibrium. The results are obtained by bounding from below the number of payoff queries that are needed. ",Computer Science - Computer Science and Game Theory ; Computer Science - Data Structures and Algorithms ; ,"Hart, Sergiu ; Nisan, Noam ; "
http://arxiv.org/abs/1305.5592,Finite-Length and Asymptotic Analysis of Correlogram for Undersampled   Data,"  This paper studies a spectrum estimation method for the case that the samples are obtained at a rate lower than the Nyquist rate. The method is referred to as the correlogram for undersampled data. The algorithm partitions the spectrum into a number of segments and estimates the average power within each spectral segment. This method is able to estimate the power spectrum density of a signal from undersampled data without essentially requiring the signal to be sparse. We derive the bias and the variance of the spectrum estimator, and show that there is a tradeoff between the accuracy of the estimation, the frequency resolution, and the complexity of the estimator. A closed-form approximation of the estimation variance is also derived, which clearly shows how the variance is related to different parameters. The asymptotic behavior of the estimator is also investigated, and it is proved that this spectrum estimator is consistent. Moreover, the estimation made for different spectral segments becomes uncorrelated as the signal length tends to infinity. Finally, numerical examples and simulation results are provided, which approve the theoretical conclusions. ",Computer Science - Information Theory ; ,"Shaghaghi, Mahdi ; Vorobyov, Sergiy A. ; "
http://arxiv.org/abs/1305.5670,What is Visualization Really for?,"  Whenever a visualization researcher is asked about the purpose of visualization, the phrase ""gaining insight"" by and large pops out instinctively. However, it is not absolutely factual that all uses of visualization are for gaining a deep understanding, unless the term insight is broadened to encompass all types of thought. Even when insight is the focus of a visualization task, it is rather difficult to know what insight is gained, how much, or how accurate. In this paper, we propose that ""saving time"" in accomplishing a user's task is the most fundamental objective. By giving emphasis to saving time, we can establish a concrete metric, alleviate unnecessary contention caused by different interpretations of insight, and stimulate new research efforts in some aspects of visualization, such as empirical studies, design optimisation and theories of visualization. ",Computer Science - Human-Computer Interaction ; ,"Chen, Min ; Floridi, Luciano ; Borgo, Rita ; "
http://arxiv.org/abs/1305.6431,Certifying Machine Code Safe from Hardware Aliasing: RISC is not   necessarily risky,"  Sometimes machine code turns out to be a better target for verification than source code. RISC machine code is especially advantaged with respect to source code in this regard because it has only two instructions that access memory. That architecture forms the basis here for an inference system that can prove machine code safe against `hardware aliasing', an effect that occurs in embedded systems. There are programming memes that ensure code is safe from hardware aliasing, but we want to certify that a given machine code is provably safe. ",Computer Science - Logic in Computer Science ; Computer Science - Software Engineering ; D.2.4 ; ,"Breuer, Peter T. ; Bowen, Jonathan P. ; "
http://arxiv.org/abs/1305.7514,Studying new classes of graph metrics,"  In data analysis, there is a strong demand for graph metrics that differ from the classical shortest path and resistance distances. Recently, several new classes of graph metrics have been proposed. This paper presents some of them featuring the cutpoint additive distances. These include the path distances, the reliability distance, the walk distances, and the logarithmic forest distances among others. We discuss a number of connections between these and other distances. ",Mathematics - Metric Geometry ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05C12 05C50 05C05 51K05 15A48 15A51 ; ,"Chebotarev, Pavel ; "
http://arxiv.org/abs/1306.0760,Mashup of Meta-Languages and its Implementation in the Kermeta Language   Workbench,"  With the growing use of domain-specific languages (DSL) in industry, DSL design and implementation goes far beyond an activity for a few experts only and becomes a challenging task for thousands of software engineers. DSL implementation indeed requires engineers to care for various concerns, from abstract syntax, static semantics, behavioral semantics, to extra-functional issues such as run-time performance. This paper presents an approach that uses one meta-language per language implementation concern. We show that the usage and combination of those meta-languages is simple and intuitive enough to deserve the term ""mashup"". We evaluate the approach by completely implementing the non trivial fUML modeling language, a semantically sound and executable subset of the Unified Modeling Language (UML). ",Computer Science - Software Engineering ; ,"Jézéquel, Jean-Marc ; Combemale, Benoit ; Barais, Olivier ; Monperrus, Martin ; Fouquet, François ; "
http://arxiv.org/abs/1306.1167,A Graphical Transformation for Belief Propagation: Maximum Weight   Matchings and Odd-Sized Cycles,"  We study the Maximum Weight Matching (MWM) problem for general graphs through the max-product Belief Propagation (BP) and related Linear Programming (LP). The BP approach provides distributed heuristics for finding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM) and respective LPs can be considered as continuous relaxations of the discrete MAP problem. It was recently shown that a BP algorithm converges to the correct MWM assignment under a simple GM formulation of MAP/MWM as long as the corresponding LP relaxation is tight. First, under the motivation for forcing the tightness condition, we consider a new GM formulation of MWM, say C-GM, using non-intersecting odd-sized cycles in the graph: the new corresponding LP relaxation, say C-LP, becomes tight for more MWM instances. However, the tightness of C-LP now does not guarantee such convergence and correctness of the new BP on C-GM. To address the issue, we introduce a novel graph transformation applied to C-GM, which results in another GM formulation of MWM, and prove that the respective BP on it converges to the correct MAP/MWM assignment as long as C-LP is tight. Finally, we also show that C-LP always has half-integral solutions, which leads to an efficient BP-based MWM heuristic consisting of making sequential, `cutting plane', modifications to the underlying GM. Our experiments show that this BP-based cutting plane heuristic performs as well as that based on traditional LP solvers. ",Computer Science - Data Structures and Algorithms ; ,"Ahn, Sungsoo ; Chertkov, Michael ; Gelfand, Andrew E. ; Park, Sejun ; Shin, Jinwoo ; "
http://arxiv.org/abs/1306.1595,Layered Separators in Minor-Closed Graph Classes with Applications,"  Graph separators are a ubiquitous tool in graph theory and computer science. However, in some applications, their usefulness is limited by the fact that the separator can be as large as $\Omega(\sqrt{n})$ in graphs with $n$ vertices. This is the case for planar graphs, and more generally, for proper minor-closed classes. We study a special type of graph separator, called a ""layered separator"", which may have linear size in $n$, but has bounded size with respect to a different measure, called the ""width"". We prove, for example, that planar graphs and graphs of bounded Euler genus admit layered separators of bounded width. More generally, we characterise the minor-closed classes that admit layered separators of bounded width as those that exclude a fixed apex graph as a minor.   We use layered separators to prove $\mathcal{O}(\log n)$ bounds for a number of problems where $\mathcal{O}(\sqrt{n})$ was a long-standing previous best bound. This includes the nonrepetitive chromatic number and queue-number of graphs with bounded Euler genus. We extend these results with a $\mathcal{O}(\log n)$ bound on the nonrepetitive chromatic number of graphs excluding a fixed topological minor, and a $\log^{ \mathcal{O}(1)}n$ bound on the queue-number of graphs excluding a fixed minor. Only for planar graphs were $\log^{ \mathcal{O}(1)}n$ bounds previously known. Our results imply that every $n$-vertex graph excluding a fixed minor has a 3-dimensional grid drawing with $n\log^{ \mathcal{O}(1)}n$ volume, whereas the previous best bound was $\mathcal{O}(n^{3/2})$. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; ,"Dujmović, Vida ; Morin, Pat ; Wood, David R. ; "
http://arxiv.org/abs/1306.2476,A Systematically Empirical Evaluation of Vulnerability Discovery Models:   a Study on Browsers' Vulnerabilities,"  A precise vulnerability discovery model (VDM) will provide a useful insight to assess software security, and could be a good prediction instrument for both software vendors and users to understand security trends and plan ahead patching schedule accordingly. Thus far, several models have been proposed and validated. Yet, no systematically independent validation by somebody other than the author exists. Furthermore, there are a number of issues that might bias previous studies in the field. In this work, we fill in the gap by introducing an empirical methodology that systematically evaluates the performance of a VDM in two aspects: quality and predictability. We further apply this methodology to assess existing VDMs. The results show that some models should be rejected outright, while some others might be adequate to capture the discovery process of vulnerabilities. We also consider different usage scenarios of VDMs and find that the simplest linear model is the most appropriate choice in terms of both quality and predictability when browsers are young. Otherwise, logistics-based models are better choices. ",Computer Science - Cryptography and Security ; ,"Nguyen, Viet Hung ; Massacci, Fabio ; "
http://arxiv.org/abs/1306.2595,Capacity Scaling in MIMO Systems with General Unitarily Invariant Random   Matrices,"  We investigate the capacity scaling of MIMO systems with the system dimensions. To that end, we quantify how the mutual information varies when the number of antennas (at either the receiver or transmitter side) is altered. For a system comprising $R$ receive and $T$ transmit antennas with $R>T$, we find the following: By removing as many receive antennas as needed to obtain a square system (provided the channel matrices before and after the removal have full rank) the maximum resulting loss of mutual information over all signal-to-noise ratios (SNRs) depends only on $R$, $T$ and the matrix of left-singular vectors of the initial channel matrix, but not on its singular values. In particular, if the latter matrix is Haar distributed the ergodic rate loss is given by $\sum_{t=1}^{T}\sum_{r=T+1}^{R}\frac{1}{r-t}$ nats. Under the same assumption, if $T,R\to \infty$ with the ratio $\phi\triangleq T/R$ fixed, the rate loss normalized by $R$ converges almost surely to $H(\phi)$ bits with $H(\cdot)$ denoting the binary entropy function. We also quantify and study how the mutual information as a function of the system dimensions deviates from the traditionally assumed linear growth in the minimum of the system dimensions at high SNR. ",Computer Science - Information Theory ; ,"Çakmak, Burak ; Müller, Ralf R. ; Fleury, Bernard H. ; "
http://arxiv.org/abs/1306.3261,arXiv e-prints and the journal of record: An analysis of roles and   relationships,"  Since its creation in 1991, arXiv has become central to the diffusion of research in a number of fields. Combining data from the entirety of arXiv and the Web of Science (WoS), this paper investigates (a) the proportion of papers across all disciplines that are on arXiv and the proportion of arXiv papers that are in the WoS, (b) elapsed time between arXiv submission and journal publication, and (c) the aging characteristics and scientific impact of arXiv e-prints and their published version. It shows that the proportion of WoS papers found on arXiv varies across the specialties of physics and mathematics, and that only a few specialties make extensive use of the repository. Elapsed time between arXiv submission and journal publication has shortened but remains longer in mathematics than in physics. In physics, mathematics, as well as in astronomy and astrophysics, arXiv versions are cited more promptly and decay faster than WoS papers. The arXiv versions of papers - both published and unpublished - have lower citation rates than published papers, although there is almost no difference in the impact of the arXiv versions of both published and unpublished papers. ",Computer Science - Digital Libraries ; ,"Lariviere, Vincent ; Sugimoto, Cassidy R. ; Macaluso, Benoit ; Milojevic, Stasa ; Cronin, Blaise ; Thelwall, Mike ; "
http://arxiv.org/abs/1306.3726,"Automatic functions, linear time and learning","  The present work determines the exact nature of {\em linear time computable} notions which characterise automatic functions (those whose graphs are recognised by a finite automaton). The paper also determines which type of linear time notions permit full learnability for learning in the limit of automatic classes (families of languages which are uniformly recognised by a finite automaton). In particular it is shown that a function is automatic iff there is a one-tape Turing machine with a left end which computes the function in linear time where the input before the computation and the output after the computation both start at the left end. It is known that learners realised as automatic update functions are restrictive for learning. In the present work it is shown that one can overcome the problem by providing work tapes additional to a resource-bounded base tape while keeping the update-time to be linear in the length of the largest datum seen so far. In this model, one additional such work tape provides additional learning power over the automatic learner model and two additional work tapes give full learning power. Furthermore, one can also consider additional queues or additional stacks in place of additional work tapes and for these devices, one queue or two stacks are sufficient for full learning power while one stack is insufficient. ",Computer Science - Formal Languages and Automata Theory ; ,"Case, John ; Jain, Sanjay ; Seah, Samuel ; Stephan, Frank ; "
http://arxiv.org/abs/1306.3875,Roughening Methods to Prevent Sample Impoverishment in the Particle PHD   Filter,"  Mahler's PHD (Probability Hypothesis Density) filter and its particle implementation (as called the particle PHD filter) have gained popularity to solve general MTT (Multi-target Tracking) problems. However, the resampling procedure used in the particle PHD filter can cause sample impoverishment. To rejuvenate the diversity of particles, two easy-to-implement roughening approaches are presented to enhance the particle PHD filter. One termed as ""separate-roughening"" is inspired by Gordon's roughening procedure that is applied on the resampled particles. Another termed as ""direct-roughening"" is implemented by increasing the simulation noise of the state propagation of particles. Four proposals are presented to customize the roughening approach. Simulations are presented showing that the roughening approach can benefit the particle PHD filter, especially when the sample size is small. ",Computer Science - Other Computer Science ; ,"Li, Tiancheng ; Sattar, Tariq P. ; Han, Qing ; Sun, Shudong ; "
http://arxiv.org/abs/1306.4664,Efficient Two-Stage Group Testing Algorithms for Genetic Screening,"  Efficient two-stage group testing algorithms that are particularly suited for rapid and less-expensive DNA library screening and other large scale biological group testing efforts are investigated in this paper. The main focus is on novel combinatorial constructions in order to minimize the number of individual tests at the second stage of a two-stage disjunctive testing procedure. Building on recent work by Levenshtein (2003) and Tonchev (2008), several new infinite classes of such combinatorial designs are presented. ",Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; Quantitative Biology - Quantitative Methods ; ,"Huber, Michael ; "
http://arxiv.org/abs/1306.5111,Low-Density Parity-Check Codes From Transversal Designs With Improved   Stopping Set Distributions,"  This paper examines the construction of low-density parity-check (LDPC) codes from transversal designs based on sets of mutually orthogonal Latin squares (MOLS). By transferring the concept of configurations in combinatorial designs to the level of Latin squares, we thoroughly investigate the occurrence and avoidance of stopping sets for the arising codes. Stopping sets are known to determine the decoding performance over the binary erasure channel and should be avoided for small sizes. Based on large sets of simple-structured MOLS, we derive powerful constraints for the choice of suitable subsets, leading to improved stopping set distributions for the corresponding codes. We focus on LDPC codes with column weight 4, but the results are also applicable for the construction of codes with higher column weights. Finally, we show that a subclass of the presented codes has quasi-cyclic structure which allows low-complexity encoding. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Gruner, Alexander ; Huber, Michael ; "
http://arxiv.org/abs/1306.5585,Soundness and Completeness of the NRB Verification Logic,"  This short paper gives a model for and a proof of completeness of the NRB verification logic for deterministic imperative programs, the logic having been used in the past as the basis for automated semantic checks of large, fast-changing, open source C code archives, such as that of the Linux kernel source. The model is a colored state transitions model that approximates from above the set of transitions possible for a program. Correspondingly, the logic catches all traces that may trigger a particular defect at a given point in the program, but may also flag false positives. ",Computer Science - Logic in Computer Science ; B.1.2 ; D.2.4 ; ,"Breuer, Peter T. ; Pickin, Simon J. ; "
http://arxiv.org/abs/1306.5720,On the Resilience of Bipartite Networks,"  Motivated by problems modeling the spread of infections in networks, in this paper we explore which bipartite graphs are most resilient to widespread infections under various parameter settings. Namely, we study bipartite networks with a requirement of a minimum degree $d$ on one side under an independent infection, independent transmission model. We completely characterize the optimal graphs in the case $d=1$, which already produces non-trivial behavior, and we give extremal results for the more general cases. We show that in the case $d=2$, surprisingly, the optimally resilient set of graphs includes a graph that is not one of the two ""extremes"" found in the case $d=1$.   Then, we briefly examine the case where we force a connectivity requirement instead of a one-sided degree requirement and again, we find that the set of the most resilient graphs contains more than the two ""extremes."" We also show that determining the subgraph of an arbitrary bipartite graph most resilient to infection is NP-hard for any one-sided minimal degree $d \ge 1$. ",Computer Science - Data Structures and Algorithms ; Computer Science - Social and Information Networks ; ,"Heinecke, Shelby ; Perkins, Will ; Reyzin, Lev ; "
http://arxiv.org/abs/1306.6109,Broadcasting in Ad Hoc Multiple Access Channels,"  We study broadcast in multiple access channels in dynamic adversarial settings. There is an unbounded supply of anonymous stations attached to a synchronous channel. There is an adversary who injects packets into stations to be broadcast on the channel. The adversary is restricted by injection rate, burstiness, and by how many passive stations can be simultaneously activated by providing them with packets. We consider deterministic distributed broadcast algorithms, which are further categorized by their properties. We investigate for which injection rates can algorithms attain bounded packet latency, when adversaries are restricted to be able to activate at most one station per round. The rates of algorithms we present make the increasing sequence consisting of $\frac{1}{3}$, $\frac{3}{8}$ and $\frac{1}{2}$, reflecting the additional features of algorithms. We show that injection rate $\frac{3}{4}$ cannot be handled with bounded packet latency. ",Computer Science - Networking and Internet Architecture ; ,"Anantharamu, Lakshmi ; Chlebus, Bogdan S. ; "
http://arxiv.org/abs/1306.6458,Harmony Perception by Periodicity Detection,"  The perception of consonance/dissonance of musical harmonies is strongly correlated with their periodicity. This is shown in this article by consistently applying recent results from psychophysics and neuroacoustics, namely that the just noticeable difference between pitches for humans is about 1% for the musically important low frequency range and that periodicities of complex chords can be detected in the human brain. Based thereon, the concepts of relative and logarithmic periodicity with smoothing are introduced as powerful measures of harmoniousness. The presented results correlate significantly with empirical investigations on the perception of chords. Even for scales, plausible results are obtained. For example, all classical church modes appear in the front ranks of all theoretically possible seven-tone scales. ",Computer Science - Sound ; ,"Stolzenburg, Frieder ; "
http://arxiv.org/abs/1307.0426,"An Empirical Study into Annotator Agreement, Ground Truth Estimation,   and Algorithm Evaluation","  Although agreement between annotators has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of computer vision (CV) object detection algorithms. Many researchers utilise ground truth (GT) in experiments and more often than not this GT is derived from one annotator's opinion. How does the difference in opinion affect an algorithm's evaluation? Four examples of typical CV problems are chosen, and a methodology is applied to each to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of GT. It is found that when detecting linear objects annotator agreement is very low. The agreement in object position, linear or otherwise, can be partially explained through basic image properties. Automatic object detectors are compared to annotator agreement and it is found that a clear relationship exists. Several methods for calculating GTs from a number of annotations are applied and the resulting differences in the performance of the object detectors are quantified. It is found that the rank of a detector is highly dependent upon the method used to form the GT. It is also found that although the STAPLE and LSML GT estimation methods appear to represent the mean of the performance measured using the individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted annotation combination methods--consensus voting--accentuates more obvious features, which results in an overestimation of the algorithm's performance. Finally, it is concluded that in some datasets it may not be possible to state with any confidence that one algorithm outperforms another when evaluating upon one GT and a method for calculating confidence bounds is discussed. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; I.4.6 ; I.5.4 ; ,"Lampert, Thomas A. ; Stumpf, André ; Gançarski, Pierre ; "
http://arxiv.org/abs/1307.0449,Arising information regularities in an observer,"  The approach defines information process from probabilistic observation, emerging microprocess,qubit, encoding bits, evolving macroprocess, and extends to Observer information self-organization, cognition, intelligence and understanding communicating information. Studying information originating in quantum process focuses not on particle physics but on natural interactive impulse modeling Bit composing information observer. Information emerges from Kolmogorov probabilities field when sequences of 1-0 probabilities link Markov probabilities modeling arising observer. These objective yes-no probabilities virtually cuts observing entropy hidden in cutting correlation decreasing Markov process entropy and increasing entropy of cutting impulse running minimax principle. Merging impulse curves and rotates yes-no conjugated entropies in microprocess. The entropies entangle within impulse time interval ending with beginning space. The opposite curvature lowers potential energy converting entropy to memorized bit. The memorized information binds reversible microprocess with irreversible information macroprocess. Multiple interacting Bits self-organize information process encoding causality, logic and complexity. Trajectory of observation process carries probabilistic and certain wave function self-building structural macrounits. Macrounits logically self-organize information networks encoding in triplet code. Multiple IN enclose observer information cognition and intelligence. Observer cognition assembles attracting common units in resonances forming IN hierarchy accepting only units recognizing IN node. Maximal number of accepted triplets measures the observer information intelligence. Intelligent observer recognizes and encodes digital images in message transmission enables understanding the message meaning. Cognitive logic self-controls encoding the intelligence in double helix code. ","Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Computer Science - Information Theory ; 58J65, 60J65, 93B52, 93E02, 93E15, 93E30 ; H.1.1 ; ","Lerner, Vladimir S. ; "
http://arxiv.org/abs/1307.2035,Periodic Strategies: A New Solution Concept and an Algorithm for   NonTrivial Strategic Form Games,"  We introduce a new solution concept, called periodicity, for selecting optimal strategies in strategic form games. This periodicity solution concept yields new insight into non-trivial games. In mixed strategy strategic form games, periodic solutions yield values for the utility function of each player that are equal to the Nash equilibrium ones. In contrast to the Nash strategies, here the payoffs of each player are robust against what the opponent plays. Sometimes, periodicity strategies yield higher utilities, and sometimes the Nash strategies do, but often the utilities of these two strategies coincide. We formally define and study periodic strategies in two player perfect information strategic form games with pure strategies and we prove that every non-trivial finite game has at least one periodic strategy, with non-trivial meaning non-degenerate payoffs. In some classes of games where mixed strategies are used, we identify quantitative features. Particularly interesting are the implications for collective action games, since there the collective action strategy can be incorporated in a purely non-cooperative context. Moreover, we address the periodicity issue when the players have a continuum set of strategies available. ",Computer Science - Computer Science and Game Theory ; ,"Oikonomou, V. K. ; Jost, J. ; "
http://arxiv.org/abs/1307.2559,General Drift Analysis with Tail Bounds,"  Drift analysis is one of the state-of-the-art techniques for the runtime analysis of randomized search heuristics (RSHs) such as evolutionary algorithms (EAs), simulated annealing etc. The vast majority of existing drift theorems yield bounds on the expected value of the hitting time for a target state, e.g., the set of optimal solutions, without making additional statements on the distribution of this time. We address this lack by providing a general drift theorem that includes bounds on the upper and lower tail of the hitting time distribution. The new tail bounds are applied to prove very precise sharp-concentration results on the running time of a simple EA on standard benchmark problems, including the class of general linear functions. Surprisingly, the probability of deviating by an $r$-factor in lower order terms of the expected time decreases exponentially with $r$ on all these problems. The usefulness of the theorem outside the theory of RSHs is demonstrated by deriving tail bounds on the number of cycles in random permutations. All these results handle a position-dependent (variable) drift that was not covered by previous drift theorems with tail bounds. Moreover, our theorem can be specialized into virtually all existing drift theorems with drift towards the target from the literature. Finally, user-friendly specializations of the general drift theorem are given. ",Computer Science - Neural and Evolutionary Computing ; 68W20 ; ,"Lehre, Per Kristian ; Witt, Carsten ; "
http://arxiv.org/abs/1307.2783,Coping with Unreliable Workers in Internet-based Computing: An   Evaluation of Reputation Mechanisms,"  We present reputation-based mechanisms for building reliable task computing systems over the Internet. The most characteristic examples of such systems are the volunteer computing and the crowdsourcing platforms. In both examples end users are offering over the Internet their computing power or their human intelligence to solve tasks either voluntarily or under payment. While the main advantage of these systems is the inexpensive computational power provided, the main drawback is the untrustworthy nature of the end users. Generally, this type of systems are modeled under the ""master-worker"" setting. A ""master"" has a set of tasks to compute and instead of computing them locally she sends these tasks to available ""workers"" that compute and report back the task results. We categorize these workers in three generic types: altruistic, malicious and rational. Altruistic workers that always return the correct result, malicious workers that always return an incorrect result, and rational workers that decide to reply or not truthfully depending on what increases their benefit. We design a reinforcement learning mechanism to induce a correct behavior to rational workers, while the mechanism is complemented by four reputation schemes that cope with malice. The goal of the mechanism is to reach a state of eventual correctness, that is, a stable state of the system in which the master always obtains the correct task results. Analysis of the system gives provable guarantees under which truthful behavior can be ensured. Finally, we observe the behavior of the mechanism through simulations that use realistic system parameters values. Simulations not only agree with the analysis but also reveal interesting trade-offs between various metrics and parameters. Finally, the four reputation schemes are assessed against the tolerance to cheaters. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Computer Science and Game Theory ; 68Q85 ; ","Christoforou, Evgenia ; Anta, Antonio Fernandez ; Georgiou, Chryssis ; Mosteiro, Miguel A. ; Sanchez, Angel ; "
http://arxiv.org/abs/1307.2968,Introduction to Queueing Theory and Stochastic Teletraffic Models,"  The aim of this textbook is to provide students with basic knowledge of stochastic models that may apply to telecommunications research areas, such as traffic modelling, resource provisioning and traffic management. These study areas are often collectively called teletraffic. This book assumes prior knowledge of a programming language, mathematics, probability and stochastic processes normally taught in an electrical engineering course. For students who have some but not sufficiently strong background in probability and stochastic processes, we provide, in the first few chapters, background on the relevant concepts in these areas. ",Mathematics - Probability ; Computer Science - Information Theory ; ,"Zukerman, Moshe ; "
http://arxiv.org/abs/1307.3142,Perfect Codes in the Discrete Simplex,"  We study the problem of existence of (nontrivial) perfect codes in the discrete $ n $-simplex $ \Delta_{\ell}^n := \left\{ \begin{pmatrix} x_0, \ldots, x_n \end{pmatrix} : x_i \in \mathbb{Z}_{+}, \sum_i x_i = \ell \right\} $ under $ \ell_1 $ metric. The problem is motivated by the so-called multiset codes, which have recently been introduced by the authors as appropriate constructs for error correction in the permutation channels. It is shown that $ e $-perfect codes in the $ 1 $-simplex $ \Delta_{\ell}^1 $ exist for any $ \ell \geq 2e + 1 $, the $ 2 $-simplex $ \Delta_{\ell}^2 $ admits an $ e $-perfect code if and only if $ \ell = 3e + 1 $, while there are no perfect codes in higher-dimensional simplices. In other words, perfect multiset codes exist only over binary and ternary alphabets. ","Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; 94B25, 05B40, 52C17, 05C12, 68R99 ; ","Kovačević, Mladen ; Vukobratović, Dejan ; "
http://arxiv.org/abs/1307.3544,Distributed Bayesian Detection with Byzantine Data,"  In this paper, we consider the problem of distributed Bayesian detection in the presence of Byzantines in the network. It is assumed that a fraction of the nodes in the network are compromised and reprogrammed by an adversary to transmit false information to the fusion center (FC) to degrade detection performance. The problem of distributed detection is formulated as a binary hypothesis test at the FC based on 1-bit data sent by the sensors. The expression for minimum attacking power required by the Byzantines to blind the FC is obtained. More specifically, we show that above a certain fraction of Byzantine attackers in the network, the detection scheme becomes completely incapable of utilizing the sensor data for detection. We analyze the problem under different attacking scenarios and derive results for different non-asymptotic cases. It is found that existing asymptotics-based results do not hold under several non-asymptotic scenarios. When the fraction of Byzantines is not sufficient to blind the FC, we also provide closed form expressions for the optimal attacking strategies for the Byzantines that most degrade the detection performance. ","Computer Science - Information Theory ; Computer Science - Cryptography and Security ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Computer Science and Game Theory ; Statistics - Applications ; ","Kailkhura, Bhavya ; Han, Yunghsiang S. ; Brahma, Swastik ; Varshney, Pramod K. ; "
http://arxiv.org/abs/1307.4062,Empirical Evidence of Large-Scale Diversity in API Usage of   Object-Oriented Software,"  In this paper, we study how object-oriented classes are used across thousands of software packages. We concentrate on ""usage diversity'"", defined as the different statically observable combinations of methods called on the same object. We present empirical evidence that there is a significant usage diversity for many classes. For instance, we observe in our dataset that Java's String is used in 2460 manners. We discuss the reasons of this observed diversity and the consequences on software engineering knowledge and research. ",Computer Science - Software Engineering ; ,"Mendez, Diego ; Baudry, Benoit ; Monperrus, Martin ; "
http://arxiv.org/abs/1307.4355,Near Linear Time Approximation Schemes for Uncapacitated and Capacitated   b--Matching Problems in Nonbipartite Graphs,"  We present the first near optimal approximation schemes for the   maximum weighted (uncapacitated or capacitated) $b$--matching   problems for non-bipartite graphs that run in time (near) linear in   the number of edges. For any $\delta>3/\sqrt{n}$ the algorithm   produces a $(1-\delta)$ approximation in $O(m \poly(\delta^{-1},\log   n))$ time. We provide fractional solutions for the standard linear   programming formulations for these problems and subsequently also   provide (near) linear time approximation schemes   for rounding the fractional solutions.   Through these problems as a vehicle, we also present several ideas   in the context of solving linear programs approximately using fast   primal-dual algorithms. First, even though the dual of these   problems have exponentially many variables and an efficient exact   computation of dual weights is infeasible, we show that we can   efficiently compute and use a sparse approximation of the dual   weights using a combination of (i) adding perturbation to the   constraints of the polytope and (ii) amplification followed by   thresholding of the dual weights. Second, we show that   approximation algorithms can be used to reduce the width of the   formulation, and faster convergence. ",Computer Science - Data Structures and Algorithms ; ,"Ahn, Kook Jin ; Guha, Sudipto ; "
http://arxiv.org/abs/1307.5001,On Lower Complexity Bounds for Large-Scale Smooth Convex Optimization,"  We derive lower bounds on the black-box oracle complexity of large-scale smooth convex minimization problems, with emphasis on minimizing smooth (with Holder continuous, with a given exponent and constant, gradient) convex functions over high-dimensional ||.||_p-balls, 1<=p<=\infty. Our bounds turn out to be tight (up to logarithmic in the design dimension factors), and can be viewed as a substantial extension of the existing lower complexity bounds for large-scale convex minimization covering the nonsmooth case and the 'Euclidean' smooth case (minimization of convex functions with Lipschitz continuous gradients over Euclidean balls). As a byproduct of our results, we demonstrate that the classical Conditional Gradient algorithm is near-optimal, in the sense of Information-Based Complexity Theory, when minimizing smooth convex functions over high-dimensional ||.||_\infty-balls and their matrix analogies -- spectral norm balls in the spaces of square matrices. ",Mathematics - Optimization and Control ; Computer Science - Computational Complexity ; ,"Guzman, Cristobal ; Nemirovski, Arkadi ; "
http://arxiv.org/abs/1307.6033,Sparse Reconstruction-based Detection of Spatial Dimension Holes in   Cognitive Radio Networks,"  In this paper, we investigate a spectrum sensing algorithm for detecting spatial dimension holes in Multiple Inputs Multiple Outputs (MIMO) transmissions for OFDM systems using Compressive Sensing (CS) tools. This extends the energy detector to allow for detecting transmission opportunities even if the band is already energy filled. We show that the task described above is not performed efficiently by regular MIMO decoders (such as MMSE decoder) due to possible sparsity in the transmit signal. Since CS reconstruction tools take into account the sparsity order of the signal, they are more efficient in detecting the activity of the users. Building on successful activity detection by the CS detector, we show that the use of a CS-aided MMSE decoders yields better performance rather than using either CS-based or MMSE decoders separately. Simulations are conducted to verify the gains from using CS detector for Primary user activity detection and the performance gain in using CS-aided MMSE decoders for decoding the PU information for future relaying. ",Computer Science - Information Theory ; Computer Science - Networking and Internet Architecture ; Mathematics - Optimization and Control ; ,"Ezzeldin, Yahya H. ; Sultan, Radwa A. ; Seddik, Karim G. ; "
http://arxiv.org/abs/1307.6864,Convex recovery from interferometric measurements,"  This note formulates a deterministic recovery result for vectors $x$ from quadratic measurements of the form $(Ax)_i \overline{(Ax)_j}$ for some left-invertible $A$. Recovery is exact, or stable in the noisy case, when the couples $(i,j)$ are chosen as edges of a well-connected graph. One possible way of obtaining the solution is as a feasible point of a simple semidefinite program. Furthermore, we show how the proportionality constant in the error estimate depends on the spectral gap of a data-weighted graph Laplacian. Such quadratic measurements have found applications in phase retrieval, angular synchronization, and more recently interferometric waveform inversion. ",Mathematics - Numerical Analysis ; Computer Science - Information Theory ; Mathematics - Optimization and Control ; ,"Demanet, Laurent ; Jugnon, Vincent ; "
http://arxiv.org/abs/1307.7050,A Comprehensive Evaluation of Machine Learning Techniques for Cancer   Class Prediction Based on Microarray Data,"  Prostate cancer is among the most common cancer in males and its heterogeneity is well known. Its early detection helps making therapeutic decision. There is no standard technique or procedure yet which is full-proof in predicting cancer class. The genomic level changes can be detected in gene expression data and those changes may serve as standard model for any random cancer data for class prediction. Various techniques were implied on prostate cancer data set in order to accurately predict cancer class including machine learning techniques. Huge number of attributes and few number of sample in microarray data leads to poor machine learning, therefore the most challenging part is attribute reduction or non significant gene reduction. In this work we have compared several machine learning techniques for their accuracy in predicting the cancer class. Machine learning is effective when number of attributes (genes) are larger than the number of samples which is rarely possible with gene expression data. Attribute reduction or gene filtering is absolutely required in order to make the data more meaningful as most of the genes do not participate in tumor development and are irrelevant for cancer prediction. Here we have applied combination of statistical techniques such as inter-quartile range and t-test, which has been effective in filtering significant genes and minimizing noise from data. Further we have done a comprehensive evaluation of ten state-of-the-art machine learning techniques for their accuracy in class prediction of prostate cancer. Out of these techniques, Bayes Network out performed with an accuracy of 94.11% followed by Navie Bayes with an accuracy of 91.17%. To cross validate our results, we modified our training dataset in six different way and found that average sensitivity, specificity, precision and accuracy of Bayes Network is highest among all other techniques used. ","Computer Science - Machine Learning ; Computer Science - Computational Engineering, Finance, and Science ; ","Raza, Khalid ; Hasan, Atif N ; "
http://arxiv.org/abs/1307.7087,Correcting Grain-Errors in Magnetic Media,"  This paper studies new bounds and constructions that are applicable to the combinatorial granular channel model previously introduced by Sharov and Roth. We derive new bounds on the maximum cardinality of a grain-error-correcting code and propose constructions of codes that correct grain-errors. We demonstrate that a permutation of the classical group codes (e.g., Constantin-Rao codes) can correct a single grain-error. In many cases of interest, our results improve upon the currently best known bounds and constructions. Some of the approaches adopted in the context of grain-errors may have application to other channel models. ",Computer Science - Information Theory ; ,"Gabrys, Ryan ; Yaakobi, Eitan ; Dolecek, Lara ; "
http://arxiv.org/abs/1307.7430,Holographic Algorithms Beyond Matchgates,"  Holographic algorithms introduced by Valiant are composed of two ingredients: matchgates, which are gadgets realizing local constraint functions by weighted planar perfect matchings, and holographic reductions, which show equivalences among problems with different descriptions via certain basis transformations. In this paper, we replace matchgates in the paradigm above by the affine type and the product type constraint functions, which are known to be tractable in general (not necessarily planar) graphs. More specifically, we present polynomial-time algorithms to decide if a given counting problem has a holographic reduction to another problem defined by the affine or product-type functions. Our algorithms also find a holographic transformation when one exists. We further present polynomial-time algorithms of the same decision and search problems for symmetric functions, where the complexity is measured in terms of the (exponentially more) succinct representations. The algorithm for the symmetric case also shows that the recent dichotomy theorem for Holant problems with symmetric constraints is efficiently decidable. Our proof techniques are mainly algebraic, e.g., using stabilizers and orbits of group actions. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; 68Q25 ; F.2.1 ; G.2.1 ; ,"Cai, Jin-Yi ; Guo, Heng ; Williams, Tyson ; "
http://arxiv.org/abs/1307.8371,The Power of Localization for Efficiently Learning Linear Separators   with Noise,"  We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise, and demonstrate its effectiveness by designing algorithms with improved noise tolerance guarantees for learning linear separators.   We consider both the malicious noise model and the adversarial label noise model. For malicious noise, where the adversary can corrupt both the label and the features, we provide a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can tolerate a nearly information-theoretically optimal noise rate of $\eta = \Omega(\epsilon)$. For the adversarial label noise model, where the distribution over the feature vectors is unchanged, and the overall probability of a noisy label is constrained to be at most $\eta$, we also give a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can handle a noise rate of $\eta = \Omega\left(\epsilon\right)$.   We show that, in the active learning model, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is polylogarithmic. This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of malicious noise or adversarial label noise. ",Computer Science - Machine Learning ; Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Statistics - Machine Learning ; F.2 ; ,"Awasthi, Pranjal ; Balcan, Maria Florina ; Long, Philip M. ; "
http://arxiv.org/abs/1308.0497,"A note on T\""uring's 1936","  T\""uring's argument that there can be no machine computing the diagonal on the enumeration of the computable sequences is not a demonstration. ","Computer Science - Computational Complexity ; 03D10, 68Qxx ; ","Cattabriga, Paola ; "
http://arxiv.org/abs/1308.0776,Dynamic Approximate All-Pairs Shortest Paths: Breaking the O(mn) Barrier   and Derandomization,"  We study dynamic $(1+\epsilon)$-approximation algorithms for the all-pairs shortest paths problem in unweighted undirected $n$-node $m$-edge graphs under edge deletions. The fastest algorithm for this problem is a randomized algorithm with a total update time of $\tilde O(mn/\epsilon)$ and constant query time by Roditty and Zwick [FOCS 2004]. The fastest deterministic algorithm is from a 1981 paper by Even and Shiloach [JACM 1981]; it has a total update time of $O(mn^2)$ and constant query time. We improve these results as follows: (1) We present an algorithm with a total update time of $\tilde O(n^{5/2}/\epsilon)$ and constant query time that has an additive error of $2$ in addition to the $1+\epsilon$ multiplicative error. This beats the previous $\tilde O(mn/\epsilon)$ time when $m=\Omega(n^{3/2})$. Note that the additive error is unavoidable since, even in the static case, an $O(n^{3-\delta})$-time (a so-called truly subcubic) combinatorial algorithm with $1+\epsilon$ multiplicative error cannot have an additive error less than $2-\epsilon$, unless we make a major breakthrough for Boolean matrix multiplication [Dor et al. FOCS 1996] and many other long-standing problems [Vassilevska Williams and Williams FOCS 2010]. The algorithm can also be turned into a $(2+\epsilon)$-approximation algorithm (without an additive error) with the same time guarantees, improving the recent $(3+\epsilon)$-approximation algorithm with $\tilde O(n^{5/2+O(\sqrt{\log{(1/\epsilon)}/\log n})})$ running time of Bernstein and Roditty [SODA 2011] in terms of both approximation and time guarantees. (2) We present a deterministic algorithm with a total update time of $\tilde O(mn/\epsilon)$ and a query time of $O(\log\log n)$. The algorithm has a multiplicative error of $1+\epsilon$ and gives the first improved deterministic algorithm since 1981. It also answers an open question raised by Bernstein [STOC 2013]. ",Computer Science - Data Structures and Algorithms ; F.2.0 ; G.2.2 ; ,"Henzinger, Monika ; Krinninger, Sebastian ; Nanongkai, Danupon ; "
http://arxiv.org/abs/1308.0801,"Spectral Sequences, Exact Couples and Persistent Homology of filtrations","  In this paper we study the relationship between a very classical algebraic object associated to a filtration of spaces, namely a spectral sequence introduced by Leray in the 1940's, and a more recently invented object that has found many applications -- namely, its persistent homology groups. We show the existence of a long exact sequence of groups linking these two objects and using it derive formulas expressing the dimensions of each individual groups of one object in terms of the dimensions of the groups in the other object. The main tool used to mediate between these objects is the notion of exact couples first introduced by Massey in 1952. ",Mathematics - Algebraic Topology ; Computer Science - Computational Geometry ; 55T05 ; ,"Basu, Saugata ; Parida, Laxmi ; "
http://arxiv.org/abs/1308.1391,Low-Dimensional Reconciliation for Continuous-Variable Quantum Key   Distribution,"  We propose an efficient logical layer-based reconciliation method for continuous-variable quantum key distribution (CVQKD) to extract binary information from correlated Gaussian variables. We demonstrate that by operating on the raw-data level, the noise of the quantum channel can be corrected in the low-dimensional (scalar) space and the reconciliation can be extended to arbitrary dimensions. The CVQKD systems allow an unconditionally secret communication over standard telecommunication networks. To exploit the real potential of CVQKD a robust reconciliation technique is needed. It is currently unavailable, which makes it impossible to reach the real performance of the CVQKD protocols. The reconciliation is a post-processing step separated from the transmission of quantum states, which is aimed to derive the secret key from the raw data. The reconciliation process of correlated Gaussian variables is a complex problem that requires either tomography in the physical layer that is intractable in a practical scenario, or high-cost calculations in the multidimensional spherical space with strict dimensional limitations. To avoid these issues we define the low-dimensional reconciliation. We prove that the error probability of one-dimensional reconciliation is zero in any practical CVQKD scenario, and provides unconditional security. The results allow to significantly improve the currently available key rates and transmission distances of CVQKD. ",Quantum Physics ; Computer Science - Information Theory ; ,"Gyongyosi, Laszlo ; Imre, Sandor ; "
http://arxiv.org/abs/1308.1603,"A Note on Topology Preservation in Classification, and the Construction   of a Universal Neuron Grid","  It will be shown that according to theorems of K. Menger, every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space. Furthermore, if this identification is made, the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions. Conclusions will be drawn for established neuron grid types as well as neural fields. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Artificial Intelligence ; Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Statistics - Machine Learning ; 92F99 ; ,"Volz, Dietmar ; "
http://arxiv.org/abs/1308.3987,Cop and robber game and hyperbolicity,"  In this note, we prove that all cop-win graphs G in the game in which the robber and the cop move at different speeds s and s' with s'<s, are \delta-hyperbolic with \delta=O(s^2). We also show that the dependency between \delta and s is linear if s-s'=\Omega(s) and G obeys a slightly stronger condition. This solves an open question from the paper (J. Chalopin et al., Cop and robber games when the robber can hide and ride, SIAM J. Discr. Math. 25 (2011) 333-359). Since any \delta-hyperbolic graph is cop-win for s=2r and s'=r+2\delta for any r>0, this establishes a new - game-theoretical - characterization of Gromov hyperbolicity. We also show that for weakly modular graphs the dependency between \delta and s is linear for any s'<s. Using these results, we describe a simple constant-factor approximation of the hyperbolicity \delta of a graph on n vertices in O(n^2) time when the graph is given by its distance-matrix. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Chalopin, Jérémie ; Chepoi, Victor ; Papasoglu, Panos ; Pecatte, Timothée ; "
http://arxiv.org/abs/1308.4201,Full-Diversity Space-Time Block Codes for Integer-Forcing Linear   Receivers,"  In multiple-input multiple-output (MIMO) fading channels, the design criterion for full-diversity space-time block codes (STBCs) is primarily determined by the decoding method at the receiver. Although constructions of STBCs have predominantly matched the maximum-likelihood (ML) decoder, design criteria and constructions of full-diversity STBCs have also been reported for low-complexity linear receivers. A new receiver architecture called Integer-Forcing (IF) linear receiver has been proposed to MIMO channels by Zhan et al. which showed promising results for the high-rate V-BLAST encoding scheme. In this paper, we address the design of full-diversity STBCs for IF linear receivers. In particular, we are interested in characterizing the structure of STBCs that provide full-diversity with the IF receiver. Along that direction, we derive an upper bound on the probability of decoding error, and show that STBCs that satisfy the restricted non-vanishing singular value (RNVS) property provide full-diversity for the IF receiver. Furthermore, we prove that all known STBCs with the non-vanishing determinant property provide full-diversity with IF receivers, as they guarantee the RNVS property. By using the formulation of RNVS property, we also prove the existence of a full-diversity STBC outside the class of perfect STBCs, thereby adding significant insights compared to the existing works on STBCs with IF decoding. Finally, we present extensive simulation results to demonstrate that linear designs with RNVS property provide full-diversity for IF receiver. ",Computer Science - Information Theory ; ,"Harshan, J. ; Sakzad, Amin ; Viterbo, Emanuele ; "
http://arxiv.org/abs/1308.4273,Adaptive matching pursuit for off-grid compressed sensing,"  Compressive sensing (CS) can effectively recover a signal when it is sparse in some discrete atoms. However, in some applications, signals are sparse in a continuous parameter space, e.g., frequency space, rather than discrete atoms. Usually, we divide the continuous parameter into finite discrete grid points and build a dictionary from these grid points. However, the actual targets may not exactly lie on the grid points no matter how densely the parameter is grided, which introduces mismatch between the predefined dictionary and the actual one. In this article, a novel method, namely adaptive matching pursuit with constrained total least squares (AMP-CTLS), is proposed to find actual atoms even if they are not included in the initial dictionary. In AMP-CTLS, the grid and the dictionary are adaptively updated to better agree with measurements. The convergence of the algorithm is discussed, and numerical experiments demonstrate the advantages of AMP-CTLS. ",Electrical Engineering and Systems Science - Signal Processing ; Computer Science - Information Theory ; ,"Huang, Tianyao ; Liu, Yimin ; Meng, Huadong ; Wang, Xiqin ; "
http://arxiv.org/abs/1308.5146,Compressive Multiplexing of Correlated Signals,"  We present a general architecture for the acquisition of ensembles of correlated signals. The signals are multiplexed onto a single line by mixing each one against a different code and then adding them together, and the resulting signal is sampled at a high rate. We show that if the $M$ signals, each bandlimited to $W/2$ Hz, can be approximated by a superposition of $R < M$ underlying signals, then the ensemble can be recovered by sampling at a rate within a logarithmic factor of $RW$ (as compared to the Nyquist rate of $MW$). This sampling theorem shows that the correlation structure of the signal ensemble can be exploited in the acquisition process even though it is unknown a priori.   The reconstruction of the ensemble is recast as a low-rank matrix recovery problem from linear measurements. The architectures we are considering impose a certain type of structure on the linear operators. Although our results depend on the mixing forms being random, this imposed structure results in a very different type of random projection than those analyzed in the low-rank recovery literature to date. ",Computer Science - Information Theory ; Statistics - Applications ; ,"Ahmed, Ali ; Romberg, Justin ; "
http://arxiv.org/abs/1308.6702,Adversarial hypothesis testing and a quantum Stein's Lemma for   restricted measurements,"  Recall the classical hypothesis testing setting with two convex sets of probability distributions P and Q. One receives either n i.i.d. samples from a distribution p in P or from a distribution q in Q and wants to decide from which set the points were sampled. It is known that the optimal exponential rate at which errors decrease can be achieved by a simple maximum-likelihood ratio test which does not depend on p or q, but only on the sets P and Q.   We consider an adaptive generalization of this model where the choice of p in P and q in Q can change in each sample in some way that depends arbitrarily on the previous samples. In other words, in the k'th round, an adversary, having observed all the previous samples in rounds 1,...,k-1, chooses p_k in P and q_k in Q, with the goal of confusing the hypothesis test. We prove that even in this case, the optimal exponential error rate can be achieved by a simple maximum-likelihood test that depends only on P and Q.   We then show that the adversarial model has applications in hypothesis testing for quantum states using restricted measurements. For example, it can be used to study the problem of distinguishing entangled states from the set of all separable states using only measurements that can be implemented with local operations and classical communication (LOCC). The basic idea is that in our setup, the deleterious effects of entanglement can be simulated by an adaptive classical adversary.   We prove a quantum Stein's Lemma in this setting: In many circumstances, the optimal hypothesis testing rate is equal to an appropriate notion of quantum relative entropy between two states. In particular, our arguments yield an alternate proof of Li and Winter's recent strengthening of strong subadditivity for quantum relative entropy. ",Computer Science - Information Theory ; Mathematics - Probability ; Quantum Physics ; ,"Brandao, Fernando G. S. L. ; Harrow, Aram W. ; Lee, James R. ; Peres, Yuval ; "
http://arxiv.org/abs/1309.0671,BayesOpt: A Library for Bayesian optimization with Robotics Applications,"  The purpose of this paper is twofold. On one side, we present a general framework for Bayesian optimization and we compare it with some related fields in active learning and Bayesian numerical analysis. On the other hand, Bayesian optimization and related problems (bandits, sequential experimental design) are highly dependent on the surrogate model that is selected. However, there is no clear standard in the literature. Thus, we present a fast and flexible toolbox that allows to test and combine different models and criteria with little effort. It includes most of the state-of-the-art contributions, algorithms and models. Its speed also removes part of the stigma that Bayesian optimization methods are only good for ""expensive functions"". The software is free and it can be used in many operating systems and computer languages. ",Computer Science - Robotics ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; Computer Science - Mathematical Software ; ,"Martinez-Cantin, Ruben ; "
http://arxiv.org/abs/1309.2348,An Overview of Nominal-Typing versus Structural-Typing in OOP,"  NOOP is a mathematical model of nominally-typed OOP that proves the identification of inheritance and subtyping in mainstream nominally-typed OO programming languages and the validity of this identification. This report gives an overview of the main notions in OOP relevant to constructing a mathematical model of OOP such as NOOP. The emphasis in this report is on defining nominality, nominal typing and nominal subtyping of mainstream nominally-typed OO languages, and on contrasting the three notions with their counterparts in structurally-typed OO languages, i.e., with structurality, structural typing and structural subtyping, respectively. An additional appendix demonstrates these notions and other related notions, and the differences between them, using some simple code examples. A detailed, more technical comparison between nominal typing and structural typing in OOP is presented in other publications. ",Computer Science - Programming Languages ; ,"AbdelGawad, Moez A. ; "
http://arxiv.org/abs/1309.3014,Hypercontractivity of spherical averages in Hamming space,"  Consider the linear space of functions on the binary hypercube and the linear operator $S_\delta$ acting by averaging a function over a Hamming sphere of radius $\delta n$ around every point. It is shown that this operator has a dimension-independent bound on the norm $L_p \to L_2$ with $p = 1+(1-2\delta)^2$. This result evidently parallels a classical estimate of Bonami and Gross for $L_p \to L_q$ norms for the operator of convolution with a Bernoulli noise. The estimate for $S_\delta$ is harder to obtain since the latter is neither a part of a semigroup, nor a tensor power. The result is shown by a detailed study of the eigenvalues of $S_\delta$ and $L_p\to L_2$ norms of the Fourier multiplier operators $\Pi_a$ with symbol equal to a characteristic function of the Hamming sphere of radius $a$ (in the notation common in boolean analysis $\Pi_a f=f^{=a}$, where $f^{=a}$ is a degree-$a$ component of function $f$). A sample application of the result is given: Any set $A\subset \FF_2^n$ with the property that $A+A$ contains a large portion of some Hamming sphere (counted with multiplicity) must have cardinality a constant multiple of $2^n$. ",Mathematics - Probability ; Computer Science - Information Theory ; Mathematics - Combinatorics ; Mathematics - Functional Analysis ; ,"Polyanskiy, Yury ; "
http://arxiv.org/abs/1309.3699,Local Support Vector Machines:Formulation and Analysis,"  We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs. ",Statistics - Machine Learning ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Ganti, Ravi ; Gray, Alexander ; "
http://arxiv.org/abs/1309.3701,New and simple algorithms for stable flow problems,"  Stable flows generalize the well-known concept of stable matchings to markets in which transactions may involve several agents, forwarding flow from one to another. An instance of the problem consists of a capacitated directed network, in which vertices express their preferences over their incident edges. A network flow is stable if there is no group of vertices that all could benefit from rerouting the flow along a walk.   Fleiner established that a stable flow always exists by reducing it to the stable allocation problem. We present an augmenting-path algorithm for computing a stable flow, the first algorithm that achieves polynomial running time for this problem without using stable allocation as a black-box subroutine. We further consider the problem of finding a stable flow such that the flow value on every edge is within a given interval. For this problem, we present an elegant graph transformation and based on this, we devise a simple and fast algorithm, which also can be used to find a solution to the stable marriage problem with forced and forbidden edges.   Finally, we study the stable multicommodity flow model introduced by Kir\'{a}ly and Pap. The original model is highly involved and allows for commodity-dependent preference lists at the vertices and commodity-specific edge capacities. We present several graph-based reductions that show equivalence to a significantly simpler model. We further show that it is NP-complete to decide whether an integral solution exists. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Cseh, Ágnes ; Matuschke, Jannik ; "
http://arxiv.org/abs/1309.3730,Automatically Extracting Instances of Code Change Patterns with AST   Analysis,"  A code change pattern represents a kind of recurrent modification in software. For instance, a known code change pattern consists of the change of the conditional expression of an if statement. Previous work has identified different change patterns. Complementary to the identification and definition of change patterns, the automatic extraction of pattern instances is essential to measure their empirical importance. For example, it enables one to count and compare the number of conditional expression changes in the history of different projects. In this paper we present a novel approach for search patterns instances from software history. Our technique is based on the analysis of Abstract Syntax Trees (AST) files within a given commit. We validate our approach by counting instances of 18 change patterns in 6 open-source Java projects. ",Computer Science - Software Engineering ; ,"Martinez, Matias ; Duchien, Laurence ; Monperrus, Martin ; "
http://arxiv.org/abs/1309.4958,Approximation of smallest linear tree grammar,"  A simple linear-time algorithm for constructing a linear context-free tree grammar of size O(rg + r g log (n/r g))for a given input tree T of size n is presented, where g is the size of a minimal linear context-free tree grammar for T, and r is the maximal rank of symbols in T (which is a constant in many applications). This is the first example of a grammar-based tree compression algorithm with a good, i.e. logarithmic in terms of the size of the input tree, approximation ratio. The analysis of the algorithm uses an extension of the recompression technique from strings to trees. ",Computer Science - Data Structures and Algorithms ; Computer Science - Formal Languages and Automata Theory ; F.4.2 ; F.2.2 ; E.4 ; ,"Jeż, Artur ; Lohrey, Markus ; "
http://arxiv.org/abs/1309.5310,Conditioning of Random Block Subdictionaries with Applications to   Block-Sparse Recovery and Regression,"  The linear model, in which a set of observations is assumed to be given by a linear combination of columns of a matrix, has long been the mainstay of the statistics and signal processing literature. One particular challenge for inference under linear models is understanding the conditions on the dictionary under which reliable inference is possible. This challenge has attracted renewed attention in recent years since many modern inference problems deal with the ""underdetermined"" setting, in which the number of observations is much smaller than the number of columns in the dictionary. This paper makes several contributions for this setting when the set of observations is given by a linear combination of a small number of groups of columns of the dictionary, termed the ""block-sparse"" case. First, it specifies conditions on the dictionary under which most block subdictionaries are well conditioned. This result is fundamentally different from prior work on block-sparse inference because (i) it provides conditions that can be explicitly computed in polynomial time, (ii) the given conditions translate into near-optimal scaling of the number of columns of the block subdictionaries as a function of the number of observations for a large class of dictionaries, and (iii) it suggests that the spectral norm and the quadratic-mean block coherence of the dictionary (rather than the worst-case coherences) fundamentally limit the scaling of dimensions of the well-conditioned block subdictionaries. Second, this paper investigates the problems of block-sparse recovery and block-sparse regression in underdetermined settings. Near-optimal block-sparse recovery and regression are possible for certain dictionaries as long as the dictionary satisfies easily computable conditions and the coefficients describing the linear combination of groups of columns can be modeled through a mild statistical prior. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; ,"Bajwa, Waheed U. ; Duarte, Marco F. ; Calderbank, Robert ; "
http://arxiv.org/abs/1309.5568,Integrating Communications and Merging Messaging via the eXtensible   Messaging and Presence Protocol,"  Common problems affecting modern email usage include spam, lack of sender verification, lack of built-in security and lack of message integrity. This paper looks at how we can utilise the extensible messaging and presence protocol also known as XMPP to, in time, replace email facilities. We present several methods for initiating a transition away from SMTP for email to rely upon the inherent benefits of XMPP with minimal disruption to existing networks and email infrastructure. We look at how a program might be used to open an existing POP3/IMAP account, scan for messages that can be sent to a XMPP network user, extract the message and then deliver it the XMPP user's client. We show that the system can be implemented and then deployed with a minimum of hassle and network disruption to demonstrate XMPP as a reliable and fast replacement for email as we know it today. ",Computer Science - Networking and Internet Architecture ; ,"Coleman, Martin A. ; "
http://arxiv.org/abs/1309.6610,Adversarial Multiple Access Channels with Individual Injection Rates,"  We study deterministic distributed broadcasting in synchronous multiple-access channels. Packets are injected into $n$ nodes by a window-type adversary that is constrained by a window $w$ and injection rates individually assigned to all nodes. We investigate what queue size and packet latency can be achieved with the maximum aggregate injection rate of one packet per round, depending on properties of channels and algorithms. We give a non-adaptive algorithm for channels with collision detection and an adaptive algorithm for channels without collision detection that achieve $O(\min(n+w,w\log n))$ packet latency. We show that packet latency has to be either $\Omega(w \max (1,\log_w n))$, when $w\le n$, or $\Omega(w+n)$, when $w>n$, as a matching lower bound to these algorithms. We develop a non-adaptive algorithm for channels without collision detection that achieves $O(n+w)$ queue size and $O(nw)$ packet latency. This is in contrast with the adversarial model of global injection rates, in which non-adaptive algorithms with bounded packet latency do not exist (Chlebus et al. Distributed Computing 22(2): 93 - 116, 2009). Our algorithm avoids collisions produced by simultaneous transmissions; we show that any algorithm with this property must have $\Omega(nw)$ packet latency. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Networking and Internet Architecture ; ","Anantharamu, Lakshmi ; Chlebus, Bogdan S. ; Rokicki, Mariusz A. ; "
http://arxiv.org/abs/1309.6838,Inverse Covariance Estimation for High-Dimensional Data in Linear Time   and Space: Spectral Methods for Riccati and Sparse Models,"  We propose maximum likelihood estimation for learning Gaussian graphical models with a Gaussian (ell_2^2) prior on the parameters. This is in contrast to the commonly used Laplace (ell_1) prior for encouraging sparseness. We show that our optimization problem leads to a Riccati matrix equation, which has a closed form solution. We propose an efficient algorithm that performs a singular value decomposition of the training data. Our algorithm is O(NT^2)-time and O(NT)-space for N variables and T samples. Our method is tailored to high-dimensional problems (N gg T), in which sparseness promoting methods become intractable. Furthermore, instead of obtaining a single solution for a specific regularization parameter, our algorithm finds the whole solution path. We show that the method has logarithmic sample complexity under the spiked covariance model. We also propose sparsification of the dense solution with provable performance guarantees. We provide techniques for using our learnt models, such as removing unimportant variables, computing likelihoods and conditional distributions. Finally, we show promising results in several gene expressions datasets. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Honorio, Jean ; Jaakkola, Tommi S. ; "
http://arxiv.org/abs/1309.6927,Inclusion-exclusion enhanced by nerve stimulation,"  When evaluating the lengthy inclusion-exclusion expansion many of its terms may turn out to be zero, and hence should be discarded beforehand. Often this can be done. The main idea is that the index sets of nonzero terms constitute a set ideal (called the 'nerve'), which often can be encoded in a compact way (Upgrade B). As a further enhancement (Upgrade A), equal nonzero terms can sometimes be efficiently collected. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Wild, Marcel ; "
http://arxiv.org/abs/1310.0441,Countering Wrapping Attack on XML Signature in SOAP Message for Cloud   Computing,"  It is known that the exchange of information between web applications is done by means of the SOAP protocol. Securing this protocol is obviously a vital issue for any computer network. However, when it comes to cloud computing systems, the sensitivity of this issue rises, as the clients of system, release their data to the cloud. XML signature is employed to secure SOAP messages. However, there are also some weak points that have been identified, named as XML signature wrapping attacks, which have been categorized into four major groups; Simple Ancestry Context Attack, Optional element context attacks, Sibling Value Context Attack, Sibling Order Context. In this paper, two existing methods, for referencing the signed part of SOAP Message, named as ID referencing and XPath method, are analyzed and examined. In addition, a new method is proposed and tested, to secure the SOAP message. In the new method, the XML any signature wrapping attack is prevented by employing the concept of XML digital signature on the SOAP message. The results of conducted experiments show that the proposed method is approximately three times faster than the XPath method and even a little faster than ID. ",Computer Science - Cryptography and Security ; ,"Kouchaksaraei, Hadi Razzaghi ; Chefranov, Alexander G. ; "
http://arxiv.org/abs/1310.0833,Flips in combinatorial pointed pseudo-triangulations with face degree at   most four,"  In this paper we consider the flip operation for combinatorial pointed pseudo-triangulations where faces have size 3 or 4, so-called combinatorial 4-PPTs. We show that every combinatorial 4-PPT is stretchable to a geometric pseudo-triangulation, which in general is not the case if faces may have size larger than 4. Moreover, we prove that the flip graph of combinatorial 4-PPTs is connected and has diameter $O(n^2)$, even in the case of labeled vertices with fixed outer face. For this case we provide an $\Omega(n\log n)$ lower bound. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; ,"Aichholzer, Oswin ; Hackl, Thomas ; Orden, David ; Pilz, Alexander ; Saumell, Maria ; Vogtenhuber, Birgit ; "
http://arxiv.org/abs/1310.1250,Learning ambiguous functions by neural networks,"  It is not, in general, possible to have access to all variables that determine the behavior of a system. Having identified a number of variables whose values can be accessed, there may still be hidden variables which influence the dynamics of the system. The result is model ambiguity in the sense that, for the same (or very similar) input values, different objective outputs should have been obtained. In addition, the degree of ambiguity may vary widely across the whole range of input values. Thus, to evaluate the accuracy of a model it is of utmost importance to create a method to obtain the degree of reliability of each output result. In this paper we present such a scheme composed of two coupled artificial neural networks: the first one being responsible for outputting the predicted value, whereas the other evaluates the reliability of the output, which is learned from the error values of the first one. As an illustration, the scheme is applied to a model for tracking slopes in a straw chamber and to a credit scoring model. ","Computer Science - Neural and Evolutionary Computing ; Computer Science - Machine Learning ; Physics - Data Analysis, Statistics and Probability ; 68T37, 82C32 ; I.2.6 ; I.5.1 ; I.5.5 ; ","Ligeiro, Rui ; Mendes, R. Vilela ; "
http://arxiv.org/abs/1310.1861,Physical-Layer Cryptography Through Massive MIMO,"  We propose the new technique of physical-layer cryptography based on using a massive MIMO channel as a key between the sender and desired receiver, which need not be secret. The goal is for low-complexity encoding and decoding by the desired transmitter-receiver pair, whereas decoding by an eavesdropper is hard in terms of prohibitive complexity. The decoding complexity is analyzed by mapping the massive MIMO system to a lattice. We show that the eavesdropper's decoder for the MIMO system with M-PAM modulation is equivalent to solving standard lattice problems that are conjectured to be of exponential complexity for both classical and quantum computers. Hence, under the widely-held conjecture that standard lattice problems are hard to solve in the worst-case, the proposed encryption scheme has a more robust notion of security than that of the most common encryption methods used today such as RSA and Diffie-Hellman. Additionally, we show that this scheme could be used to securely communicate without a pre-shared secret and little computational overhead. Thus, by exploiting the physical layer properties of the radio channel, the massive MIMO system provides for low-complexity encryption commensurate with the most sophisticated forms of application-layer encryption that are currently known. ",Computer Science - Information Theory ; Computer Science - Cryptography and Security ; ,"Dean, Thomas ; Goldsmith, Andrea ; "
http://arxiv.org/abs/1310.2728,The asymptotic $k$-SAT threshold,"  Since the early 2000s physicists have developed an ingenious but non-rigorous formalism called the cavity method to put forward precise conjectures on phase transitions in random problems [Mezard, Parisi, Zecchina: Science 2002]. The cavity method predicts that the satisfiability threshold in the random $k$-SAT problem is $2^k\ln2-\frac12(1+\ln 2)+\epsilon_k$, with $\lim_{k\rightarrow\infty}\epsilon_k=0$ [Mertens, Mezard, Zecchina: Random Structures and Algorithms 2006]. This paper contains a proof of that conjecture. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Mathematics - Probability ; 05C80 ; ,"Coja-Oghlan, Amin ; Panagiotou, Konstantinos ; "
http://arxiv.org/abs/1310.3389,Spectra of random networks in the weak clustering regime,"  The asymptotic behaviour of dynamical processes in networks can be expressed as a function of spectral properties of the corresponding adjacency and Laplacian matrices. Although many theoretical results are known for the spectra of traditional configuration models, networks generated through these models fail to describe many topological features of real-world networks, in particular non-null values of the clustering coefficient. Here we study effects of cycles of order three (triangles) in network spectra. By using recent advances in random matrix theory, we determine the spectral distribution of the network adjacency matrix as a function of the average number of triangles attached to each node for networks without modular structure and degree-degree correlations. Implications to network dynamics are discussed. Our findings can shed light in the study of how particular kinds of subgraphs influence network dynamics. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Social and Information Networks ; ,"Peron, Thomas K. DM. ; Ji, Peng ; Kurths, Jürgen ; Rodrigues, Francisco A. ; "
http://arxiv.org/abs/1310.4345,Moser's Shadow Problem,"  Moser's shadow problem asks to estimate the shadow function $\mathfrak{s}_b(n)$, which is the largest number such that for each bounded convex polyhedron $P$ with $n$ vertices in $3$-space there is some direction ${\bf v}$ (depending on $P$) such that, when illuminated by parallel light rays from infinity in direction ${\bf v}$, the polyhedron casts a shadow having at least $\mathfrak{s}_b(n)$ vertices. A general version of the problem allows unbounded polyhedra as well, and has associated shadow function $\mathfrak{s}_u(n)$. This paper presents correct order of magnitude asymptotic bounds on these functions. The bounded case has answer $\mathfrak{s}_b(n) = \Theta \big( \log (n)/ (\log(\log (n))\big$. The unbounded shadow problem is shown to have the different asymptotic growth rate $\mathfrak{s}_u(n) = \Theta \big(1\big)$. Results on the bounded shadow problem follow from 1989 work of Chazelle, Edelsbrunner and Guibas on the (bounded) silhouette span number $\mathfrak{s}_b^{\ast}(n)$, defined analogously but with arbitrary light sources. We complete the picture by showing that the unbounded silhouette span number $\mathfrak{s}_u^{\ast}(n)$ grows as $\Theta \big( \log (n)/ (\log(\log (n))\big)$. ","Mathematics - Metric Geometry ; Computer Science - Computational Geometry ; Primary: 52B10, Secondary: 51N15, 65D18, 68U05, 90C05 ; ","Lagarias, Jeffrey C. ; Luo, Yusheng ; Padrol, Arnau ; "
http://arxiv.org/abs/1310.4349,An Improved Majority-Logic Decoder Offering Massively Parallel Decoding   for Real-Time Control in Embedded Systems,"  We propose an easy-to-implement hard-decision majority-logic decoding algorithm for Reed-Muller codes RM(r,m) with m >= 3, m/2 >= r >= 1. The presented algorithm outperforms the best known majority-logic decoding algorithms and offers highly parallel decoding. The result is of special importance for safety- and time-critical applications in embedded systems. A simple combinational circuit can perform the proposed decoding. In particular, we show how our decoder for the three-error-correcting code RM(2,5) of dimension 16 and length 32 can be realized on hardware level. ","Computer Science - Information Theory ; Computer Science - Hardware Architecture ; Computer Science - Discrete Mathematics ; Computer Science - Emerging Technologies ; 94B35, 68P30 ; E.4 ; ","Bertram, Juliane ; Hauck, Peter ; Huber, Michael ; "
http://arxiv.org/abs/1310.5251,Sparsity-Promoting Sensor Selection for Non-linear Measurement Models,"  Sensor selection is an important design problem in large-scale sensor networks. Sensor selection can be interpreted as the problem of selecting the best subset of sensors that guarantees a certain estimation performance. We focus on observations that are related to a general non-linear model. The proposed framework is valid as long as the observations are independent, and its likelihood satisfies the regularity conditions. We use several functions of the Cram\'er-Rao bound (CRB) as a performance measure. We formulate the sensor selection problem as the design of a selection vector, which in its original form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed sensor selection solvers that can be efficiently solved in polynomial time. We also propose a projected subgradient algorithm that is attractive for large-scale problems and also show how the algorithm can be easily distributed. The proposed framework is illustrated with a number of examples related to sensor placement design for localization. ",Computer Science - Information Theory ; Electrical Engineering and Systems Science - Signal Processing ; ,"Chepuri, Sundeep Prabhakar ; Leus, Geert ; "
http://arxiv.org/abs/1310.6324,On Jacobian group arithmetic for typical divisors on curves,"  In a previous joint article with F. Abu Salem, we gave efficient algorithms for Jacobian group arithmetic of ""typical"" divisor classes on C_{3,4} curves, improving on similar results by other authors. At that time, we could only state that a generic divisor was typical, and hence unlikely to be encountered if one implemented these algorithms over a very large finite field. This article pins down an explicit characterization of these typical divisors, for an arbitrary smooth projective curve of genus g >= 1 having at least one rational point. We give general algorithms for Jacobian group arithmetic with these typical divisors, and prove not only that the algorithms are correct if various divisors are typical, but also that the success of our algorithms provides a guarantee that the resulting output is correct and that the resulting input and/or output divisors are also typical. These results apply in particular to our earlier algorithms for C_{3,4} curves. As a byproduct, we obtain a further speedup of approximately 15% on our previous algorithms for C_{3,4} curves. ","Mathematics - Number Theory ; Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; 14Q05 (primary), 11Y16, 14H40, 11G20 ; ","Khuri-Makdisi, Kamal ; "
http://arxiv.org/abs/1310.6398,Some Remarks on Lower Bounds for Queue Machines (Preliminary Report),"  We first give an improved lower bound for the deterministic online simulation of tapes or pushdown stores by queues. Then we inspect some proofs in a classical work on queue machines in the area of Formal Languages and outline why a main argument in the proofs is incomplete. Based on descriptional complexity, we show the intuition behind the argument to be correct. ",Computer Science - Computational Complexity ; Computer Science - Formal Languages and Automata Theory ; ,"Petersen, Holger ; "
http://arxiv.org/abs/1310.8097,Guaranteed Collision Detection With Toleranced Motions,"  We present a method for guaranteed collision detection with toleranced motions. The basic idea is to consider the motion as a curve in the 12-dimensional space of affine displacements, endowed with an object-oriented Euclidean metric, and cover it with balls. The associated orbits of points, lines, planes and polygons have particularly simple shapes that lend themselves well to exact and fast collision queries. We present formulas for elementary collision tests with these orbit shapes and we suggest an algorithm, based on motion subdivision and computation of bounding balls, that can give a no-collision guarantee. It allows a robust and efficient implementation and parallelization. At hand of several examples we explore the asymptotic behavior of the algorithm and compare different implementation strategies. ","Computer Science - Computational Geometry ; Computer Science - Robotics ; 65D18, 70B10 ; ","Schröcker, Hans-Peter ; Weber, Matthias J. ; "
http://arxiv.org/abs/1310.8121,Easy Accurate Reading and Writing of Floating-Point Numbers,"  Presented here are algorithms for converting between (decimal) scientific-notation and (binary) IEEE-754 double-precision floating-point numbers. By employing a rounding integer quotient operation these algorithms are much simpler than those previously published. The values are stable under repeated conversions between the formats. Unlike Java-1.8, the scientific representations generated use only the minimum number of mantissa digits needed to convert back to the original binary values. ",Computer Science - Numerical Analysis ; 65G04 ; G.1.0 ; ,"Jaffer, Aubrey ; "
http://arxiv.org/abs/1311.0320,An Improved Solution for Restricted and Uncertain TRQ,"  CSPTRQ is an interesting problem and its has attracted much attention. The CSPTRQ is a variant of the traditional PTRQ. As objects moving in a constrained-space are common, clearly, it can also find many applications. At the first sight, our problem can be easily tackled by extending existing methods used to answer the PTRQ. Unfortunately, those classical techniques are not well suitable for our problem, due to a set of new challenges. We develop targeted solutions and demonstrate the efficiency and effectiveness of the proposed methods through extensive experiments. ",Computer Science - Databases ; H.3.3 ; G.3 ; G.3.1 ; ,"Wang, Jack ; "
http://arxiv.org/abs/1311.0913,Bidding Games and Efficient Allocations,"  Richman games are zero-sum games, where in each turn players bid in order to determine who will play next [Lazarus et al.'99]. We extend the theory to impartial general-sum two player games called \emph{bidding games}, showing the existence of pure subgame-perfect equilibria (PSPE). In particular, we show that PSPEs form a semilattice, with a unique and natural \emph{Bottom Equilibrium}.   Our main result shows that if only two actions available to the players in each node, then the Bottom Equilibrium has additional properties: (a) utilities are monotone in budget; (b) every outcome is Pareto-efficient; and (c) any Pareto-efficient outcome is attained for some budget.   In the context of combinatorial bargaining, we show that a player with a fraction of X% of the total budget prefers her allocation to X% of the possible allocations. In addition, we provide a polynomial-time algorithm to compute the Bottom Equilibrium of a binary bidding game. ",Computer Science - Computer Science and Game Theory ; I.2.11 ; ,"Kalai, Gil ; Meir, Reshef ; Tennenholtz, Moshe ; "
http://arxiv.org/abs/1311.1339,Zero-Error Capacity of a Class of Timing Channels,"  We analyze the problem of zero-error communication through timing channels that can be interpreted as discrete-time queues with bounded waiting times. The channel model includes the following assumptions: 1) Time is slotted, 2) at most $ N $ ""particles"" are sent in each time slot, 3) every particle is delayed in the channel for a number of slots chosen randomly from the set $ \{0, 1, \ldots, K\} $, and 4) the particles are identical. It is shown that the zero-error capacity of this channel is $ \log r $, where $ r $ is the unique positive real root of the polynomial $ x^{K+1} - x^{K} - N $. Capacity-achieving codes are explicitly constructed, and a linear-time decoding algorithm for these codes devised. In the particular case $ N = 1 $, $ K = 1 $, the capacity is equal to $ \log \phi $, where $ \phi = (1 + \sqrt{5}) / 2 $ is the golden ratio, and the constructed codes give another interpretation of the Fibonacci sequence. ","Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; 94B25, 94A40, 94A24, 68R05, 65Q30 ; ","Kovačević, Mladen ; Popovski, Petar ; "
http://arxiv.org/abs/1311.2828,Private Matchings and Allocations,"  We consider a private variant of the classical allocation problem: given k goods and n agents with individual, private valuation functions over bundles of goods, how can we partition the goods amongst the agents to maximize social welfare? An important special case is when each agent desires at most one good, and specifies her (private) value for each good: in this case, the problem is exactly the maximum-weight matching problem in a bipartite graph.   Private matching and allocation problems have not been considered in the differential privacy literature, and for good reason: they are plainly impossible to solve under differential privacy. Informally, the allocation must match agents to their preferred goods in order to maximize social welfare, but this preference is exactly what agents wish to hide. Therefore, we consider the problem under the relaxed constraint of joint differential privacy: for any agent i, no coalition of agents excluding i should be able to learn about the valuation function of agent i. In this setting, the full allocation is no longer published---instead, each agent is told what good to get. We first show that with a small number of identical copies of each good, it is possible to efficiently and accurately solve the maximum weight matching problem while guaranteeing joint differential privacy. We then consider the more general allocation problem, when bidder valuations satisfy the gross substitutes condition. Finally, we prove that the allocation problem cannot be solved to non-trivial accuracy under joint differential privacy without requiring multiple copies of each type of good. ",Computer Science - Computer Science and Game Theory ; Computer Science - Cryptography and Security ; Computer Science - Data Structures and Algorithms ; ,"Hsu, Justin ; Huang, Zhiyi ; Roth, Aaron ; Roughgarden, Tim ; Wu, Zhiwei Steven ; "
http://arxiv.org/abs/1311.2970,Coordinated Tethering for Multi-RAT Cellular Networks: An Algorithmic   Solution and Performance Analysis,"  The exploitation of already deployed wireless local area networks (WLAN)s (e.g., WiFi access points (AP)s) has attracted considerable attention, as an efficient and practical method to improve the performance of beyond 4G wireless networks. In this paper, we propose a novel communication paradigm to satisfy the performance demands of future wireless networks: a hybrid Cellular/WLAN network architecture with wireless offloading. In contrast to the commonly adopted practice of WiFi offloading, where the WLAN APs have a wired backhaul (e.g., Digital Subscriber Line), we propose a wireless offloading approach, where the WLAN APs will share their wireless cellular broadband connection with other users. These users will select their serving node, i.e., the macro-cell eNodeB or a WLAN AP, based on a certain selection criterion. Thus a challenging research field is originated, where interfering effects and wireless resources limitations play a dominant role. Important performance metrics of the proposed hybrid scheme, including the bit error probability, the ergodic capacity and the average signal-to-interference-plus noise ratio, are theoretically studied and closed form expressions are derived for the single-user case with multiple interferers, for both identical and non-identical fading conditions. Also, based on the general multi-cellular hybrid WLAN-Cellular concept, we first propose a intercell interference minimization approach. Then we present a novel scheme for achieving frequency reuse equal to one within a single macro-cell, under specific performance criteria and constraints, that guarantee the overall cell or the individual user QoS requirements. ",Computer Science - Networking and Internet Architecture ; ,"Bithas, Petros S. ; Lioumpas, Athanasios S. ; "
http://arxiv.org/abs/1311.3158,Fingerprinting Codes and the Price of Approximate Differential Privacy,"  We show new lower bounds on the sample complexity of $(\varepsilon, \delta)$-differentially private algorithms that accurately answer large sets of counting queries. A counting query on a database $D \in (\{0,1\}^d)^n$ has the form ""What fraction of the individual records in the database satisfy the property $q$?"" We show that in order to answer an arbitrary set $\mathcal{Q}$ of $\gg nd$ counting queries on $D$ to within error $\pm \alpha$ it is necessary that $$ n \geq \tilde{\Omega}\Bigg(\frac{\sqrt{d} \log |\mathcal{Q}|}{\alpha^2 \varepsilon} \Bigg). $$ This bound is optimal up to poly-logarithmic factors, as demonstrated by the Private Multiplicative Weights algorithm (Hardt and Rothblum, FOCS'10). In particular, our lower bound is the first to show that the sample complexity required for accuracy and $(\varepsilon, \delta)$-differential privacy is asymptotically larger than what is required merely for accuracy, which is $O(\log |\mathcal{Q}| / \alpha^2)$. In addition, we show that our lower bound holds for the specific case of $k$-way marginal queries (where $|\mathcal{Q}| = 2^k \binom{d}{k}$) when $\alpha$ is not too small compared to $d$ (e.g. when $\alpha$ is any fixed constant).   Our results rely on the existence of short \emph{fingerprinting codes} (Boneh and Shaw, CRYPTO'95, Tardos, STOC'03), which we show are closely connected to the sample complexity of differentially private data release. We also give a new method for combining certain types of sample complexity lower bounds into stronger lower bounds. ",Computer Science - Cryptography and Security ; ,"Bun, Mark ; Ullman, Jonathan ; Vadhan, Salil ; "
http://arxiv.org/abs/1311.3414,Mining Software Repair Models for Reasoning on the Search Space of   Automated Program Fixing,"  This paper is about understanding the nature of bug fixing by analyzing thousands of bug fix transactions of software repositories. It then places this learned knowledge in the context of automated program repair. We give extensive empirical results on the nature of human bug fixes at a large scale and a fine granularity with abstract syntax tree differencing. We set up mathematical reasoning on the search space of automated repair and the time to navigate through it. By applying our method on 14 repositories of Java software and 89,993 versioning transactions, we show that not all probabilistic repair models are equivalent. ",Computer Science - Software Engineering ; ,"Martinez, Matias ; Monperrus, Martin ; "
http://arxiv.org/abs/1311.4257,A parallel directional Fast Multipole Method,"  This paper introduces a parallel directional fast multipole method (FMM) for solving N-body problems with highly oscillatory kernels, with a focus on the Helmholtz kernel in three dimensions. This class of oscillatory kernels requires a more restrictive low-rank criterion than that of the low-frequency regime, and thus effective parallelizations must adapt to the modified data dependencies. We propose a simple partition at a fixed level of the octree and show that, if the partitions are properly balanced between p processes, the overall runtime is essentially O(N log N/p+ p). By the structure of the low-rank criterion, we are able to avoid communication at the top of the octree. We demonstrate the effectiveness of our parallelization on several challenging models. ","Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; 65Y05, 65Y20, 78A45 ; ","Benson, Austin R. ; Poulson, Jack ; Tran, Kenneth ; Engquist, Björn ; Ying, Lexing ; "
http://arxiv.org/abs/1311.4766,Notions of Symmetry for Finite Strategic-Form Games,  In this paper we survey various notions of symmetry for finite strategic-form games; show that game bijections and game isomorphisms form groupoids; introduce matchings as a convenient characterisation of strategy triviality; and outline how to construct and partially order parameterised symmetric games with numerous examples that range all combinations of surveyed symmetry notions. ,Mathematics - Combinatorics ; Computer Science - Computer Science and Game Theory ; ,"Ham, Nicholas ; "
http://arxiv.org/abs/1311.4821,On the Complexity of Random Satisfiability Problems with Planted   Solutions,"  The problem of identifying a planted assignment given a random $k$-SAT formula consistent with the assignment exhibits a large algorithmic gap: while the planted solution becomes unique and can be identified given a formula with $O(n\log n)$ clauses, there are distributions over clauses for which the best known efficient algorithms require $n^{k/2}$ clauses. We propose and study a unified model for planted $k$-SAT, which captures well-known special cases. An instance is described by a planted assignment $\sigma$ and a distribution on clauses with $k$ literals. We define its distribution complexity as the largest $r$ for which the distribution is not $r$-wise independent ($1 \le r \le k$ for any distribution with a planted assignment).   Our main result is an unconditional lower bound, tight up to logarithmic factors, for statistical (query) algorithms [Kearns 1998, Feldman et. al 2012], matching known upper bounds, which, as we show, can be implemented using a statistical algorithm. Since known approaches for problems over distributions have statistical analogues (spectral, MCMC, gradient-based, convex optimization etc.), this lower bound provides a rigorous explanation of the observed algorithmic gap. The proof introduces a new general technique for the analysis of statistical query algorithms. It also points to a geometric paring phenomenon in the space of all planted assignments.   We describe consequences of our lower bounds to Feige's refutation hypothesis [Feige 2002] and to lower bounds on general convex programs that solve planted $k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in particular, provide concrete evidence for the security of Goldreich's one-way function and the associated pseudorandom generator when used with a sufficiently hard predicate [Goldreich 2000]. ",Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; Mathematics - Probability ; ,"Feldman, Vitaly ; Perkins, Will ; Vempala, Santosh ; "
http://arxiv.org/abs/1311.6126,An energy function and its application to the periodic behavior of   k-reversible processes,"  We consider the graph dynamical systems known as k-reversible processes. In such processes, each vertex in the graph has one of two possible states at each discrete time step. Each vertex changes its state between the current time and the next if and only if it currently has at least k neighbors in a state different than its own. For such processes, we present a monotonic function similar to the decreasing energy functions used to study threshold networks. Using this new function, we show an alternative proof for the maximum period length in a k-reversible process and provide better upper bounds on the transient length in both the general case and the case of trees. ",Computer Science - Data Structures and Algorithms ; ,"Oliveira, Leonardo I. L. ; Barbosa, Valmir C. ; Protti, Fábio ; "
http://arxiv.org/abs/1311.6876,Want a Good Answer? Ask a Good Question First!,"  Community Question Answering (CQA) websites have become valuable repositories which host a massive volume of human knowledge. To maximize the utility of such knowledge, it is essential to evaluate the quality of an existing question or answer, especially soon after it is posted on the CQA website.   In this paper, we study the problem of inferring the quality of questions and answers through a case study of a software CQA (Stack Overflow). Our key finding is that the quality of an answer is strongly positively correlated with that of its question. Armed with this observation, we propose a family of algorithms to jointly predict the quality of questions and answers, for both quantifying numerical quality scores and differentiating the high-quality questions/answers from those of low quality. We conduct extensive experimental evaluations to demonstrate the effectiveness and efficiency of our methods. ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; Computer Science - Software Engineering ; ,"Yao, Yuan ; Tong, Hanghang ; Xie, Tao ; Akoglu, Leman ; Xu, Feng ; Lu, Jian ; "
http://arxiv.org/abs/1312.0049,One-Class Classification: Taxonomy of Study and Review of Techniques,"  One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; ,"Khan, Shehroz S. ; Madden, Michael G. ; "
http://arxiv.org/abs/1312.0233,On Optimal Disc Covers and a New Characterization of the Steiner Center,"  Given N points in the plane $P_1 P_2...P_N$ and a location $\Omega$, the union of discs with diameters $[\Omega P_i], i = 1, 2,...N$ covers the convex hull of the points. The location $\Omega_s$ minimizing the area covered by the union of discs, is shown to be the Steiner center of the convex hull of the points. Similar results for $d$-dimensional Euclidean space are conjectured. ",Computer Science - Computational Geometry ; ,"Yankelevsky, Yael ; Bruckstein, Alfred M. ; "
http://arxiv.org/abs/1312.0461,Abmash: Mashing Up Legacy Web Applications by Automated Imitation of   Human Actions,"  Many business web-based applications do not offer applications programming interfaces (APIs) to enable other applications to access their data and functions in a programmatic manner. This makes their composition difficult (for instance to synchronize data between two applications). To address this challenge, this paper presents Abmash, an approach to facilitate the integration of such legacy web applications by automatically imitating human interactions with them. By automatically interacting with the graphical user interface (GUI) of web applications, the system supports all forms of integrations including bi-directional interactions and is able to interact with AJAX-based applications. Furthermore, the integration programs are easy to write since they deal with end-user, visual user-interface elements. The integration code is simple enough to be called a ""mashup"". ",Computer Science - Software Engineering ; ,"Ortac, Alper ; Monperrus, Martin ; Mezini, Mira ; "
http://arxiv.org/abs/1312.1001,Optimal detection of intersections between convex polyhedra,"  For a polyhedron $P$ in $\mathbb{R}^d$, denote by $|P|$ its combinatorial complexity, i.e., the number of faces of all dimensions of the polyhedra. In this paper, we revisit the classic problem of preprocessing polyhedra independently so that given two preprocessed polyhedra $P$ and $Q$ in $\mathbb{R}^d$, each translated and rotated, their intersection can be tested rapidly.   For $d=3$ we show how to perform such a test in $O(\log |P| + \log |Q|)$ time after linear preprocessing time and space. This running time is the best possible and improves upon the last best known query time of $O(\log|P| \log|Q|)$ by Dobkin and Kirkpatrick (1990).   We then generalize our method to any constant dimension $d$, achieving the same optimal $O(\log |P| + \log |Q|)$ query time using a representation of size $O(|P|^{\lfloor d/2\rfloor + \varepsilon})$ for any $\varepsilon>0$ arbitrarily small. This answers an even older question posed by Dobkin and Kirkpatrick 30 years ago.   In addition, we provide an alternative $O(\log |P| + \log |Q|)$ algorithm to test the intersection of two convex polygons $P$ and $Q$ in the plane. ",Computer Science - Computational Geometry ; ,"Barba, Luis ; Langerman, Stefan ; "
http://arxiv.org/abs/1312.1277,Bandits and Experts in Metric Spaces,"  In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions.   In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the ""Lipschitz MAB problem"". We present a solution for the multi-armed bandit problem in this setting. That is, for every metric space we define an isometry invariant which bounds from below the performance of Lipschitz MAB algorithms for this metric space, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions. We also address the full-feedback (""best expert"") version of the problem, where after every round the payoffs from all arms are revealed. ",Computer Science - Data Structures and Algorithms ; Computer Science - Machine Learning ; ,"Kleinberg, Robert ; Slivkins, Aleksandrs ; Upfal, Eli ; "
http://arxiv.org/abs/1312.1529,Instruction sequences expressing multiplication algorithms,"  For each function on bit strings, its restriction to bit strings of any given length can be computed by a finite instruction sequence that contains only instructions to set and get the content of Boolean registers, forward jump instructions, and a termination instruction. We describe instruction sequences of this kind that compute the function on bit strings that models multiplication on natural numbers less than $2^N$ with respect to their binary representation by bit strings of length $N$, for a fixed but arbitrary $N > 0$, according to the long multiplication algorithm and the Karatsuba multiplication algorithm. We find among other things that the instruction sequence expressing the former algorithm is longer than the one expressing the latter algorithm only if the length of the bit strings involved is greater than $2^8$. We also go into the use of an instruction sequence with backward jump instructions for expressing the long multiplication algorithm. This leads to an instruction sequence that it is shorter than the other two if the length of the bit strings involved is greater than $2$. ",Computer Science - Programming Languages ; F.1.1 ; F.2.1 ; ,"Bergstra, J. A. ; Middelburg, C. A. ; "
http://arxiv.org/abs/1312.1559,Outerstring graphs are $\chi$-bounded,"  An outerstring graph is an intersection graph of curves that lie in a common half-plane and have one endpoint on the boundary of that half-plane. We prove that the class of outerstring graphs is $\chi$-bounded, which means that their chromatic number is bounded by a function of their clique number. This generalizes a series of previous results on $\chi$-boundedness of outerstring graphs with various additional restrictions on the shape of curves or the number of times the pairs of curves can cross. The assumption that each curve has an endpoint on the boundary of the half-plane is justified by the known fact that triangle-free intersection graphs of straight-line segments can have arbitrarily large chromatic number. ","Mathematics - Combinatorics ; Computer Science - Computational Geometry ; Computer Science - Discrete Mathematics ; 05C62, 05C15 ; ","Rok, Alexandre ; Walczak, Bartosz ; "
http://arxiv.org/abs/1312.1664,Simplicial Homology for Future Cellular Networks,"  Simplicial homology is a tool that provides a mathematical way to compute the connectivity and the coverage of a cellular network without any node location information. In this article, we use simplicial homology in order to not only compute the topology of a cellular network, but also to discover the clusters of nodes still with no location information. We propose three algorithms for the management of future cellular networks. The first one is a frequency auto-planning algorithm for the self-configuration of future cellular networks. It aims at minimizing the number of planned frequencies while maximizing the usage of each one. Then, our energy conservation algorithm falls into the self-optimization feature of future cellular networks. It optimizes the energy consumption of the cellular network during off-peak hours while taking into account both coverage and user traffic. Finally, we present and discuss the performance of a disaster recovery algorithm using determinantal point processes to patch coverage holes. ",Computer Science - Networking and Internet Architecture ; ,"Vergne, Anaïs ; Decreusefond, Laurent ; Martins, Philippe ; "
http://arxiv.org/abs/1312.2048,The False Premises and Promises of Bitcoin,"  Designed to compete with fiat currencies, bitcoin proposes it is a crypto-currency alternative. Bitcoin makes a number of false claims, including: solving the double-spending problem is a good thing; bitcoin can be a reserve currency for banking; hoarding equals saving, and that we should believe bitcoin can expand by deflation to become a global transactional currency supply. Bitcoin's developers combine technical implementation proficiency with ignorance of currency and banking fundamentals. This has resulted in a failed attempt to change finance. A set of recommendations to change finance are provided in the Afterword: Investment/venture banking for the masses; Venture banking to bring back what investment banks once were; Open-outcry exchange for all CDS contracts; Attempting to develop CDS type contracts on investments in startup and existing enterprises; and Improving the connection between startup tech/ideas, business organization and investment. ","Computer Science - Computational Engineering, Finance, and Science ; Quantitative Finance - General Finance ; J.4.1 ; ","Hanley, Brian P. ; "
http://arxiv.org/abs/1312.2226,On two Algorithmic Problems about Synchronizing Automata,"  Under the assumption $\mathcal{P} \neq \mathcal{NP}$, we prove that two natural problems from the theory of synchronizing automata cannot be solved in polynomial time. The first problem is to decide whether a given reachable partial automaton is synchronizing. The second one is, given an $n$-state binary complete synchronizing automaton, to compute its reset threshold within performance ratio less than $d \ln{(n)}$ for a specific constant $d>0$. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; F.2.0 ; F.4.3 ; ,"Berlinkov, Mikhail V. ; "
http://arxiv.org/abs/1312.2674,Silent error detection in numerical time-stepping schemes,"  Errors due to hardware or low level software problems, if detected, can be fixed by various schemes, such as recomputation from a checkpoint. Silent errors are errors in application state that have escaped low-level error detection. At extreme scale, where machines can perform astronomically many operations per second, silent errors threaten the validity of computed results.   We propose a new paradigm for detecting silent errors at the application level. Our central idea is to frequently compare computed values to those provided by a cheap checking computation, and to build error detectors based on the difference between the two output sequences. Numerical analysis provides us with usable checking computations for the solution of initial-value problems in ODEs and PDEs, arguably the most common problems in computational science. Here, we provide, optimize, and test methods based on Runge-Kutta and linear multistep methods for ODEs, and on implicit and explicit finite difference schemes for PDEs. We take the heat equation and Navier-Stokes equations as examples. In tests with artificially injected errors, this approach effectively detects almost all meaningful errors, without significant slowdown. ",Computer Science - Numerical Analysis ; Computer Science - Mathematical Software ; Mathematics - Numerical Analysis ; ,"Benson, Austin R. ; Schmit, Sven ; Schreiber, Robert ; "
http://arxiv.org/abs/1312.3614,Multiple Access Multicarrier Continuous-Variable Quantum Key   Distribution,"  One of the most important practical realizations of the fundamentals of quantum mechanics is continuous-variable quantum key distribution (CVQKD). Here we propose the adaptive multicarrier quadrature division-multiuser quadrature allocation (AMQD-MQA) multiple access technique for continuous-variable quantum key distribution. The MQA scheme is based on the AMQD modulation, which granulates the inputs of the users into Gaussian subcarrier continuous-variables (CVs). In an AMQD-MQA multiple access scenario, the simultaneous reliable transmission of the users is handled by the dynamic allocation of the Gaussian subcarrier CVs. We propose two different settings of AMQD-MQA for multiple input-multiple output communication. We introduce a rate-selection strategy that tunes the modulation variances and allocates adaptively the quadratures of the users over the sub-channels. We also prove the rate formulas if only partial channel side information is available for the users of the sub-channel conditions. We show a technique for the compensation of a nonideal Gaussian input modulation, which allows the users to overwhelm the modulation imperfections to reach optimal capacity-achieving communication over the Gaussian sub-channels. We investigate the diversity amplification of the sub-channel transmittance coefficients and reveal that a strong diversity can be exploited by opportunistic Gaussian modulation. ",Quantum Physics ; Computer Science - Information Theory ; ,"Gyongyosi, Laszlo ; Imre, Sandor ; "
http://arxiv.org/abs/1312.3748,On Eavesdropper-Tolerance Capability of Two-Hop Wireless Networks,"  Two-hop wireless network serves as the basic net-work model for the study of general wireless networks, while cooperative jamming is a promising scheme to achieve the physi-cal layer security. This paper establishes a theoretical framework for the study of eavesdropper-tolerance capability (i.e., the exact maximum number of eavesdroppers that can be tolerated) in a two-hop wireless network, where the cooperative jamming is adopted to ensure security defined by secrecy outage probability (SOP) and opportunistic relaying is adopted to guarantee relia-bility defined by transmission outage probability (TOP). For the concerned network, closed form modeling for both SOP and TOP is first conducted based on the Central Limit Theorem. With the help of SOP and TOP models and also the Stochastic Ordering Theory, the model for eavesdropper-tolerance capability analysis is then developed. Finally, extensive simulation and numerical results are provided to illustrate the efficiency of our theoretical framework as well as the eavesdropper-tolerance capability of the concerned network from adopting cooperative jamming and opportunistic relaying. ",Computer Science - Information Theory ; ,"Zhang, Yuanyu ; Shen, Yulong ; Wang, Hua ; Jiang, Xiaohong ; "
http://arxiv.org/abs/1312.3876,The Symmetric Convex Ordering: A Novel Partial Order for B-DMCs Ordering   the Information Sets of Polar Codes,"  In this paper, we propose a novel partial order for binary discrete memoryless channels that we call the symmetric convex ordering. We show that Ar{\i}kan's polar transform preserves 'symmetric convex orders'. Furthermore, we show that while for symmetric channels this ordering turns out to be equivalent to the stochastic degradation ordering already known to order the information sets of polar codes, a strictly weaker partial order is obtained when at least one of the channels is asymmetric. In between, we also discuss two tools which can be useful for verifying this ordering: a criterion known as the cut criterion and channel symmetrization. Finally, we discuss potential applications of the results to polar coding over non-stationary channels. ",Computer Science - Information Theory ; ,"Alsan, Mine ; "
http://arxiv.org/abs/1312.4510,On the genericity of Whitehead minimality,"  We show that a finitely generated subgroup of a free group, chosen uniformly at random, is strictly Whitehead minimal with overwhelming probability. Whitehead minimality is one of the key elements of the solution of the orbit problem in free groups. The proofs strongly rely on combinatorial tools, notably those of analytic combinatorics. The result we prove actually depends implicitly on the choice of a distribution on finitely generated subgroups, and we establish it for the two distributions which appear in the literature on random subgroups. ",Mathematics - Group Theory ; Computer Science - Computational Complexity ; Mathematics - Combinatorics ; ,"Bassino, Frédérique ; Nicaud, Cyril ; Weil, Pascal ; "
http://arxiv.org/abs/1312.6809,The Micro Dynamics of Collective Violence,"  Collective violence in direct confrontations between two opposing groups happens in short bursts wherein small subgroups briefly attack small numbers of opponents, while the others form a non-fighting audience. The mechanism is fighters' synchronization of intentionalities during preliminary interactions, by which they feel one and overcome their fear. To explain these bursts, subgroups' small sizes and leaders' role, a social influence model and a synchronization model are compared. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Bruggeman, Jeroen ; "
http://arxiv.org/abs/1401.1061,Learning optimization models in the presence of unknown relations,"  In a sequential auction with multiple bidding agents, it is highly challenging to determine the ordering of the items to sell in order to maximize the revenue due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction.   The main contribution of this paper is two-fold. First, we demonstrate how to apply machine learning techniques to solve the optimal ordering problem in sequential auctions. We learn regression models from historical auctions, which are subsequently used to predict the expected value of orderings for new auctions. Given the learned models, we propose two types of optimization methods: a black-box best-first search approach, and a novel white-box approach that maps learned models to integer linear programs (ILP) which can then be solved by any ILP-solver. Although the studied auction design problem is hard, our proposed optimization methods obtain good orderings with high revenues.   Our second main contribution is the insight that the internal structure of regression models can be efficiently evaluated inside an ILP solver for optimization purposes. To this end, we provide efficient encodings of regression trees and linear regression models as ILP constraints. This new way of using learned models for optimization is promising. As the experimental results show, it significantly outperforms the black-box best-first search in nearly all settings. ",Computer Science - Artificial Intelligence ; Computer Science - Computer Science and Game Theory ; F.5.3 ; K.3 ; K.4 ; ,"Verwer, Sicco ; Zhang, Yingqian ; Ye, Qing Chuan ; "
http://arxiv.org/abs/1401.1140,Efficient random sampling of binary and unary-binary trees via holonomic   equations,  We present a new uniform random sampler for binary trees with $n$ internal nodes consuming $2n + \Theta(\log(n)^2)$ random bits on average. This makes it quasi-optimal and out-performs the classical Remy algorithm. We also present a sampler for unary-binary trees with $n$ nodes taking $\Theta(n)$ random bits on average. Both are the first linear-time algorithms to be optimal up to a constant. ,Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Bacher, Axel ; Bodini, Olivier ; Jacquot, Alice ; "
http://arxiv.org/abs/1401.1333,Time series forecasting using neural networks,  Recent studies have shown the classification and prediction power of the Neural Networks. It has been demonstrated that a NN can approximate any continuous function. Neural networks have been successfully used for forecasting of financial data series. The classical methods used for time series prediction like Box-Jenkins or ARIMA assumes that there is a linear relationship between inputs and outputs. Neural Networks have the advantage that can approximate nonlinear functions. In this paper we compared the performances of different feed forward and recurrent neural networks and training algorithms for predicting the exchange rate EUR/RON and USD/RON. We used data series with daily exchange rates starting from 2005 until 2013. ,Computer Science - Neural and Evolutionary Computing ; ,"Oancea, Bogdan ; Ciucu, ŞTefan Cristian ; "
http://arxiv.org/abs/1401.1671,Distributed Energy Efficient Channel Allocation,"  Design of energy efficient protocols for modern wireless systems has become an important area of research. In this paper, we propose a distributed optimization algorithm for the channel assignment problem for multiple interfering transceiver pairs that cannot communicate with each other. We first modify the auction algorithm for maximal energy efficiency and show that the problem can be solved without explicit message passing using the carrier sense multiple access (CSMA) protocols. We then develop a novel scheme by converting the channel assignment problem into perfect matchings on bipartite graphs. The proposed scheme improves the energy efficiency and does not require any explicit message passing or a shared memory between the users. We derive bounds on the convergence rate and show that the proposed algorithm converges faster than the distributed auction algorithm and achieves near-optimal performance under Rayleigh fading channels. We also present an asymptotic performance analysis of the fast matching algorithm for energy efficient resource allocation and prove the optimality for large enough number of users and number of channels. Finally, we provide numerical assessments that confirm the energy efficiency gains compared to the state of the art. ",Computer Science - Networking and Internet Architecture ; Computer Science - Information Theory ; ,"Naparstek, Oshri ; Zafaruddin, S. M. ; Leshem, Amir ; Jorswieck, Eduard ; "
http://arxiv.org/abs/1401.1861,Empirical Patterns in Google Scholar Citation Counts,"  Scholarly impact may be metricized using an author's total number of citations as a stand-in for real worth, but this measure varies in applicability between disciplines. The detail of the number of citations per publication is nowadays mapped in much more detail on the Web, exposing certain empirical patterns. This paper explores those patterns, using the citation data from Google Scholar for a number of authors. ","Computer Science - Digital Libraries ; 62P99, 01A90 ; I.5.1 ; I.7.5 ; ","Breuer, Peter T. ; Bowen, Jonathan P. ; "
http://arxiv.org/abs/1401.2411,"Clustering, Coding, and the Concept of Similarity","  This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$, which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$. We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data. ",Computer Science - Machine Learning ; ,"McCarty, L. Thorne ; "
http://arxiv.org/abs/1401.3580,Bits Through Bufferless Queues,"  This paper investigates the capacity of a channel in which information is conveyed by the timing of consecutive packets passing through a queue with independent and identically distributed service times. Such timing channels are commonly studied under the assumption of a work-conserving queue. In contrast, this paper studies the case of a bufferless queue that drops arriving packets while a packet is in service. Under this bufferless model, the paper provides upper bounds on the capacity of timing channels and establishes achievable rates for the case of bufferless M/M/1 and M/G/1 queues. In particular, it is shown that a bufferless M/M/1 queue at worst suffers less than 10% reduction in capacity when compared to an M/M/1 work-conserving queue. ",Computer Science - Information Theory ; ,"Tavan, Mehrnaz ; Yates, Roy D. ; Bajwa, Waheed U. ; "
http://arxiv.org/abs/1401.3667,Group Testing with Prior Statistics,"  We consider a new group testing model wherein each item is a binary random variable defined by an a priori probability of being defective. We assume that each probability is small and that items are independent, but not necessarily identically distributed. The goal of group testing algorithms is to identify with high probability the subset of defectives via non-linear (disjunctive) binary measurements. Our main contributions are two classes of algorithms: (1) adaptive algorithms with tests based either on a maximum entropy principle, or on a Shannon-Fano/Huffman code; (2) non-adaptive algorithms. Under loose assumptions and with high probability, our algorithms only need a number of measurements that is close to the information-theoretic lower bound, up to an explicitly-calculated universal constant factor. We provide simulations to support our results. ",Computer Science - Information Theory ; ,"Li, Tongxin ; Chan, Chun Lam ; Huang, Wenhao ; Kaced, Tarik ; Jaggi, Sidharth ; "
http://arxiv.org/abs/1401.5277,Towards a Uniform Theory of Effectful State Machines,"  We use recent developments on coalgebraic and monad-based semantics to obtain a generic notion of a T-automaton, where T is a monad. This enables a uniform study of various notions of machines: e.g. finite state machines, multi-stack machines, Turing machines, valence automata, and weighted automata. We use the generalized powerset construction to define a generic language semantics for T-automata, and we show by numerous examples that it correctly instantiates for some known classes of machines/languages, including regular, context-free, recursively-enumerable and various subclasses of context free languages (e.g. deterministic and real-time ones). Moreover, our approach provides new generic techniques for studying expressivity power of various machine-based models. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; ,"Goncharov, Sergey ; Milius, Stefan ; Silva, Alexandra ; "
http://arxiv.org/abs/1401.5791,Advanced Signal Processing Techniqes to Study Normal and Epileptic EEG,"  EEG monitoring has an important milestone provide valuable information of those candidates who suffer from epilepsy.In this paper human normal and epileptic Electroencephalogram signals are analyzed with popular and efficient signal processing techniques like Fourier and Wavelet transform. The delta, theta, alpha, beta and gamma sub bands of EEG are obtained and studied for detection of seizure and epilepsy. The extracted feature is then applied to ANN for classification of the EEG signals. ","Computer Science - Computational Engineering, Finance, and Science ; ","Dash, Debadatta ; "
http://arxiv.org/abs/1401.6312,Predicate Logic as a Modelling Language: The IDP System,"  With the technology of the time, Kowalski's seminal 1974 paper {\em Predicate Logic as a Programming Language} was a breakthrough for the use of logic in computer science. It introduced two fundamental ideas: on the declarative side, the use of the Horn clause logic fragment of classical logic, which was soon extended with negation as failure, on the procedural side the procedural interpretation which made it possible to write algorithms in the formalism.   Since then, strong progress was made both on the declarative understanding of the logic programming formalism and in automated reasoning technologies, particularly in SAT solving, Constraint Programming and Answer Set Programming. This has paved the way for the development of an extension of logic programming that embodies a more pure view of logic as a modelling language and its role for problem solving.   In this paper, we present the \idp language and system. The language is essentially classical logic extended with one of logic programmings most important contributions to knowledge representation: the representation of complex definitions as rule sets under well-founded semantics. The system is a knowledge base system: a system in which complex declarative information is stored in a knowledge base which can be used to solve different computational problems by applying multiple forms of inference. In this view, theories are declarative modellings, bags of information, descriptions of possible states of affairs. They are neither procedures nor descriptions of computational problems. As such, the \idp language and system preserve the fundamental idea of a declarative reading of logic programs, while they break with the fundamental idea of the procedural interpretation of logic programs. ",Computer Science - Logic in Computer Science ; ,"De Cat, Broes ; Bogaerts, Bart ; Bruynooghe, Maurice ; Janssens, Gerda ; Denecker, Marc ; "
http://arxiv.org/abs/1401.6681,On giant components and treewidth in the layers model,"  Given an undirected $n$-vertex graph $G(V,E)$ and an integer $k$, let $T_k(G)$ denote the random vertex induced subgraph of $G$ generated by ordering $V$ according to a random permutation $\pi$ and including in $T_k(G)$ those vertices with at most $k-1$ of their neighbors preceding them in this order. The distribution of subgraphs sampled in this manner is called the \emph{layers model with parameter} $k$. The layers model has found applications in studying $\ell$-degenerate subgraphs, the design of algorithms for the maximum independent set problem, and in bootstrap percolation.   In the current work we expand the study of structural properties of the layers model.   We prove that there are $3$-regular graphs $G$ for which with high probability $T_3(G)$ has a connected component of size $\Omega(n)$. Moreover, this connected component has treewidth $\Omega(n)$. This lower bound on the treewidth extends to many other random graph models. In contrast, $T_2(G)$ is known to be a forest (hence of treewidth~1), and we establish that if $G$ is of bounded degree then with high probability the largest connected component in $T_2(G)$ is of size $O(\log n)$. We also consider the infinite two-dimensional grid, for which we prove that the first four layers contain a unique infinite connected component with probability $1$. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Feige, Uriel ; Hermon, Jonathan ; Reichman, Daniel ; "
http://arxiv.org/abs/1401.7860,Motion planning and control of a planar polygonal linkage,"  For a polygonal linkage, we produce a fast navigation algorithm on its configuration space. The basic idea is to approximate the configuration space by the vertex-edge graph of its cell decomposition discovered by the first author. The algorithm has three aspects: (1) the number of navigation steps does not exceed 15 (independent of the linkage), (2) each step is a disguised flex of a quadrilateral from one triangular configuration to another, which is a well understood type of flex, and (3) each step can be performed explicitly by adding some extra bars and obtaining a mechanism with one degree of freedom. ",Mathematics - Metric Geometry ; Computer Science - Robotics ; 57Q99 52C99 57Q55 ; ,"Panina, Gaiane ; Siersma, Dirk ; "
http://arxiv.org/abs/1401.8219,On the Properties of the Priority Deriving Procedure in the Pairwise   Comparisons Method,"  The pairwise comparisons method is a convenient tool used when the relative order of preferences among different concepts (alternatives) needs to be determined. There are several popular implementations of this method, including the Eigenvector Method, the Least Squares Method, the Chi Squares Method and others. Each of the above methods comes with one or more inconsistency indices that help to decide whether the consistency of input guarantees obtaining a reliable output, thus taking the optimal decision. This article explores the relationship between inconsistency of input and discrepancy of output. A global ranking discrepancy describes to what extent the obtained results correspond to the single expert's assessments. On the basis of the inconsistency and discrepancy indices, two properties of the weight deriving procedure are formulated. These properties are proven for Eigenvector Method and Koczkodaj's Inconsistency Index. Several estimates using Koczkodaj's Inconsistency Index for a principal eigenvalue, Saaty's inconsistency index and the Condition of Order Preservation are also provided. ",Computer Science - Discrete Mathematics ; Computer Science - Computer Science and Game Theory ; ,"Kułakowski, Konrad ; "
http://arxiv.org/abs/1402.0485,Local algorithms for independent sets are half-optimal,"  We show that the largest density of factor of i.i.d. independent sets on the d-regular tree is asymptotically at most (log d)/d as d tends to infinity. This matches the lower bound given by previous constructions. It follows that the largest independent sets given by local algorithms on random d-regular graphs have the same asymptotic density. In contrast, the density of the largest independent sets on these graphs is asymptotically 2(log d)/d. We also prove analogous results for Poisson-Galton-Watson trees, which yield bounds for local algorithms on sparse Erdos-Renyi graphs. ","Mathematics - Probability ; Computer Science - Distributed, Parallel, and Cluster Computing ; Mathematics - Combinatorics ; ","Rahman, Mustazee ; Virag, Balint ; "
http://arxiv.org/abs/1402.1526,Dual Query: Practical Private Query Release for High Dimensional Data,"  We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude. ",Computer Science - Data Structures and Algorithms ; Computer Science - Cryptography and Security ; Computer Science - Databases ; Computer Science - Machine Learning ; ,"Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Roth, Aaron ; Wu, Zhiwei Steven ; "
http://arxiv.org/abs/1402.1607,Generalized Signal Alignment For MIMO Two-Way X Relay Channels,"  We study the degrees of freedom (DoF) of MIMO two-way X relay channels. Previous work studied the case $N < 2M$, where $N$ and $M$ denote the number of antennas at the relay and each source, respectively, and showed that the maximum DoF of $2N$ is achievable when $N \leq \lfloor\frac{8M}{5}\rfloor$ by applying signal alignment (SA) for network coding and interference cancelation. This work considers the case $N>2M$ where the performance is limited by the number of antennas at each source node and conventional SA is not feasible. We propose a \textit{generalized signal alignment} (GSA) based transmission scheme. The key is to let the signals to be exchanged between every source node align in a transformed subspace, rather than the direct subspace, at the relay so as to form network-coded signals. This is realized by jointly designing the precoding matrices at all source nodes and the processing matrix at the relay. Moreover, the aligned subspaces are orthogonal to each other. By applying the GSA, we show that the DoF upper bound $4M$ is achievable when $M \leq \lfloor\frac{2N}{5}\rfloor$ ($M$ is even) or $M \leq \lfloor\frac{2N-1}{5}\rfloor$ ($M$ is odd). Numerical results also demonstrate that our proposed transmission scheme is feasible and effective. ",Computer Science - Information Theory ; ,"Liu, Kangqi ; Tao, Meixia ; Xiang, Zhengzheng ; Long, Xin ; "
http://arxiv.org/abs/1402.1794,In silico Proteome Cleavage Reveals Iterative Digestion Strategy for   High Sequence Coverage,"  In the post-genome era, biologists have sought to measure the complete complement of proteins, termed proteomics. Currently, the most effective method to measure the proteome is with shotgun, or bottom-up, proteomics, in which the proteome is digested into peptides that are identified followed by protein inference. Despite continuous improvements to all steps of the shotgun proteomics workflow, observed proteome coverage is often low; some proteins are identified by a single peptide sequence. Complete proteome sequence coverage would allow comprehensive characterization of RNA splicing variants and all post translational modifications, which would drastically improve the accuracy of biological models. There are many reasons for the sequence coverage deficit, but ultimately peptide length determines sequence observability. Peptides that are too short are lost because they match many protein sequences and their true origin is ambiguous. The maximum observable peptide length is determined by several analytical challenges. This paper explores computationally how peptide lengths produced from several common proteome digestion methods limit observable proteome coverage. Iterative proteome cleavage strategies are also explored. These simulations reveal that maximized proteome coverage can be achieved by use of an iterative digestion protocol involving multiple proteases and chemical cleavages that theoretically allow 91.1% proteome coverage. ","Quantitative Biology - Genomics ; Computer Science - Computational Engineering, Finance, and Science ; ","Meyer, Jesse G. ; "
http://arxiv.org/abs/1402.2016,Leveraging Long-Term Predictions and Online-Learning in Agent-based   Multiple Person Tracking,"  We present a multiple-person tracking algorithm, based on combining particle filters and RVO, an agent-based crowd model that infers collision-free velocities so as to predict pedestrian's motion. In addition to position and velocity, our tracking algorithm can estimate the internal goals (desired destination or desired velocity) of the tracked pedestrian in an online manner, thus removing the need to specify this information beforehand. Furthermore, we leverage the longer-term predictions of RVO by deriving a higher-order particle filter, which aggregates multiple predictions from different prior time steps. This yields a tracker that can recover from short-term occlusions and spurious noise in the appearance model. Experimental results show that our tracking algorithm is suitable for predicting pedestrians' behaviors online without needing scene priors or hand-annotated goal information, and improves tracking in real-world crowded scenes under low frame rates. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Liu, Wenxi ; Chan, Antoni B. ; Lau, Rynson W. H. ; Manocha, Dinesh ; "
http://arxiv.org/abs/1402.2760,Rendezvous in Networks in Spite of Delay Faults,"  Two mobile agents, starting from different nodes of an unknown network, have to meet at the same node. Agents move in synchronous rounds using a deterministic algorithm. Each agent has a different label, which it can use in the execution of the algorithm, but it does not know the label of the other agent. Agents do not know any bound on the size of the network. In each round an agent decides if it remains idle or if it wants to move to one of the adjacent nodes. Agents are subject to delay faults: if an agent incurs a fault in a given round, it remains in the current node, regardless of its decision. If it planned to move and the fault happened, the agent is aware of it. We consider three scenarios of fault distribution: random (independently in each round and for each agent with constant probability 0 < p < 1), unbounded adver- sarial (the adversary can delay an agent for an arbitrary finite number of consecutive rounds) and bounded adversarial (the adversary can delay an agent for at most c consecutive rounds, where c is unknown to the agents). The quality measure of a rendezvous algorithm is its cost, which is the total number of edge traversals. For random faults, we show an algorithm with cost polynomial in the size n of the network and polylogarithmic in the larger label L, which achieves rendezvous with very high probability in arbitrary networks. By contrast, for unbounded adversarial faults we show that rendezvous is not feasible, even in the class of rings. Under this scenario we give a rendezvous algorithm with cost O(nl), where l is the smaller label, working in arbitrary trees, and we show that \Omega(l) is the lower bound on rendezvous cost, even for the two-node tree. For bounded adversarial faults, we give a rendezvous algorithm working for arbitrary networks, with cost polynomial in n, and logarithmic in the bound c and in the larger label L. ",Computer Science - Data Structures and Algorithms ; ,"Chalopin, Jérémie ; Dieudonné, Yoann ; Labourel, Arnaud ; Pelc, Andrzej ; "
http://arxiv.org/abs/1402.3175,Information-Geometric Equivalence of Transportation Polytopes,"  This paper deals with transportation polytopes in the probability simplex (that is, sets of categorical bivariate probability distributions with prescribed marginals). Information projections between such polytopes are studied, and a sufficient condition is described under which these mappings are homeomorphisms. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; 94A17, 52B11, 52B12, 62B10, 62H17, 54C99 ; ","Kovačević, Mladen ; Stanojević, Ivan ; Šenk, Vojin ; "
http://arxiv.org/abs/1402.3210,On the Convergence of Approximate Message Passing with Arbitrary   Matrices,"  Approximate message passing (AMP) methods and their variants have attracted considerable recent attention for the problem of estimating a random vector $\mathbf{x}$ observed through a linear transform $\mathbf{A}$. In the case of large i.i.d. zero-mean Gaussian $\mathbf{A}$, the methods exhibit fast convergence with precise analytic characterizations on the algorithm behavior. However, the convergence of AMP under general transforms $\mathbf{A}$ is not fully understood. In this paper, we provide sufficient conditions for the convergence of a damped version of the generalized AMP (GAMP) algorithm in the case of quadratic cost functions (i.e., Gaussian likelihood and prior). It is shown that, with sufficient damping, the algorithm is guaranteed to converge, although the amount of damping grows with peak-to-average ratio of the squared singular values of the transforms $\mathbf{A}$. This result explains the good performance of AMP on i.i.d. Gaussian transforms $\mathbf{A}$, but also their difficulties with ill-conditioned or non-zero-mean transforms $\mathbf{A}$. A related sufficient condition is then derived for the local stability of the damped GAMP method under general cost functions, assuming certain strict convexity conditions. ",Computer Science - Information Theory ; ,"Rangan, Sundeep ; Schniter, Philip ; Fletcher, Alyson K. ; Sarkar, Subrata ; "
http://arxiv.org/abs/1402.3329,Differential Privacy: An Economic Method for Choosing Epsilon,"  Differential privacy is becoming a gold standard for privacy research; it offers a guaranteed bound on loss of privacy due to release of query results, even under worst-case assumptions. The theory of differential privacy is an active research area, and there are now differentially private algorithms for a wide range of interesting problems.   However, the question of when differential privacy works in practice has received relatively little attention. In particular, there is still no rigorous method for choosing the key parameter $\epsilon$, which controls the crucial tradeoff between the strength of the privacy guarantee and the accuracy of the published results.   In this paper, we examine the role that these parameters play in concrete applications, identifying the key questions that must be addressed when choosing specific values. This choice requires balancing the interests of two different parties: the data analyst and the prospective participant, who must decide whether to allow their data to be included in the analysis. We propose a simple model that expresses this balance as formulas over a handful of parameters, and we use our model to choose $\epsilon$ on a series of simple statistical studies. We also explore a surprising insight: in some circumstances, a differentially private study can be more accurate than a non-private study for the same cost, under our model. Finally, we discuss the simplifying assumptions in our model and outline a research agenda for possible refinements. ",Computer Science - Databases ; ,"Hsu, Justin ; Gaboardi, Marco ; Haeberlen, Andreas ; Khanna, Sanjeev ; Narayan, Arjun ; Pierce, Benjamin C. ; Roth, Aaron ; "
http://arxiv.org/abs/1402.3427,Indian Buffet Process Deep Generative Models for Semi-Supervised   Classification,"  Deep generative models (DGMs) have brought about a major breakthrough, as well as renewed interest, in generative latent variable models. However, DGMs do not allow for performing data-driven inference of the number of latent features needed to represent the observed data. Traditional linear formulations address this issue by resorting to tools from the field of nonparametric statistics. Indeed, linear latent variable models imposed an Indian Buffet Process (IBP) prior have been extensively studied by the machine learning community; inference for such models can been performed either via exact sampling or via approximate variational techniques. Based on this inspiration, in this paper we examine whether similar ideas from the field of Bayesian nonparametrics can be utilized in the context of modern DGMs in order to address the latent variable dimensionality inference problem. To this end, we propose a novel DGM formulation, based on the imposition of an IBP prior. We devise an efficient Black-Box Variational inference algorithm for our model, and exhibit its efficacy in a number of semi-supervised classification experiments. In all cases, we use popular benchmark datasets, and compare to state-of-the-art DGMs. ",Computer Science - Machine Learning ; ,"Chatzis, Sotirios P. ; "
http://arxiv.org/abs/1402.3631,Privately Solving Linear Programs,"  In this paper, we initiate the systematic study of solving linear programs under differential privacy. The first step is simply to define the problem: to this end, we introduce several natural classes of private linear programs that capture different ways sensitive data can be incorporated into a linear program. For each class of linear programs we give an efficient, differentially private solver based on the multiplicative weights framework, or we give an impossibility result. ",Computer Science - Data Structures and Algorithms ; Computer Science - Cryptography and Security ; Computer Science - Machine Learning ; ,"Hsu, Justin ; Roth, Aaron ; Roughgarden, Tim ; Ullman, Jonathan ; "
http://arxiv.org/abs/1402.4178,A reclaimer scheduling problem arising in coal stockyard management,"  We study a number of variants of an abstract scheduling problem inspired by the scheduling of reclaimers in the stockyard of a coal export terminal. We analyze the complexity of each of the variants, providing complexity proofs for some and polynomial algorithms for others. For one, especially interesting variant, we also develop a constant factor approximation algorithm. ",Computer Science - Data Structures and Algorithms ; ,"Angelelli, Enrico ; Kalinowski, Thomas ; Kapoor, Reena ; Savelsbergh, Martin W. P. ; "
http://arxiv.org/abs/1402.4327,Unification and Logarithmic Space,"  We present an algebraic characterization of the complexity classes Logspace and NLogspace, using an algebra with a composition law based on unification. This new bridge between unification and complexity classes is inspired from proof theory and more specifically linear logic and Geometry of Interaction.   We show how unification can be used to build a model of computation by means of specific subalgebras associated to finite permutations groups. We then prove that whether an observation (the algebraic counterpart of a program) accepts a word can be decided within logarithmic space. We also show that the construction can naturally represent pointer machines, an intuitive way of understanding logarithmic space computing. ",Computer Science - Logic in Computer Science ; ,"Aubert, Clément ; Bagnol, Marc ; "
http://arxiv.org/abs/1402.4338,Proof Complexity and the Kneser-Lov\'asz Theorem,"  We investigate the proof complexity of a class of propositional formulas expressing a combinatorial principle known as the Kneser-Lov\'{a}sz Theorem. This is a family of propositional tautologies, indexed by an nonnegative integer parameter $k$ that generalizes the Pigeonhole Principle (obtained for $k=1$).   We show, for all fixed $k$, $2^{\Omega(n)}$ lower bounds on resolution complexity and exponential lower bounds for bounded depth Frege proofs. These results hold even for the more restricted class of formulas encoding Schrijver's strenghtening of the Kneser-Lov\'{a}sz Theorem. On the other hand for the cases $k=2,3$ (for which combinatorial proofs of the Kneser-Lov\'{a}sz Theorem are known) we give polynomial size Frege ($k=2$), respectively extended Frege ($k=3$) proofs. The paper concludes with a brief announcement of the results (presented in subsequent work) on the proof complexity of the general case of the Kneser-Lov\'{a}sz theorem. ",Computer Science - Computational Complexity ; Computer Science - Logic in Computer Science ; ,"Istrate, Gabriel ; Crăciun, Adrian ; "
http://arxiv.org/abs/1402.5208,Densely Entangled Financial Systems,"  In [1] Zawadoski introduces a banking network model in which the asset and counter-party risks are treated separately and the banks hedge their assets risks by appropriate OTC contracts. In his model, each bank has only two counter-party neighbors, a bank fails due to the counter-party risk only if at least one of its two neighbors default, and such a counter-party risk is a low probability event. Informally, the author shows that the banks will hedge their asset risks by appropriate OTC contracts, and, though it may be socially optimal to insure against counter-party risk, in equilibrium banks will {\em not} choose to insure this low probability event.   In this paper, we consider the above model for more general network topologies, namely when each node has exactly 2r counter-party neighbors for some integer r>0. We extend the analysis of [1] to show that as the number of counter-party neighbors increase the probability of counter-party risk also increases, and in particular the socially optimal solution becomes privately sustainable when each bank hedges its risk to at least n/2 banks, where n is the number of banks in the network, i.e., when 2r is at least n/2, banks not only hedge their asset risk but also hedge its counter-party risk. ","Quantitative Finance - Risk Management ; Computer Science - Computational Engineering, Finance, and Science ; 91G99, 91B30 ; J.1 ; J.4 ; ","DasGupta, Bhaskar ; Kaligounder, Lakshmi ; "
http://arxiv.org/abs/1402.5481,From Predictive to Prescriptive Analytics,"  In this paper, we combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with specific methods, for using data to prescribe optimal decisions in OR/MS problems. In a departure from other work on data-driven optimization and reflecting our practical experience with the data available in applications of OR/MS, we consider data consisting, not only of observations of quantities with direct effect on costs/revenues, such as demand or returns, but predominantly of observations of associated auxiliary quantities. The main problem of interest is a conditional stochastic optimization problem, given imperfect observations, where the joint probability distributions that specify the problem are unknown. We demonstrate that our proposed solution methods, which are inspired by ML methods such as local regression, CART, and random forests, are generally applicable to a wide range of decision problems. We prove that they are tractable and asymptotically optimal even when data is not iid and may be censored. We extend this to the case where decision variables may directly affect uncertainty in unknown ways, such as pricing's effect on demand. As an analogue to R^2, we develop a metric P termed the coefficient of prescriptiveness to measure the prescriptive content of data and the efficacy of a policy from an operations perspective. To demonstrate the power of our approach in a real-world setting we study an inventory management problem faced by the distribution arm of an international media conglomerate, which ships an average of 1bil units per year. We leverage internal data and public online data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational decisions that outperform baseline measures. Specifically, the data we collect, leveraged by our methods, accounts for an 88\% improvement as measured by our P. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Mathematics - Optimization and Control ; ,"Bertsimas, Dimitris ; Kallus, Nathan ; "
http://arxiv.org/abs/1402.6208,The Anatomy of a Modular System for Media Content Analysis,"  Intelligent systems for the annotation of media content are increasingly being used for the automation of parts of social science research. In this domain the problem of integrating various Artificial Intelligence (AI) algorithms into a single intelligent system arises spontaneously. As part of our ongoing effort in automating media content analysis for the social sciences, we have built a modular system by combining multiple AI modules into a flexible framework in which they can cooperate in complex tasks. Our system combines data gathering, machine translation, topic classification, extraction and annotation of entities and social networks, as well as many other tasks that have been perfected over the past years of AI research. Over the last few years, it has allowed us to realise a series of scientific studies over a vast range of applications including comparative studies between news outlets and media content in different countries, modelling of user preferences, and monitoring public mood. The framework is flexible and allows the design and implementation of modular agents, where simple modules cooperate in the annotation of a large dataset without central coordination. ","Computer Science - Multiagent Systems ; Computer Science - Artificial Intelligence ; Computer Science - Distributed, Parallel, and Cluster Computing ; ","Flaounas, Ilias ; Lansdall-Welfare, Thomas ; Antonakaki, Panagiota ; Cristianini, Nello ; "
http://arxiv.org/abs/1402.6787,Learning multifractal structure in large networks,"  Generating random graphs to model networks has a rich history. In this paper, we analyze and improve upon the multifractal network generator (MFNG) introduced by Palla et al. We provide a new result on the probability of subgraphs existing in graphs generated with MFNG. From this result it follows that we can quickly compute moments of an important set of graph properties, such as the expected number of edges, stars, and cliques. Specifically, we show how to compute these moments in time complexity independent of the size of the graph and the number of recursive levels in the generative model. We leverage this theory to a new method of moments algorithm for fitting large networks to MFNG. Empirically, this new approach effectively simulates properties of several social and information networks. In terms of matching subgraph counts, our method outperforms similar algorithms used with the Stochastic Kronecker Graph model. Furthermore, we present a fast approximation algorithm to generate graph instances following the multi- fractal structure. The approximation scheme is an improvement over previous methods, which ran in time complexity quadratic in the number of vertices. Combined, our method of moments and fast sampling scheme provide the first scalable framework for effectively modeling large networks with MFNG. ",Computer Science - Social and Information Networks ; H.4.0 ; E.1 ; ,"Benson, Austin R. ; Riquelme, Carlos ; Schmit, Sven ; "
http://arxiv.org/abs/1402.6964,Scalable methods for nonnegative matrix factorizations of near-separable   tall-and-skinny matrices,"  Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms efficient for data matrices that have many more rows than columns, so-called ""tall-and-skinny matrices"". One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need a single pass over the data matrix and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized synthetic matrices and real-world matrices from scientific computing and bioinformatics. ","Computer Science - Machine Learning ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Numerical Analysis ; Statistics - Machine Learning ; G.1.3 ; G.1.6 ; ","Benson, Austin R. ; Lee, Jason D. ; Rajwa, Bartek ; Gleich, David F. ; "
http://arxiv.org/abs/1402.7242,Percolation with small clusters on random graphs,"  Consider the problem of determining the maximal induced subgraph in a random $d$-regular graph such that its components remain bounded as the size of the graph becomes arbitrarily large. We show, for asymptotically large $d$, that any such induced subgraph has size density at most $2(\log d)/d$ with high probability. A matching lower bound is known for independent sets. We also prove the analogous result for sparse Erd\H{o}s-R\'{e}nyi graphs. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Rahman, Mustazee ; "
http://arxiv.org/abs/1403.0505,A search for quantum coin-flipping protocols using optimization   techniques,"  Coin-flipping is a cryptographic task in which two physically separated, mistrustful parties wish to generate a fair coin-flip by communicating with each other. Chailloux and Kerenidis (2009) designed quantum protocols that guarantee coin-flips with near optimal bias. The probability of any outcome in these protocols is provably at most $1/\sqrt{2} + \delta$ for any given $\delta > 0$. However, no explicit description of these protocols is known, and the number of rounds in the protocols tends to infinity as $\delta$ goes to 0. In fact, the smallest bias achieved by known explicit protocols is $1/4$ (Ambainis, 2001).   We take a computational optimization approach, based mostly on convex optimization, to the search for simple and explicit quantum strong coin-flipping protocols. We present a search algorithm to identify protocols with low bias within a natural class, protocols based on bit-commitment (Nayak and Shor, 2003) restricting to commitment states used by Mochon (2005). An analysis of the resulting protocols via semidefinite programs (SDPs) unveils a simple structure. For example, we show that the SDPs reduce to second-order cone programs. We devise novel cheating strategies in the protocol by restricting the semidefinite programs and use the strategies to prune the search.   The techniques we develop enable a computational search for protocols given by a mesh over the parameter space. The protocols have up to six rounds of communication, with messages of varying dimension and include the best known explicit protocol (with bias 1/4). We conduct two kinds of search: one for protocols with bias below 0.2499, and one for protocols in the neighbourhood of protocols with bias 1/4. Neither of these searches yields better bias. Based on the mathematical ideas behind the search algorithm, we prove a lower bound on the bias of a class of four-round protocols. ",Mathematics - Optimization and Control ; Computer Science - Cryptography and Security ; Quantum Physics ; ,"Nayak, Ashwin ; Sikora, Jamie ; Tunçel, Levent ; "
http://arxiv.org/abs/1403.0734,Clique counting in MapReduce: theory and experiments,"  We tackle the problem of counting the number of $k$-cliques in large-scale graphs, for any constant $k \ge 3$. Clique counting is essential in a variety of applications, among which social network analysis. Due to its computationally intensive nature, we settle for parallel solutions in the MapReduce framework, which has become in the last few years a {\em de facto} standard for batch processing of massive data sets. We give both theoretical and experimental contributions.   On the theory side, we design the first exact scalable algorithm for counting (and listing) $k$-cliques. Our algorithm uses $O(m^{3/2})$ total space and $O(m^{k/2})$ work, where $m$ is the number of graph edges. This matches the best-known bounds for triangle listing when $k=3$ and is work-optimal in the worst case for any $k$, while keeping the communication cost independent of $k$. We also design a sampling-based estimator that can dramatically reduce the running time and space requirements of the exact approach, while providing very accurate solutions with high probability.   We then assess the effectiveness of different clique counting approaches through an extensive experimental analysis over the Amazon EC2 platform, considering both our algorithms and their state-of-the-art competitors. The experimental results clearly highlight the algorithm of choice in different scenarios and prove our exact approach to be the most effective when the number of $k$-cliques is large, gracefully scaling to non-trivial values of $k$ even on clusters of small/medium size. Our approximation algorithm achieves extremely accurate estimates and large speedups, especially on the toughest instances for the exact algorithms. As a side effect, our study also sheds light on the number of $k$-cliques of several real-world graphs, mainly social networks, and on its growth rate as a function of $k$. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; ","Finocchi, Irene ; Finocchi, Marco ; Fusco, Emanuele G. ; "
http://arxiv.org/abs/1403.1080,New Ideas for Brain Modelling,"  This paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain. New to this paper, a 'refined' neuron will be proposed. This is a group of neurons that by joining together can produce a more analogue system, but with the same level of control and reliability that a binary neuron would have. With this new structure, it will be possible to think of an essentially binary system in terms of a more variable set of values. The paper also shows how recent research associated with the new model, can be combined with established theories, to produce a more complete picture. The propositions are largely in line with conventional thinking, but possibly with one or two more radical suggestions. An earlier cognitive model can be filled in with more specific details, based on the new research results, where the components appear to fit together almost seamlessly. The intention of the research has been to describe plausible 'mechanical' processes that can produce the appropriate brain structures and mechanisms, but that could be used without the magical 'intelligence' part that is still not fully understood. There are also some important updates from an earlier version of this paper. ",Computer Science - Artificial Intelligence ; Quantitative Biology - Neurons and Cognition ; ,"Greer, Kieran ; "
http://arxiv.org/abs/1403.1142,Automated analysis of security protocols with global state,"  Security APIs, key servers and protocols that need to keep the status of transactions, require to maintain a global, non-monotonic state, e.g., in the form of a database or register. However, most existing automated verification tools do not support the analysis of such stateful security protocols - sometimes because of fundamental reasons, such as the encoding of the protocol as Horn clauses, which are inherently monotonic. A notable exception is the recent tamarin prover which allows specifying protocols as multiset rewrite (msr) rules, a formalism expressive enough to encode state. As multiset rewriting is a ""low-level"" specification language with no direct support for concurrent message passing, encoding protocols correctly is a difficult and error-prone process. We propose a process calculus which is a variant of the applied pi calculus with constructs for manipulation of a global state by processes running in parallel. We show that this language can be translated to msr rules whilst preserving all security properties expressible in a dedicated first-order logic for security properties. The translation has been implemented in a prototype tool which uses the tamarin prover as a backend. We apply the tool to several case studies among which a simplified fragment of PKCS\#11, the Yubikey security token, and an optimistic contract signing protocol. ",Computer Science - Cryptography and Security ; ,"Kremer, Steve ; Künnemann, Robert ; "
http://arxiv.org/abs/1403.1639,Optimal Patching in Clustered Malware Epidemics,"  Studies on the propagation of malware in mobile networks have revealed that the spread of malware can be highly inhomogeneous. Platform diversity, contact list utilization by the malware, clustering in the network structure, etc. can also lead to differing spreading rates. In this paper, a general formal framework is proposed for leveraging such heterogeneity to derive optimal patching policies that attain the minimum aggregate cost due to the spread of malware and the surcharge of patching. Using Pontryagin's Maximum Principle for a stratified epidemic model, it is analytically proven that in the mean-field deterministic regime, optimal patch disseminations are simple single-threshold policies. Through numerical simulations, the behavior of optimal patching policies is investigated in sample topologies and their advantages are demonstrated. ",Computer Science - Cryptography and Security ; Computer Science - Networking and Internet Architecture ; Computer Science - Social and Information Networks ; Computer Science - Systems and Control ; Mathematics - Optimization and Control ; ,"Eshghi, Soheil ; Khouzani, MHR. ; Sarkar, Saswati ; Venkatesh, Santosh S. ; "
http://arxiv.org/abs/1403.1642,Optimal Energy-Aware Epidemic Routing in DTNs,"  In this work, we investigate the use of epidemic routing in energy constrained Delay Tolerant Networks (DTNs). In epidemic routing, messages are relayed by intermediate nodes at contact opportunities, i.e., when pairs of nodes come within the transmission range of each other. Each node needs to decide whether to forward its message upon contact with a new node based on its own residual energy level and the age of that message. We mathematically characterize the fundamental trade-off between energy conservation and a measure of Quality of Service as a dynamic energy-dependent optimal control problem. We prove that in the mean-field regime, the optimal dynamic forwarding decisions follow simple threshold-based structures in which the forwarding threshold for each node depends on its current remaining energy. We then characterize the nature of this dependence. Our simulations reveal that the optimal dynamic policy significantly outperforms heuristics. ","Computer Science - Systems and Control ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Networking and Internet Architecture ; Mathematics - Optimization and Control ; ","Eshghi, Soheil ; Khouzani, MHR. ; Sarkar, Saswati ; Shroff, Ness B. ; Venkatesh, Santosh S. ; "
http://arxiv.org/abs/1403.2975,Optimal ancilla-free Clifford+T approximation of z-rotations,"  We consider the problem of approximating arbitrary single-qubit z-rotations by ancilla-free Clifford+T circuits, up to given epsilon. We present a fast new probabilistic algorithm for solving this problem optimally, i.e., for finding the shortest possible circuit whatsoever for the given problem instance. The algorithm requires a factoring oracle (such as a quantum computer). Even in the absence of a factoring oracle, the algorithm is still near-optimal under a mild number-theoretic hypothesis. In this case, the algorithm finds a solution of T-count m + O(log(log(1/epsilon))), where m is the T-count of the second-to-optimal solution. In the typical case, this yields circuit approximations of T-count 3log_2(1/epsilon) + O(log(log(1/epsilon))). Our algorithm is efficient in practice, and provably efficient under the above-mentioned number-theoretic hypothesis, in the sense that its expected runtime is O(polylog(1/epsilon)). ",Quantum Physics ; Computer Science - Emerging Technologies ; ,"Ross, Neil J. ; Selinger, Peter ; "
http://arxiv.org/abs/1403.3772,Study of Behaviours via Visitable Paths,"  Around 2000, J.-Y. Girard developed a logical theory, called Ludics. This theory was a step in his program of Geometry of Interaction, the aim of which being to account for the dynamics of logical proofs. In Ludics, objects called designs keep only what is relevant for the cut elimination process, hence the dynamics of a proof: a design is an abstraction of a formal proof. The notion of behaviour is the counterpart in Ludics of the notion of type or the logical notion of formula. Formally a behaviour is a closed set of designs. Our aim is to explore the constructions of behaviours and to analyse their properties. In this paper a design is viewed as a set of coherent paths. We recall or give variants of properties concerning visitable paths, where a visitable path is a path in a design or a set of designs that may be traversed by interaction with a design of the orthogonal of the set. We are then able to answer the following question: which properties should satisfy a set of paths for being exactly the set of visitable paths of a behaviour? Such a set and its dual should be prefix-closed, daimon-closed and satisfy two saturation properties. This allows us to have a means for defining the whole set of visitable paths of a given set of designs without closing it explicitly, that is without computing the orthogonal of this set of designs. We finally apply all these results for making explicit the structure of a behaviour generated by constants and multiplicative/additive connectives. We end by proposing an oriented tensor for which we give basic properties. ",Computer Science - Logic in Computer Science ; F.4.1 ; ,"Fouqueré, Christophe ; Quatrini, Myriam ; "
http://arxiv.org/abs/1403.4143,P is not equal to NP by Modus Tollens,"  An artificially designed Turing Machine algorithm $\mathbf{M}_{}^{o}$ generates the instances of the satisfiability problem, and check their satisfiability. Under the assumption $\mathcal{P}=\mathcal{NP}$, we show that $\mathbf{M}_{}^{o}$ has a certain property, which, without the assumption, $\mathbf{M}_{}^{o}$ does not have. This leads to $\mathcal{P}\neq\mathcal{NP}$ $ $ by modus tollens. ",Computer Science - Computational Complexity ; ,"Kim, Joonmo ; "
http://arxiv.org/abs/1403.4539,Occam Bound on Lowest Complexity of Elements,"  The combined universal probability M(D) of strings x in sets D is close to max_{x \in D} M({x}): their ~ logs differ by at most D's information j = I(D:H) about the halting sequence H. Thus if all x have complexity K(x) > k, D carries > i bits of information on each x where i+j ~ k. Note, there are no ways (whether natural or artificial) to generate D with significant I(D:H). ",Computer Science - Computational Complexity ; ,"Levin, Leonid A. ; "
http://arxiv.org/abs/1403.4622,Complete simultaneous conjugacy invariants in Artin's braid groups,"  We solve the simultaneous conjugacy problem in Artin's braid groups and, more generally, in Garside groups, by means of a complete, effectively computable, finite invariant. This invariant generalizes the one-dimensional notion of super summit set to arbitrary dimensions. One key ingredient in our solution is the introduction of a provable high-dimensional version of the Birman--Ko--Lee cycling theorem. The complexity of this solution is a small degree polynomial in the cardinalities of our generalized super summit sets and the input parameters. Computer experiments suggest that the cardinality of this invariant, for a list of order $N$ independent elements of Artin's braid group $B_N$, is generically close to~1. ","Mathematics - Group Theory ; Computer Science - Computational Complexity ; Computer Science - Cryptography and Security ; 20F36, 20F65, 20C40 ; ","Kalka, Arkadius ; Tsaban, Boaz ; Vinokur, Gary ; "
http://arxiv.org/abs/1403.4861,Improved Approximation Algorithms for Box Contact Representations,"  We study the following geometric representation problem: Given a graph whose vertices correspond to axis-aligned rectangles with fixed dimensions, arrange the rectangles without overlaps in the plane such that two rectangles touch if the graph contains an edge between them. This problem is called \textsc{Contact Representation of Word Networks} (\textsc{Crown}) since it formalizes the geometric problem behind drawing word clouds in which semantically related words are close to each other. \textsc{Crown} is known to be NP-hard, and there are approximation algorithms for certain graph classes for the optimization version, \textsc{Max-Crown}, in which realizing each desired adjacency yields a certain profit. We present the first $O(1)$-approximation algorithm for the general case, when the input is a complete weighted graph, and for the bipartite case. Since the subgraph of realized adjacencies is necessarily planar, we also consider several planar graph classes (namely stars, trees, outerplanar, and planar graphs), improving upon the known results. For some graph classes, we also describe improvements in the unweighted case, where each adjacency yields the same profit. Finally, we show that the problem is APX-hard on bipartite graphs of bounded maximum degree. ",Computer Science - Data Structures and Algorithms ; ,"Bekos, Michael A. ; van Dijk, Thomas C. ; Fink, Martin ; Kindermann, Philipp ; Kobourov, Stephen ; Pupyrev, Sergey ; Spoerhase, Joachim ; Wolff, Alexander ; "
http://arxiv.org/abs/1403.5361,Parameter Estimation of Social Forces in Crowd Dynamics Models via a   Probabilistic Method,"  Focusing on a specific crowd dynamics situation, including real life experiments and measurements, our paper targets a twofold aim: (1) we present a Bayesian probabilistic method to estimate the value and the uncertainty (in the form of a probability density function) of parameters in crowd dynamic models from the experimental data; and (2) we introduce a fitness measure for the models to classify a couple of model structures (forces) according to their fitness to the experimental data, preparing the stage for a more general model-selection and validation strategy inspired by probabilistic data analysis. Finally, we review the essential aspects of our experimental setup and measurement technique. ","Physics - Data Analysis, Statistics and Probability ; Computer Science - Social and Information Networks ; Mathematics - Probability ; Mathematics - Statistics Theory ; Physics - Physics and Society ; ","Corbetta, Alessandro ; Muntean, Adrian ; Toschi, Federico ; Vafayi, Kiamars ; "
http://arxiv.org/abs/1403.5543,Disaster Recovery in Wireless Networks: A Homology-Based Algorithm,"  In this paper, we present an algorithm for the recovery of wireless networks after a disaster. Considering a damaged wireless network, presenting coverage holes or/and many disconnected components, we propose a disaster recovery algorithm which repairs the network. It provides the list of locations where to put new nodes in order to patch the coverage holes and mend the disconnected components. In order to do this we first consider the simplicial complex representation of the network, then the algorithm adds supplementary vertices in excessive number, and afterwards runs a reduction algorithm in order to reach an optimal result. One of the novelty of this work resides in the proposed method for the addition of vertices. We use a determinantal point process: the Ginibre point process which has inherent repulsion between vertices, and has never been simulated before for wireless networks representation. We compare both the determinantal point process addition method with other vertices addition methods, and the whole disaster recovery algorithm to the greedy algorithm for the set cover problem. ",Mathematics - Probability ; Computer Science - Networking and Internet Architecture ; ,"Vergne, Anaïs ; Flint, Ian ; Decreusefond, Laurent ; Martins, Philippe ; "
http://arxiv.org/abs/1403.5715,Mining Attribute-Based Access Control Policies from Logs,"  Attribute-based access control (ABAC) provides a high level of flexibility that promotes security and information sharing. ABAC policy mining algorithms have potential to significantly reduce the cost of migration to ABAC, by partially automating the development of an ABAC policy from information about the existing access-control policy and attribute data. This paper presents an algorithm for mining ABAC policies from operation logs and attribute data. To the best of our knowledge, it is the first algorithm for this problem. ",Computer Science - Cryptography and Security ; Computer Science - Databases ; ,"Xu, Zhongyuan ; Stoller, Scott D. ; "
http://arxiv.org/abs/1403.6322,Do the Fix Ingredients Already Exist? An Empirical Inquiry into the   Redundancy Assumptions of Program Repair Approaches,"  Much initial research on automatic program repair has focused on experimental results to probe their potential to find patches and reduce development effort. Relatively less effort has been put into understanding the hows and whys of such approaches. For example, a critical assumption of the GenProg technique is that certain bugs can be fixed by copying and re-arranging existing code. In other words, GenProg assumes that the fix ingredients already exist elsewhere in the code. In this paper, we formalize these assumptions around the concept of ''temporal redundancy''. A temporally redundant commit is only composed of what has already existed in previous commits. Our experiments show that a large proportion of commits that add existing code are temporally redundant. This validates the fundamental redundancy assumption of GenProg. ",Computer Science - Software Engineering ; ,"Martinez, Matias ; Weimer, Westley ; Monperrus, Martin ; "
http://arxiv.org/abs/1404.1008,Spectral concentration and greedy k-clustering,"  A popular graph clustering method is to consider the embedding of an input graph into R^k induced by the first k eigenvectors of its Laplacian, and to partition the graph via geometric manipulations on the resulting metric space. Despite the practical success of this methodology, there is limited understanding of several heuristics that follow this framework. We provide theoretical justification for one such natural and computationally efficient variant.   Our result can be summarized as follows. A partition of a graph is called strong if each cluster has small external conductance, and large internal conductance. We present a simple greedy spectral clustering algorithm which returns a partition that is provably close to a suitably strong partition, provided that such a partition exists. A recent result shows that strong partitions exist for graphs with a sufficiently large spectral gap between the k-th and (k+1)-st eigenvalues. Taking this together with our main theorem gives a spectral algorithm which finds a partition close to a strong one for graphs with large enough spectral gap. We also show how this simple greedy algorithm can be implemented in near-linear time for any fixed k and error guarantee. Finally, we evaluate our algorithm on some real-world and synthetic inputs. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Geometry ; ,"Dey, Tamal K. ; Peng, Pan ; Rossi, Alfred ; Sidiropoulos, Anastasios ; "
http://arxiv.org/abs/1404.1864,Sublinear algorithms for local graph centrality estimation,"  We study the complexity of local graph centrality estimation, with the goal of approximating the centrality score of a given target node while exploring only a sublinear number of nodes/arcs of the graph and performing a sublinear number of elementary operations. We develop a technique, that we apply to the PageRank and Heat Kernel centralities, for building a low-variance score estimator through a local exploration of the graph. We obtain an algorithm that, given any node in any graph of $m$ arcs, with probability $(1-\delta)$ computes a multiplicative $(1\pm\epsilon)$-approximation of its score by examining only $\tilde{O}(\min(m^{2/3} \Delta^{1/3} d^{-2/3},\, m^{4/5} d^{-3/5}))$ nodes/arcs, where $\Delta$ and $d$ are respectively the maximum and average outdegree of the graph (omitting for readability $\operatorname{poly}(\epsilon^{-1})$ and $\operatorname{polylog}(\delta^{-1})$ factors). A similar bound holds for computational complexity. We also prove a lower bound of $\Omega(\min(m^{1/2} \Delta^{1/2} d^{-1/2}, \, m^{2/3} d^{-1/3}))$ for both query complexity and computational complexity. Moreover, our technique yields a $\tilde{O}(n^{2/3})$ query complexity algorithm for the graph access model of [Brautbar et al., 2010], widely used in social network mining; we show this algorithm is optimal up to a sublogarithmic factor. These are the first algorithms yielding worst-case sublinear bounds for general directed graphs and any choice of the target node. ",Computer Science - Data Structures and Algorithms ; Computer Science - Information Retrieval ; Computer Science - Social and Information Networks ; ,"Bressan, Marco ; Peserico, Enoch ; Pretto, Luca ; "
http://arxiv.org/abs/1404.2329,Duality and Optimality of Auctions for Uniform Distributions,"  We develop a general duality-theory framework for revenue maximization in additive Bayesian auctions. The framework extends linear programming duality and complementarity to constraints with partial derivatives. The dual system reveals the geometric nature of the problem and highlights its connection with the theory of bipartite graph matchings. We demonstrate the power of the framework by applying it to a multiple-good monopoly setting where the buyer has uniformly distributed valuations for the items, the canonical long-standing open problem in the area. We propose a deterministic selling mechanism called Straight-Jacket Auction (SJA), which we prove to be exactly optimal for up to 6 items, and conjecture its optimality for any number of goods. The duality framework is used not only for proving optimality, but perhaps more importantly for deriving the optimal mechanism itself; as a result, SJA is defined by natural geometric constraints. ",Computer Science - Computer Science and Game Theory ; ,"Giannakopoulos, Yiannis ; Koutsoupias, Elias ; "
http://arxiv.org/abs/1404.2458,r-Extreme Signalling for Congestion Control,"  In many ""smart city"" applications, congestion arises in part due to the nature of signals received by individuals from a central authority. In the model of Marecek et al. [arXiv:1406.7639, Int. J. Control 88(10), 2015], each agent uses one out of multiple resources at each time instant. The per-use cost of a resource depends on the number of concurrent users. A central authority has up-to-date knowledge of the congestion across all resources and uses randomisation to provide a scalar or an interval for each resource at each time. In this paper, the interval to broadcast per resource is obtained by taking the minima and maxima of costs observed within a time window of length r, rather than by randomisation. We show that the resulting distribution of agents across resources also converges in distribution, under plausible assumptions about the evolution of the population over time. ",Mathematics - Optimization and Control ; Computer Science - Artificial Intelligence ; Computer Science - Multiagent Systems ; ,"Marecek, Jakub ; Shorten, Robert ; Yu, Jia Yuan ; "
http://arxiv.org/abs/1404.2743,Infinite dimensional finitely forcible graphon,"  Graphons are analytic objects associated with convergent sequences of dense graphs. Finitely forcible graphons, i.e., those determined by finitely many subgraph densities, are of particular interest because of their relation to various problems in extremal combinatorics and theoretical computer science. Lovasz and Szegedy conjectured that the topological space of typical vertices of a finitely forcible graphon always has finite dimension, which would have implications on the minimum number of parts in its weak eps-regular partition. We disprove the conjecture by constructing a finitely forcible graphon with the space of typical vertices that has infinite dimension. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Glebov, Roman ; Klimosova, Tereza ; Kral, Daniel ; "
http://arxiv.org/abs/1404.3056,Principles of Antifragile Software,"  The goal of this paper is to study and define the concept of ""antifragile software"". For this, I start from Taleb's statement that antifragile systems love errors, and discuss whether traditional software dependability fits into this class. The answer is somewhat negative, although adaptive fault tolerance is antifragile: the system learns something when an error happens, and always imrpoves. Automatic runtime bug fixing is changing the code in response to errors, fault injection in production means injecting errors in business critical software. I claim that both correspond to antifragility. Finally, I hypothesize that antifragile development processes are better at producing antifragile software systems. ",Computer Science - Software Engineering ; ,"Monperrus, Martin ; "
http://arxiv.org/abs/1404.3186,Automatic Repair of Buggy If Conditions and Missing Preconditions with   SMT,"  We present Nopol, an approach for automatically repairing buggy if conditions and missing preconditions. As input, it takes a program and a test suite which contains passing test cases modeling the expected behavior of the program and at least one failing test case embodying the bug to be repaired. It consists of collecting data from multiple instrumented test suite executions, transforming this data into a Satisfiability Modulo Theory (SMT) problem, and translating the SMT result -- if there exists one -- into a source code patch. Nopol repairs object oriented code and allows the patches to contain nullness checks as well as specific method calls. ",Computer Science - Software Engineering ; ,"Demarco, Favio ; Xuan, Jifeng ; Berre, Daniel Le ; Monperrus, Martin ; "
http://arxiv.org/abs/1404.3311,Generating Synchronizing Automata with Large Reset Lengths,"  We study synchronizing automata with the shortest reset words of relatively large length. First, we refine the Frankl-Pin result on the length of the shortest words of rank $m$, and the B\'eal, Berlinkov, Perrin, and Steinberg results on the length of the shortest reset words in one-cluster automata. The obtained results are useful in computation aimed in extending the class of small automata for which the \v{C}ern\'y conjecture is verified and discovering new automata with special properties regarding synchronization. ",Computer Science - Formal Languages and Automata Theory ; ,"Kisielewicz, Andrzej ; Szykuła, Marek ; "
http://arxiv.org/abs/1404.3368,Near-optimal sample compression for nearest neighbors,"  We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented. ",Computer Science - Machine Learning ; Computer Science - Computational Complexity ; ,"Gottlieb, Lee-Ad ; Kontorovich, Aryeh ; Nisnevitch, Pinhas ; "
http://arxiv.org/abs/1404.3442,Optimal versus Nash Equilibrium Computation for Networked Resource   Allocation,"  Motivated by emerging resource allocation and data placement problems such as web caches and peer-to-peer systems, we consider and study a class of resource allocation problems over a network of agents (nodes). In this model, nodes can store only a limited number of resources while accessing the remaining ones through their closest neighbors. We consider this problem under both optimization and game-theoretic frameworks. In the case of optimal resource allocation we will first show that when there are only k=2 resources, the optimal allocation can be found efficiently in O(n^2\log n) steps, where n denotes the total number of nodes. However, for k>2 this problem becomes NP-hard with no polynomial time approximation algorithm with a performance guarantee better than 1+1/102k^2, even under metric access costs. We then provide a 3-approximation algorithm for the optimal resource allocation which runs only in linear time O(n). Subsequently, we look at this problem under a selfish setting formulated as a noncooperative game and provide a 3-approximation algorithm for obtaining its pure Nash equilibria under metric access costs. We then establish an equivalence between the set of pure Nash equilibria and flip-optimal solutions of the Max-k-Cut problem over a specific weighted complete graph. Using this reduction, we show that finding the lexicographically smallest Nash equilibrium for k> 2 is NP-hard, and provide an algorithm to find it in O(n^3 2^n) steps. While the reduction to weighted Max-k-Cut suggests that finding a pure Nash equilibrium using best response dynamics might be PLS-hard, it allows us to use tools from quadratic programming to devise more systematic algorithms towards obtaining Nash equilibrium points. ",Computer Science - Computer Science and Game Theory ; Computer Science - Discrete Mathematics ; Computer Science - Systems and Control ; Mathematics - Combinatorics ; ,"Etesami, S. Rasoul ; "
http://arxiv.org/abs/1404.3626,Optimal Power Flow as a Polynomial Optimization Problem,  Formulating the alternating current optimal power flow (ACOPF) as a polynomial optimization problem makes it possible to solve large instances in practice and to guarantee asymptotic convergence in theory. ,Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Ghaddar, Bissan ; Marecek, Jakub ; Mevissen, Martin ; "
http://arxiv.org/abs/1404.5029,Using Covert Topological Information for Defense Against Malicious   Attacks on DC State Estimation,"  Accurate state estimation is of paramount importance to maintain the power system operating in a secure and efficient state. The recently identified coordinated data injection attacks to meter measurements can bypass the current security system and introduce errors to the state estimates. The conventional wisdom to mitigate such attacks is by securing meter measurements to evade malicious injections. In this paper, we provide a novel alternative to defend against false-data injection attacks using covert power network topological information. By keeping the exact reactance of a set of transmission lines from attackers, no false data injection attack can be launched to compromise any set of state variables. We first investigate from the attackers' perspective the necessary condition to perform injection attack. Based on the arguments, we characterize the optimal protection problem, which protects the state variables with minimum cost, as a well-studied Steiner tree problem in a graph. Besides, we also propose a mixed defending strategy that jointly considers the use of covert topological information and secure meter measurements when either method alone is costly or unable to achieve the protection objective. A mixed integer linear programming (MILP) formulation is introduced to obtain the optimal mixed defending strategy. To tackle the NP-hardness of the problem, a tree pruning-based heuristic is further presented to produce an approximate solution in polynomial time. The advantageous performance of the proposed defending mechanisms is verified in IEEE standard power system testcases. ",Computer Science - Cryptography and Security ; ,"Bi, Suzhi ; Zhang, Ying Jun ; "
http://arxiv.org/abs/1404.6898,Quantum Attacks on Classical Proof Systems - The Hardness of Quantum   Rewinding,"  Quantum zero-knowledge proofs and quantum proofs of knowledge are inherently difficult to analyze because their security analysis uses rewinding. Certain cases of quantum rewinding are handled by the results by Watrous (SIAM J Comput, 2009) and Unruh (Eurocrypt 2012), yet in general the problem remains elusive. We show that this is not only due to a lack of proof techniques: relative to an oracle, we show that classically secure proofs and proofs of knowledge are insecure in the quantum setting.   More specifically, sigma-protocols, the Fiat-Shamir construction, and Fischlin's proof system are quantum insecure under assumptions that are sufficient for classical security. Additionally, we show that for similar reasons, computationally binding commitments provide almost no security guarantees in a quantum setting.   To show these results, we develop the ""pick-one trick"", a general technique that allows an adversary to find one value satisfying a given predicate, but not two. ",Quantum Physics ; Computer Science - Cryptography and Security ; ,"Ambainis, Andris ; Rosmanis, Ansis ; Unruh, Dominique ; "
http://arxiv.org/abs/1404.7325,Tight Bounds for Restricted Grid Scheduling,"  The following online bin packing problem is considered: Items with integer sizes are given and variable sized bins arrive online. A bin must be used if there is still an item remaining which fits in it when the bin arrives. The goal is to minimize the total size of all the bins used. Previously, a lower bound of 5/4 on the competitive ratio of this problem was achieved using jobs of size S and 2S-1. For these item sizes and maximum bin size 4S-3, we obtain asymptotically matching upper and lower bounds, which vary depending on the ratio of the number of small jobs to the number of large jobs. ",Computer Science - Data Structures and Algorithms ; ,"Boyar, Joan ; Ellen, Faith ; "
http://arxiv.org/abs/1405.0149,Coding Theoretic Construction of Quantum Ramp Secret Sharing,  We show a construction of a quantum ramp secret sharing scheme from a nested pair of linear codes. Necessary and sufficient conditions for qualified sets and forbidden sets are given in terms of combinatorial properties of nested linear codes. An algebraic geometric construction for quantum secret sharing is also given. ,"Computer Science - Information Theory ; Mathematics - Algebraic Geometry ; Mathematics - Combinatorics ; Quantum Physics ; 81P94 (Primary) 94A62, 94B27 (Secondary) ; E.3 ; ","Matsumoto, Ryutaroh ; "
http://arxiv.org/abs/1405.0637,Crux: Locality-Preserving Distributed Services,"  Distributed systems achieve scalability by distributing load across many machines, but wide-area deployments can introduce worst-case response latencies proportional to the network's diameter. Crux is a general framework to build locality-preserving distributed systems, by transforming an existing scalable distributed algorithm A into a new locality-preserving algorithm ALP, which guarantees for any two clients u and v interacting via ALP that their interactions exhibit worst-case response latencies proportional to the network latency between u and v. Crux builds on compact-routing theory, but generalizes these techniques beyond routing applications. Crux provides weak and strong consistency flavors, and shows latency improvements for localized interactions in both cases, specifically up to several orders of magnitude for weakly-consistent Crux (from roughly 900ms to 1ms). We deployed on PlanetLab locality-preserving versions of a Memcached distributed cache, a Bamboo distributed hash table, and a Redis publish/subscribe. Our results indicate that Crux is effective and applicable to a variety of existing distributed algorithms. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Basescu, Cristina ; Nowlan, Michael F. ; Nikitin, Kirill ; Faleiro, Jose M. ; Ford, Bryan ; "
http://arxiv.org/abs/1405.0713,Further result on acyclic chromatic index of planar graphs,"  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that every cycle is colored with at least three colors. The acyclic chromatic index $\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) + 2$ for any simple graph $G$ with maximum degree $\Delta(G)$. In this paper, we prove that every planar graph $G$ admits an acyclic edge coloring with $\Delta(G) + 6$ colors. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Wang, Tao ; Zhang, Yaqiong ; "
http://arxiv.org/abs/1405.0718,Generalized Signal Alignment: On the Achievable DoF for Multi-User MIMO   Two-Way Relay Channels,"  This paper studies the achievable degrees of freedom for multi-user MIMO two-way relay channels, where there are $K$ source nodes, each equipped with $M$ antennas, one relay node, equipped with $N$ antennas, and each source node exchanges independent messages with an arbitrary set of other source nodes via the relay. By allowing an arbitrary information exchange pattern, the considered channel model is a unified one. It includes several existing channel models as special cases: $K$-user MIMO Y channel, multi-pair MIMO two-way relay channel, generalized MIMO two-way X relay channel, and $L$-cluster MIMO multiway relay channel. Previous studies mainly considered the achievability of the DoF cut-set bound $2N$ at the antenna configuration $N < 2M$ by applying signal alignment. This work aims to investigate the achievability of the DoF cut-set bound $KM$ for the case $N\geq 2M$. To this end, we first derive tighter DoF upper bounds for three special cases of the considered channel model. Then, we propose a new transmission framework, generalized signal alignment, to approach these bounds. The notion of GSA is to form network-coded symbols by aligning every pair of signals to be exchanged in a compressed subspace at the relay. A necessary and sufficient condition to construct the relay compression matrix is given. We show that using GSA, the new DoF upper bound is achievable when i) $\frac{N}{M} \in \big(0, 2+\frac{4}{K(K-1)}\big] \cup \big[K-2, +\infty\big)$ for the $K$-user MIMO Y channel; ii) $\frac{N}{M} \in \big(0, 2+\frac{4}{K}\big] \cup \big[K-2, +\infty\big)$ for the multi-pair MIMO two-way relay channel; iii) $\frac{N}{M} \in \big(0, 2+\frac{8}{K^2}\big] \cup \big[K-2, +\infty\big)$ for the generalized MIMO two-way X relay channel. We also provide the antenna configuration regions for the general multi-user MIMO two-way relay channel to achieve the total DoF $KM$. ",Computer Science - Information Theory ; ,"Liu, Kangqi ; Tao, Meixia ; "
http://arxiv.org/abs/1405.0854,Unguarded Recursion on Coinductive Resumptions,"  We study a model of side-effecting processes obtained by starting from a monad modelling base effects and adjoining free operations using a cofree coalgebra construction; one thus arrives at what one may think of as types of non-wellfounded side-effecting trees, generalizing the infinite resumption monad. Correspondingly, the arising monad transformer has been termed the coinductive generalized resumption transformer. Monads of this kind have received some attention in the recent literature; in particular, it has been shown that they admit guarded iteration. Here, we show that they also admit unguarded iteration, i.e. form complete Elgot monads, provided that the underlying base effect supports unguarded iteration. Moreover, we provide a universal characterization of the coinductive resumption monad transformer in terms of coproducts of complete Elgot monads. ",Computer Science - Logic in Computer Science ; F.3.2 ; F.3.3 ; D.3.3 ; ,"Goncharov, Sergey ; Schröder, Lutz ; Rauch, Christoph ; Jakob, Julian ; "
http://arxiv.org/abs/1405.0982,Some undecidability results for asynchronous transducers and the   Brin-Thompson group 2V,"  Using a result of Kari and Ollinger, we prove that the torsion problem for elements of the Brin-Thompson group 2V is undecidable. As a result, we show that there does not exist an algorithm to determine whether an element of the rational group R of Grigorchuk, Nekrashevich, and Sushchanskii has finite order. A modification of the construction gives other undecidability results about the dynamics of the action of elements of 2V on Cantor Space. Arzhantseva, Lafont, and Minasyanin prove in 2012 that there exists a finitely presented group with solvable word problem and unsolvable torsion problem. To our knowledge, 2V furnishes the first concrete example of such a group, and gives an example of a direct undecidability result in the extended family of R. Thompson type groups. ","Mathematics - Group Theory ; Computer Science - Formal Languages and Automata Theory ; Mathematics - Dynamical Systems ; Mathematics - Logic ; 20F10 (Primary) 68Q45, 37B99, 03B25 (Secondary) ; ","Belk, James ; Bleak, Collin ; "
http://arxiv.org/abs/1405.1481,Graphical potential games,"  We study the class of potential games that are also graphical games with respect to a given graph $G$ of connections between the players. We show that, up to strategic equivalence, this class of games can be identified with the set of Markov random fields on $G$.   From this characterization, and from the Hammersley-Clifford theorem, it follows that the potentials of such games can be decomposed to local potentials. We use this decomposition to strongly bound the number of strategy changes of a single player along a better response path. This result extends to generalized graphical potential games, which are played on infinite graphs. ",Mathematics - Probability ; Computer Science - Computer Science and Game Theory ; Economics - Theoretical Economics ; ,"Babichenko, Yakov ; Tamuz, Omer ; "
http://arxiv.org/abs/1405.1906,Leader-following Consensus of Multi-agent Systems over Finite Fields,"  The leader-following consensus problem of multi-agent systems over finite fields ${\mathbb F}_p$ is considered in this paper. Dynamics of each agent is governed by a linear equation over ${\mathbb F}_p$, where a distributed control protocol is utilized by the followers.Sufficient and/or necessary conditions on system matrices and graph weights in ${\mathbb F}_p$ are provided for the followers to track the leader. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Xu, Xiangru ; Hong, Yiguang ; "
http://arxiv.org/abs/1405.2690,Policy Gradients for CVaR-Constrained MDPs,"  We study a risk-constrained version of the stochastic shortest path (SSP) problem, where the risk measure considered is Conditional Value-at-Risk (CVaR). We propose two algorithms that obtain a locally risk-optimal policy by employing four tools: stochastic approximation, mini batches, policy gradients and importance sampling. Both the algorithms incorporate a CVaR estimation procedure, along the lines of Bardou et al. [2009], which in turn is based on Rockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio principle for estimating the gradient of the sum of one cost function (objective of the SSP) and the gradient of the CVaR of the sum of another cost function (in the constraint of SSP). The algorithms differ in the manner in which they approximate the CVaR estimates/necessary gradients - the first algorithm uses stochastic approximation, while the second employ mini-batches in the spirit of Monte Carlo methods. We establish asymptotic convergence of both the algorithms. Further, since estimating CVaR is related to rare-event simulation, we incorporate an importance sampling based variance reduction scheme into our proposed algorithms. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Mathematics - Optimization and Control ; ,"A., Prashanth L. ; "
http://arxiv.org/abs/1405.4472,AND-compression of NP-complete problems: Streamlined proof and minor   observations,"  Drucker (2012) proved the following result: Unless the unlikely complexity-theoretic collapse coNP is in NP/poly occurs, there is no AND-compression for SAT. The result has implications for the compressibility and kernelizability of a whole range of NP-complete parameterized problems. We present a streamlined proof of Drucker's theorem.   An AND-compression is a deterministic polynomial-time algorithm that maps a set of SAT-instances $x_1,\dots,x_t$ to a single SAT-instance $y$ of size poly(max $|x_i|$) such that $y$ is satisfiable if and only if all $x_i$ are satisfiable. The ""AND"" in the name stems from the fact that the predicate ""$y$ is satisfiable"" can be written as the AND of all predicates ""$x_i$ is satisfiable"". Drucker's result complements the result by Bodlaender et al. (2009) and Fortnow and Santhanam (2010), who proved the analogous statement for OR-compressions, and Drucker's proof not only subsumes that result but also extends it to randomized compression algorithms that are allowed to have a certain probability of failure.   Drucker (2012) presented two proofs: The first uses information theory and the minimax theorem from game theory, and the second is an elementary, iterative proof that is not as general. In our proof, we realize the iterative structure as a generalization of the arguments of Ko (1983) for P-selective sets, which use the fact that tournaments have dominating sets of logarithmic size. We generalize this fact to hypergraph tournaments. Our proof achieves the full generality of Drucker's theorem, avoids the minimax theorem, and restricts the use of information theory to a single, intuitive lemma about the average noise sensitivity of compressive maps. To prove this lemma, we use the same information-theoretic inequalities as Drucker. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; ,"Dell, Holger ; "
http://arxiv.org/abs/1405.4713,Signal-noise search RMT estimator with adaptive decision criterion for   estimating the number of signals based on random matrix theory,"  Estimating the number of signals embedded in noise is a fundamental problem in signal processing. As a classic estimator based on random matrix theory (RMT), the RMT estimator estimates the number of signals via sequentially testing the likelihood of an eigenvalue as arising from a signal or noise for a given over-detection probability. However, it tends to down-estimate the number of signals as weak signal eigenvalues may be immersed in the bias term among eigenvalues. In order to solve this problem, in this paper we focus on developing novel RMT-based estimators by incorporating this bias term into RMT estimator. Firstly, we derive a novel decision statistics for signal detection by incorporating the bias term into the RMT estimator, and propose a signal-test RMT estimator for signal number estimation for a given miss-detection probability. Secondly, we analyze the effect of the bias term on the detection performance of the signal-test RMT estimator and the RMT estimator. It shows that the signal-test RMT estimator has lower down-estimation probability than the RMT estimator when weak signal eigenvalues are immersed in the bias term, but has higher over-estimation probability than the RMT estimator when all signals are strong enough to be detected by the RMT estimator. Thirdly, we derive analytical formulas for the increased over-estimation probability of the signal-test RMT estimator and the increased down-estimation probability of the RMT estimator incurred by this bias term, and then propose a signal-noise-test RMT estimator which can adaptively select its decision criterion between the RMT estimator and the signal-test RMT estimator to make benefits of these two estimators while avoiding their individual drawbacks. Finally, simulation results are presented to show that the proposed signal-noise-test RMT estimator significantly outperforms the existing estimators in all cases. ",Computer Science - Information Theory ; Statistics - Methodology ; ,"Yi, Huiyue ; "
http://arxiv.org/abs/1405.6397,Efficient Evaluation of the Probability Density Function of a Wrapped   Normal Distribution,"  The wrapped normal distribution arises when a the density of a one-dimensional normal distribution is wrapped around the circle infinitely many times. At first look, evaluation of its probability density function appears tedious as an infinite series is involved. In this paper, we investigate the evaluation of two truncated series representations. As one representation performs well for small uncertainties whereas the other performs well for large uncertainties, we show that in all cases a small number of summands is sufficient to achieve high accuracy. ",Statistics - Computation ; Computer Science - Systems and Control ; Mathematics - Numerical Analysis ; ,"Kurz, Gerhard ; Gilitschenski, Igor ; Hanebeck, Uwe D. ; "
http://arxiv.org/abs/1405.7264,"A Datalog-based Computational Model for Coordination-free, Data-Parallel   Systems","  Cloud computing refers to maximizing efficiency by sharing computational and storage resources, while data-parallel systems exploit the resources available in the cloud to perform parallel transformations over large amounts of data. In the same line, considerable emphasis has been recently given to two apparently disjoint research topics: data-parallel, and eventually consistent, distributed systems. Declarative networking has been recently proposed to ease the task of programming in the cloud, by allowing the programmer to express only the desired result and leave the implementation details to the responsibility of the run-time system. In this context, we propose a study on a logic-programming-based computational model for eventually consistent, data-parallel systems, the keystone of which is provided by the recent finding that the class of programs that can be computed in an eventually consistent, coordination-free way is that of monotonic programs. This principle is called CALM and has been proven by Ameloot et al. for distributed, asynchronous settings. We advocate that CALM should be employed as a basic theoretical tool also for data-parallel systems, wherein computation usually proceeds synchronously in rounds and where communication is assumed to be reliable. It is general opinion that coordination-freedom can be seen as a major discriminant factor. In this work we make the case that the current form of CALM does not hold in general for data-parallel systems, and show how, using novel techniques, the satisfiability of the CALM principle can still be obtained although just for the subclass of programs called connected monotonic queries. We complete the study with considerations on the relationships between our model and the one employed by Ameloot et al., showing that our techniques subsume the latter when the synchronization constraints imposed on the system are loosened. ",Computer Science - Databases ; ,"Interlandi, Matteo ; Tanca, Letizia ; "
http://arxiv.org/abs/1405.7709,A Stable Marriage Requires Communication,"  The Gale-Shapley algorithm for the Stable Marriage Problem is known to take $\Theta(n^2)$ steps to find a stable marriage in the worst case, but only $\Theta(n \log n)$ steps in the average case (with $n$ women and $n$ men). In 1976, Knuth asked whether the worst-case running time can be improved in a model of computation that does not require sequential access to the whole input. A partial negative answer was given by Ng and Hirschberg, who showed that $\Theta(n^2)$ queries are required in a model that allows certain natural random-access queries to the participants' preferences. A significantly more general - albeit slightly weaker - lower bound follows from Segal's general analysis of communication complexity, namely that $\Omega(n^2)$ Boolean queries are required in order to find a stable marriage, regardless of the set of allowed Boolean queries.   Using a reduction to the communication complexity of the disjointness problem, we give a far simpler, yet significantly more powerful argument showing that $\Omega(n^2)$ Boolean queries of any type are indeed required for finding a stable - or even an approximately stable - marriage. Notably, unlike Segal's lower bound, our lower bound generalizes also to (A) randomized algorithms, (B) allowing arbitrary separate preprocessing of the women's preferences profile and of the men's preferences profile, (C) several variants of the basic problem, such as whether a given pair is married in every/some stable marriage, and (D) determining whether a proposed marriage is stable or far from stable. In order to analyze ""approximately stable"" marriages, we introduce the notion of ""distance to stability"" and provide an efficient algorithm for its computation. ",Computer Science - Computer Science and Game Theory ; Computer Science - Computational Complexity ; ,"Gonczarowski, Yannai A. ; Nisan, Noam ; Ostrovsky, Rafail ; Rosenbaum, Will ; "
http://arxiv.org/abs/1406.0263,"The ""Runs"" Theorem","  We give a new characterization of maximal repetitions (or runs) in strings based on Lyndon words. The characterization leads to a proof of what was known as the ""runs"" conjecture (Kolpakov \& Kucherov (FOCS '99)), which states that the maximum number of runs $\rho(n)$ in a string of length $n$ is less than $n$. The proof is remarkably simple, considering the numerous endeavors to tackle this problem in the last 15 years, and significantly improves our understanding of how runs can occur in strings. In addition, we obtain an upper bound of $3n$ for the maximum sum of exponents $\sigma(n)$ of runs in a string of length $n$, improving on the best known bound of $4.1n$ by Crochemore et al. (JDA 2012), as well as other improved bounds on related problems. The characterization also gives rise to a new, conceptually simple linear-time algorithm for computing all the runs in a string. A notable characteristic of our algorithm is that, unlike all existing linear-time algorithms, it does not utilize the Lempel-Ziv factorization of the string. We also establish a relationship between runs and nodes of the Lyndon tree, which gives a simple optimal solution to the 2-Period Query problem that was recently solved by Kociumaka et al. (SODA 2015). ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; ,"Bannai, Hideo ; I, Tomohiro ; Inenaga, Shunsuke ; Nakashima, Yuto ; Takeda, Masayuki ; Tsuruta, Kazuya ; "
http://arxiv.org/abs/1406.0342,A faster method for computing Gama-Nguyen-Regev's extreme pruning   coefficients,"  This paper considers Gama-Nguyen-Regev's strategy [GNR10] for optimizing pruning coefficients for lattice vector enumeration. We give a table of optimized coefficients and proposes a faster method for computing near-optimized coefficients for any parameters by interpolation.   From the first version published in 2014, we inserted new Section 3.3 to introduce our recent technique to compute approximations of enumeration cost and success probability; both are completed in O(n^2) floating point operations where n is the lattice dimension.   For readers who are interested in this topic, we keep the descriptions of our heuristic optimization method in Section 4 although they are outdated now. ",Computer Science - Cryptography and Security ; ,"Aono, Yoshinori ; "
http://arxiv.org/abs/1406.0641,Extensions of Configuration Structures,"  The present paper defines ST-structures (and an extension of these, called STC-structures). The main purpose is to provide concrete relationships between highly expressive concurrency models coming from two different schools of thought: the higher dimensional automata, a \textit{state-based} approach of Pratt and van Glabbeek; and the configuration structures and (in)pure event structures, an \textit{event-based} approach of van Glabbeek and Plotkin. In this respect we make comparative studies of the expressive power of ST-structures relative to the above models. Moreover, standard notions from other concurrency models are defined for ST(C)-structures, like steps and paths, bisimilarities, and action refinement, and related results are given. These investigations of ST(C)-structures are intended to provide a better understanding of the \textit{state-event duality} described by Pratt, and also of the (a)cyclic structures of higher dimensional automata. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Logic in Computer Science ; ","Prisacariu, Cristian ; "
http://arxiv.org/abs/1406.1758,Scaling limits and influence of the seed graph in preferential   attachment trees,"  We are interested in the asymptotics of random trees built by linear preferential attachment, also known in the literature as Barab\'asi-Albert trees or plane-oriented recursive trees. We first prove a conjecture of Bubeck, Mossel \& R\'acz concerning the influence of the seed graph on the asymptotic behavior of such trees. Separately we study the geometric structure of nodes of large degrees in a plane version of Barab\'asi-Albert trees via their associated looptrees. As the number of nodes grows, we show that these looptrees, appropriately rescaled, converge in the Gromov-Hausdorff sense towards a random compact metric space which we call the Brownian looptree. The latter is constructed as a quotient space of Aldous' Brownian Continuum Random Tree and is shown to have almost sure Hausdorff dimension $2$. ",Mathematics - Probability ; Computer Science - Discrete Mathematics ; Mathematics - Statistics Theory ; ,"Curien, Nicolas ; Duquesne, Thomas ; Kortchemski, Igor ; Manolescu, Ioan ; "
http://arxiv.org/abs/1406.1949,Distinct Distances: Open Problems and Current Bounds,  We survey the variants of Erd\H{o}s' distinct distances problem and the current best bounds for each of those. ,Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Sheffer, Adam ; "
http://arxiv.org/abs/1406.2534,Load Hiding of Household's Power Demand,"  With the development and introduction of smart metering, the energy information for costumers will change from infrequent manual meter readings to fine-grained energy consumption data. On the one hand these fine-grained measurements will lead to an improvement in costumers' energy habits, but on the other hand the fined-grained data produces information about a household and also households' inhabitants, which are the basis for many future privacy issues. To ensure household privacy and smart meter information owned by the household inhabitants, load hiding techniques were introduced to obfuscate the load demand visible at the household energy meter. In this work, a state-of-the-art battery-based load hiding (BLH) technique, which uses a controllable battery to disguise the power consumption and a novel load hiding technique called load-based load hiding (LLH) are presented. An LLH system uses an controllable household appliance to obfuscate the household's power demand. We evaluate and compare both load hiding techniques on real household data and show that both techniques can strengthen household privacy but only LLH can increase appliance level privacy. ",Computer Science - Other Computer Science ; ,"Egarter, Dominik ; Prokop, Christoph ; Elmenreich, Wilfried ; "
http://arxiv.org/abs/1406.2587,Structural Sparsity of Complex Networks: Bounded Expansion in Random   Models and Real-World Graphs,"  This research establishes that many real-world networks exhibit bounded expansion, a strong notion of structural sparsity, and demonstrates that it can be leveraged to design efficient algorithms for network analysis. We analyze several common network models regarding their structural sparsity. We show that, with high probability, (1) graphs sampled with a prescribed s parse degree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block models with small probabilities; result in graphs of bounded expansion.   In contrast, we show that the Kleinberg and the Barabasi-Albert model have unbounded expansion. We support our findings with empirical measurements on a corpus of real-world networks. ",Computer Science - Social and Information Networks ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Physics - Physics and Society ; ,"Demaine, Erik D. ; Reidl, Felix ; Rossmanith, Peter ; Villaamil, Fernando Sanchez ; Sikdar, Somnath ; Sullivan, Blair D. ; "
http://arxiv.org/abs/1406.3065,Lower Bounds for Tropical Circuits and Dynamic Programs,"  Tropical circuits are circuits with Min and Plus, or Max and Plus operations as gates. Their importance stems from their intimate relation to dynamic programming algorithms. The power of tropical circuits lies somewhere between that of monotone boolean circuits and monotone arithmetic circuits. In this paper we present some lower bounds arguments for tropical circuits, and hence, for dynamic programs. ",Computer Science - Computational Complexity ; ,"Jukna, Stasys ; "
http://arxiv.org/abs/1406.4060,Consistency of Quine's New Foundations using nominal techniques,  We build a model in nominal sets for TST+; typed set theory with typical ambiguity. It is known that this is equivalent to the consistency of Quine's New Foundations.   Nominal techniques are used to constrain the size of powersets and thus model typical ambiguity. ,"Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03E35 (Primary), 03B70 (Secondary) ; F.4.1 ; ","Gabbay, Murdoch J. ; "
http://arxiv.org/abs/1406.4426,The number system hidden inside the Boolean satisfiability problem,"  This paper gives a novel approach to analyze SAT problem more deeply. First, I define new elements of Boolean formula such as dominant variable, decision chain, and chain coupler. Through the analysis of the SAT problem using the elements, I prove that we can construct a k-SAT (k>2) instance where the coefficients of cutting planes take exponentially large values in the input size. This exponential property is caused by the number system formed from the calculation of coefficients. In addition, I show that 2-SAT does not form the number system and Horn-SAT partially forms the number system according to the feasible value of the dominant variable. Whether or not the coefficients of cutting planes in cutting plane proof are polynomially bounded was open problem. Many researchers believed that cutting plane proofs with large coefficients are highly non-intuitive20. However, we can construct a k-SAT (k>2) instance in which cutting planes take exponentially large coefficients by the number system. In addition, this exponential property is so strong that it gives definite answers for several questions: why Horn-SAT has the intermediate property between 2-SAT and 3-SAT; why random-SAT is so easy; and why k-SAT (k>2) cannot be solved with the linear programming technique. As we know, 2-SAT is NL-complete, Horn-SAT is P-complete, and k-SAT (k>2) is NP-complete. In terms of computational complexity, this paper gives a clear mathematical property by which SAT problems in three different classes are distinguished. ",Computer Science - Computational Complexity ; Mathematics - Logic ; ,"Cho, Keum-Bae ; "
http://arxiv.org/abs/1406.5688,"Information, Meaning, and Intellectual Organization in Networks of   Inter-Human Communication","  The Shannon-Weaver model of linear information transmission is extended with two loops potentially generating redundancies: (i) meaning is provided locally to the information from the perspective of hindsight, and (ii) meanings can be codified differently and then refer to other horizons of meaning. Thus, three layers are distinguished: variations in the communications, historical organization at each moment of time, and evolutionary self-organization of the codes of communication over time. Furthermore, the codes of communication can functionally be different and then the system is both horizontally and vertically differentiated. All these subdynamics operate in parallel and necessarily generate uncertainty. However, meaningful information can be considered as the specific selection of a signal from the noise; the codes of communication are social constructs that can generate redundancy by giving different meanings to the same information. Reflexively, one can translate among codes in more elaborate discourses. The second (instantiating) layer can be operationalized in terms of semantic maps using the vector space model; the third in terms of mutual redundancy among the latent dimensions of the vector space. Using Blaise Cronin's {\oe}uvre, the different operations of the three layers are demonstrated empirically. ",Computer Science - Digital Libraries ; Computer Science - Computers and Society ; ,"Leydesdorff, Loet ; "
http://arxiv.org/abs/1406.5943,The Moser-Tardos Framework with Partial Resampling,"  The resampling algorithm of Moser \& Tardos is a powerful approach to develop constructive versions of the Lov\'{a}sz Local Lemma (LLL). We generalize this to partial resampling: when a bad event holds, we resample an appropriately-random subset of the variables that define this event, rather than the entire set as in Moser & Tardos. This is particularly useful when the bad events are determined by sums of random variables. This leads to several improved algorithmic applications in scheduling, graph transversals, packet routing etc. For instance, we settle a conjecture of Szab\'{o} & Tardos (2006) on graph transversals asymptotically, and obtain improved approximation ratios for a packet routing problem of Leighton, Maggs, & Rao (1994). ",Mathematics - Combinatorics ; Computer Science - Data Structures and Algorithms ; ,"Harris, David G. ; Srinivasan, Aravind ; "
http://arxiv.org/abs/1406.6145,"Fast, Robust and Non-convex Subspace Recovery","  This work presents a fast and non-convex algorithm for robust subspace recovery. The data sets considered include inliers drawn around a low-dimensional subspace of a higher dimensional ambient space, and a possibly large portion of outliers that do not lie nearby this subspace. The proposed algorithm, which we refer to as Fast Median Subspace (FMS), is designed to robustly determine the underlying subspace of such data sets, while having lower computational complexity than existing methods. We prove convergence of the FMS iterates to a stationary point. Further, under a special model of data, FMS converges to a point which is near to the global minimum with overwhelming probability. Under this model, we show that the iteration complexity is globally bounded and locally $r$-linear. The latter theorem holds for any fixed fraction of outliers (less than 1) and any fixed positive distance between the limit point and the global minimum. Numerical experiments on synthetic and real data demonstrate its competitive speed and accuracy. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Statistics - Applications ; Statistics - Machine Learning ; ,"Lerman, Gilad ; Maunu, Tyler ; "
http://arxiv.org/abs/1406.6924,Strongly stable ideals and Hilbert polynomials,  The \texttt{StronglyStableIdeals} package for \textit{Macaulay2} provides a method to compute all saturated strongly stable ideals in a given polynomial ring with a fixed Hilbert polynomial. A description of the main method and auxiliary tools is given. ,"Computer Science - Symbolic Computation ; Computer Science - Mathematical Software ; Mathematics - Commutative Algebra ; Mathematics - Algebraic Geometry ; Mathematics - Combinatorics ; 13P10, 13P99 ; ","Alberelli, Davide ; Lella, Paolo ; "
http://arxiv.org/abs/1406.7373,How to Achieve the Capacity of Asymmetric Channels,"  We survey coding techniques that enable reliable transmission at rates that approach the capacity of an arbitrary discrete memoryless channel. In particular, we take the point of view of modern coding theory and discuss how recent advances in coding for symmetric channels help provide more efficient solutions for the asymmetric case. We consider, in more detail, three basic coding paradigms.   The first one is Gallager's scheme that consists of concatenating a linear code with a non-linear mapping so that the input distribution can be appropriately shaped. We explicitly show that both polar codes and spatially coupled codes can be employed in this scenario. Furthermore, we derive a scaling law between the gap to capacity, the cardinality of the input and output alphabets, and the required size of the mapper.   The second one is an integrated scheme in which the code is used both for source coding, in order to create codewords distributed according to the capacity-achieving input distribution, and for channel coding, in order to provide error protection. Such a technique has been recently introduced by Honda and Yamamoto in the context of polar codes, and we show how to apply it also to the design of sparse graph codes.   The third paradigm is based on an idea of B\""ocherer and Mathar, and separates the two tasks of source coding and channel coding by a chaining construction that binds together several codewords. We present conditions for the source code and the channel code, and we describe how to combine any source code with any channel code that fulfill those conditions, in order to provide capacity-achieving schemes for asymmetric channels. In particular, we show that polar codes, spatially coupled codes, and homophonic codes are suitable as basic building blocks of the proposed coding strategy. ",Computer Science - Information Theory ; ,"Mondelli, Marco ; Hassani, S. Hamed ; Urbanke, Rüdiger ; "
http://arxiv.org/abs/1407.0208,A Bayes consistent 1-NN classifier,"  We show that a simple modification of the 1-nearest neighbor classifier yields a strongly Bayes consistent learner. Prior to this work, the only strongly Bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. We will argue that a margin-regularized 1-NN enjoys considerable statistical and algorithmic advantages over the k-NN classifier. These include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging empirical results are reported. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Kontorovich, Aryeh ; Weiss, Roi ; "
http://arxiv.org/abs/1407.0756,Geometrical Localization Algorithm for 3-D Wireless Sensor Networks,"  In this paper, we propose an efficient range free localization scheme for large scale three dimensional wireless sensor networks. Our system environment consists of two type of sensors, randomly deployed static sensors and global positioning system equipped moving sensors. These moving anchors travels across the network field and broadcast their current locations on specified intervals. As soon as the sensors which are deployed in random fashion receives three beacon messages (known locations broadcasted by anchors), they computes their locations automatically by using our proposed algorithm. One of our significant contributions is, we use only three different beacon messages to localize one sensor, while in the best of our knowledge, all previously proposed methods use at least four different known locations. The ability of our method to localize by using only three known locations not only saves computation, time, energy, but also reduces the number of anchors needed to be deployed and more importantly reduces the communication overheads. Experimental results demonstrate that our proposed scheme improves the overall efficiency of localization process significantly.   Important Note: Final version of this paper is accepted and published by Journal of Wireless Personal Communication, Springer : June, 2014 The final version of publication is available at link.springer.com Link: http://link.springer.com/article/10.1007\%2Fs11277-014-1852-6 ",Computer Science - Networking and Internet Architecture ; ,"Kumar, Rajesh ; Kumar, Sushil ; Shukla, Diksha ; Raw, Ram Shringar ; "
http://arxiv.org/abs/1407.1103,Synchronization of finite-state pulse-coupled oscillators,"  We propose a novel generalized cellular automaton(GCA) model for discrete-time pulse-coupled oscillators and study the emergence of synchrony. Given a finite simple graph and an integer $n\ge 3$, each vertex is an identical oscillator of period $n$ with the following weak coupling along the edges: each oscillator inhibits its phase update if it has at least one neighboring oscillator at a particular ""blinking"" state and if its state is ahead of this blinking state. We obtain conditions on initial configurations and on network topologies for which states of all vertices eventually synchronize. We show that our GCA model synchronizes arbitrary initial configurations on paths, trees, and with random perturbation, any connected graph. In particular, our main result is the following local-global principle for tree networks: for $n\in \{3,4,5,6\}$, any $n$-periodic network on a tree synchronizes arbitrary initial configuration if and only if the maximum degree of the tree is less than the period $n$. ",Computer Science - Systems and Control ; Mathematics - Combinatorics ; Mathematics - Dynamical Systems ; Mathematics - Optimization and Control ; Nonlinear Sciences - Cellular Automata and Lattice Gases ; ,"Lyu, Hanbaek ; "
http://arxiv.org/abs/1407.2109,Planar Graphs: Random Walks and Bipartiteness Testing,"  We initiate the study of property testing in arbitrary planar graphs. We prove that bipartiteness can be tested in constant time, improving on the previous bound of $\tilde{O}(\sqrt{n})$ for graphs on $n$ vertices. The constant-time testability was only known for planar graphs with bounded degree.   Our algorithm is based on random walks. Since planar graphs have good separators, i.e., bad expansion, our analysis diverges from standard techniques that involve the fast convergence of random walks on expanders. We reduce the problem to the task of detecting an odd-parity cycle in a multigraph induced by constant-length cycles. We iteratively reduce the length of cycles while preserving the detection probability, until the multigraph collapses to a collection of easily discoverable self-loops.   Our approach extends to arbitrary minor-free graphs. We also believe that our techniques will find applications to testing other properties in arbitrary minor-free graphs. ",Computer Science - Data Structures and Algorithms ; ,"Czumaj, Artur ; Monemizadeh, Morteza ; Onak, Krzysztof ; Sohler, Christian ; "
http://arxiv.org/abs/1407.2506,Discovery of Important Crossroads in Road Network using Massive Taxi   Trajectories,"  A major problem in road network analysis is discovery of important crossroads, which can provide useful information for transport planning. However, none of existing approaches addresses the problem of identifying network-wide important crossroads in real road network. In this paper, we propose a novel data-driven based approach named CRRank to rank important crossroads. Our key innovation is that we model the trip network reflecting real travel demands with a tripartite graph, instead of solely analysis on the topology of road network. To compute the importance scores of crossroads accurately, we propose a HITS-like ranking algorithm, in which a procedure of score propagation on our tripartite graph is performed. We conduct experiments on CRRank using a real-world dataset of taxi trajectories. Experiments verify the utility of CRRank. ",Computer Science - Artificial Intelligence ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Xu, Ming ; Wu, Jianping ; Du, Yiman ; Wang, Haohan ; Qi, Geqi ; Hu, Kezhen ; Xiao, Yunpeng ; "
http://arxiv.org/abs/1407.2524,"An improved analysis of the M\""omke-Svensson algorithm for graph-TSP on   subquartic graphs","  M\""omke and Svensson presented a beautiful new approach for the traveling salesman problem on a graph metric (graph-TSP), which yields a $4/3$-approximation guarantee on subcubic graphs as well as a substantial improvement over the $3/2$-approximation guarantee of Christofides' algorithm on general graphs. The crux of their approach is to compute an upper bound on the minimum cost of a circulation in a particular network, $C(G,T)$, where $G$ is the input graph and $T$ is a carefully chosen spanning tree. The cost of this circulation is directly related to the number of edges in a tour output by their algorithm. Mucha subsequently improved the analysis of the circulation cost, proving that M\""omke and Svensson's algorithm for graph-TSP has an approximation ratio of at most $13/9$ on general graphs.   This analysis of the circulation is local, and vertices with degree four and five can contribute the most to its cost. Thus, hypothetically, there could exist a subquartic graph (a graph with degree at most four at each vertex) for which Mucha's analysis of the M\""omke-Svensson algorithm is tight. We show that this is not the case and that M\""omke and Svensson's algorithm for graph-TSP has an approximation guarantee of at most $25/18$ on subquartic graphs. To prove this, we present different methods to upper bound the minimum cost of a circulation on the network $C(G,T)$. Our approximation guarantee holds for all graphs that have an optimal solution to a standard linear programming relaxation of graph-TSP with subquartic support. ",Computer Science - Data Structures and Algorithms ; ,"Newman, Alantha ; "
http://arxiv.org/abs/1407.2988,Proving differential privacy in Hoare logic,"  Differential privacy is a rigorous, worst-case notion of privacy-preserving computation. Informally, a probabilistic program is differentially private if the participation of a single individual in the input database has a limited effect on the program's distribution on outputs. More technically, differential privacy is a quantitative 2-safety property that bounds the distance between the output distributions of a probabilistic program on adjacent inputs. Like many 2-safety properties, differential privacy lies outside the scope of traditional verification techniques. Existing approaches to enforce privacy are based on intricate, non-conventional type systems, or customized relational logics. These approaches are difficult to implement and often cumbersome to use.   We present an alternative approach that verifies differential privacy by standard, non-relational reasoning on non-probabilistic programs. Our approach transforms a probabilistic program into a non-probabilistic program which simulates two executions of the original program. We prove that if the target program is correct with respect to a Hoare specification, then the original probabilistic program is differentially private. We provide a variety of examples from the differential privacy literature to demonstrate the utility of our approach. Finally, we compare our approach with existing verification techniques for privacy. ",Computer Science - Logic in Computer Science ; Computer Science - Cryptography and Security ; ,"Barthe, Gilles ; Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Kunz, César ; Strub, Pierre-Yves ; "
http://arxiv.org/abs/1407.3556,Optimal Spectrum Management in Two-User Interference Channels,"  In this work, we address the problem of optimal spectrum management in continuous frequency domain in multiuser interference channels. The objective is to maximize the weighted sum of user capacities. Our main results are as follows: (i) For frequency-selective channels, we prove that in an optimal solution, each user uses maximum power; this result also generalizes to the cases where the objective is to maximize the weighted product (i.e., proportional fairness) of user capacities. (ii) For the special case of two users in flat channels, we solve the problem optimally. ",Computer Science - Information Theory ; ,"Hamedazimi, Navid ; Gupta, Himanshu ; "
http://arxiv.org/abs/1407.3764,Perfect sampling algorithm for Schur processes,"  We describe random generation algorithms for a large class of random combinatorial objects called Schur processes, which are sequences of random (integer) partitions subject to certain interlacing conditions. This class contains several fundamental combinatorial objects as special cases, such as plane partitions, tilings of Aztec diamonds, pyramid partitions and more generally steep domino tilings of the plane. Our algorithm, which is of polynomial complexity, is both exact (i.e. the output follows exactly the target probability law, which is either Boltzmann or uniform in our case), and entropy optimal (i.e. it reads a minimal number of random bits as an input).   The algorithm encompasses previous growth procedures for special Schur processes related to the primal and dual RSK algorithm, as well as the famous domino shuffling algorithm for domino tilings of the Aztec diamond. It can be easily adapted to deal with symmetric Schur processes and general Schur processes involving infinitely many parameters. It is more concrete and easier to implement than Borodin's algorithm, and it is entropy optimal.   At a technical level, it relies on unified bijective proofs of the different types of Cauchy and Littlewood identities for Schur functions, and on an adaptation of Fomin's growth diagram description of the RSK algorithm to that setting. Simulations performed with this algorithm suggest interesting limit shape phenomena for the corresponding tiling models, some of which are new. ","Mathematics - Probability ; Condensed Matter - Statistical Mechanics ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05A17, 05E10, 60C05, 60J10, 68U20, 82B20 ; ","Betea, Dan ; Boutillier, Cédric ; Bouttier, Jérémie ; Chapuy, Guillaume ; Corteel, Sylvie ; Vuletić, Mirjana ; "
http://arxiv.org/abs/1407.3975,A Generalized Write Channel Model for Bit-Patterned Media Recording,"  In this paper, we propose a generalized write channel model for bit-patterned media recording by considering all sources of errors causing some extra disturbances during write process, in addition to data dependent write synchronization errors. We investigate information-theoretic bounds for this new model according to various input distributions and also compare it numerically to the last proposed model. ",Computer Science - Information Theory ; ,"Naseri, Sima ; Yazdani, Somaie ; Razeghi, Behrooz ; Hodtani, Ghosheh Abed ; "
http://arxiv.org/abs/1407.4066,Minors and dimension,"  It has been known for 30 years that posets with bounded height and with cover graphs of bounded maximum degree have bounded dimension. Recently, Streib and Trotter proved that dimension is bounded for posets with bounded height and planar cover graphs, and Joret et al. proved that dimension is bounded for posets with bounded height and with cover graphs of bounded tree-width. In this paper, it is proved that posets of bounded height whose cover graphs exclude a fixed topological minor have bounded dimension. This generalizes all the aforementioned results and verifies a conjecture of Joret et al. The proof relies on the Robertson-Seymour and Grohe-Marx graph structure theorems. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 06A07, 05C35 ; ","Walczak, Bartosz ; "
http://arxiv.org/abs/1407.4723,Toward Selectivity Based Keyword Extraction for Croatian News,"  Preliminary report on network based keyword extraction for Croatian is an unsupervised method for keyword extraction from the complex network. We build our approach with a new network measure the node selectivity, motivated by the research of the graph based centrality approaches. The node selectivity is defined as the average weight distribution on the links of the single node. We extract nodes (keyword candidates) based on the selectivity value. Furthermore, we expand extracted nodes to word-tuples ranked with the highest in/out selectivity values. Selectivity based extraction does not require linguistic knowledge while it is purely derived from statistical and structural information en-compassed in the source text which is reflected into the structure of the network. Obtained sets are evaluated on a manually annotated keywords: for the set of extracted keyword candidates average F1 score is 24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates average F1 score is 25,9% and average F2 score is 24,47%. ",Computer Science - Computation and Language ; Computer Science - Information Retrieval ; Computer Science - Social and Information Networks ; ,"Beliga, Slobodan ; Meštrović, Ana ; Martinčić-Ipšić, Sanda ; "
http://arxiv.org/abs/1407.4729,Sparse Partially Linear Additive Models,"  The generalized partially linear additive model (GPLAM) is a flexible and interpretable approach to building predictive models. It combines features in an additive manner, allowing each to have either a linear or nonlinear effect on the response. However, the choice of which features to treat as linear or nonlinear is typically assumed known. Thus, to make a GPLAM a viable approach in situations in which little is known $a~priori$ about the features, one must overcome two primary model selection challenges: deciding which features to include in the model and determining which of these features to treat nonlinearly. We introduce the sparse partially linear additive model (SPLAM), which combines model fitting and $both$ of these model selection challenges into a single convex optimization problem. SPLAM provides a bridge between the lasso and sparse additive models. Through a statistical oracle inequality and thorough simulation, we demonstrate that SPLAM can outperform other methods across a broad spectrum of statistical regimes, including the high-dimensional ($p\gg N$) setting. We develop efficient algorithms that are applied to real data sets with half a million samples and over 45,000 features with excellent predictive performance. ",Statistics - Methodology ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Lou, Yin ; Bien, Jacob ; Caruana, Rich ; Gehrke, Johannes ; "
http://arxiv.org/abs/1407.4908,Integrating R and Hadoop for Big Data Analysis,"  Analyzing and working with big data could be very diffi cult using classical means like relational database management systems or desktop software packages for statistics and visualization. Instead, big data requires large clusters with hundreds or even thousands of computing nodes. Offi cial statistics is increasingly considering big data for deriving new statistics because big data sources could produce more relevant and timely statistics than traditional sources. One of the software tools successfully and wide spread used for storage and processing of big data sets on clusters of commodity hardware is Hadoop. Hadoop framework contains libraries, a distributed fi le-system (HDFS), a resource-management platform and implements a version of the MapReduce programming model for large scale data processing. In this paper we investigate the possibilities of integrating Hadoop with R which is a popular software used for statistical computing and data visualization. We present three ways of integrating them: R with Streaming, Rhipe and RHadoop and we emphasize the advantages and disadvantages of each solution. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Oancea, Bogdan ; Dragoescu, Raluca Mariana ; "
http://arxiv.org/abs/1407.5117,Implementing Transitive Credit with JSON-LD,"  Science and engineering research increasingly relies on activities that facilitate research but are not currently rewarded or recognized, such as: data sharing; developing common data resources, software and methodologies; and annotating data and publications. To promote and advance these activities, we must develop mechanisms for assigning credit, facilitate the appropriate attribution of research outcomes, devise incentives for activities that facilitate research, and allocate funds to maximize return on investment. In this article, we focus on addressing the issue of assigning credit for both direct and indirect contributions, specifically by using JSON-LD to implement a prototype transitive credit system. ",Computer Science - Computers and Society ; Computer Science - Digital Libraries ; ,"Katz, Daniel S. ; Smith, Arfon M. ; "
http://arxiv.org/abs/1407.5218,"Abstractions, Algorithms and Data Structures for Structural   Bioinformatics in PyCogent","  To facilitate flexible and efficient structural bioinformatics analyses, new functionality for three-dimensional structure processing and analysis has been introduced into PyCogent -- a popular feature-rich framework for sequence-based bioinformatics, but one which has lacked equally powerful tools for handling stuctural/coordinate-based data. Extensible Python modules have been developed, which provide object-oriented abstractions (based on a hierarchical representation of macromolecules), efficient data structures (e.g. kD-trees), fast implementations of common algorithms (e.g. surface-area calculations), read/write support for Protein Data Bank-related file formats and wrappers for external command-line applications (e.g. Stride). Integration of this code into PyCogent is symbiotic, allowing sequence-based work to benefit from structure-derived data and, reciprocally, enabling structural studies to leverage PyCogent's versatile tools for phylogenetic and evolutionary analyses. ",Quantitative Biology - Biomolecules ; Computer Science - Data Structures and Algorithms ; Computer Science - Software Engineering ; ,"Cieslik, Marcin ; Derewenda, Zygmunt ; Mura, Cameron ; "
http://arxiv.org/abs/1407.5374,Acyclic Edge Coloring through the Lov\'asz Local Lemma,"  We give a probabilistic analysis of a Moser-type algorithm for the Lov\'{a}sz Local Lemma (LLL), adjusted to search for acyclic edge colorings of a graph. We thus improve the best known upper bound to acyclic chromatic index, also obtained by analyzing a similar algorithm, but through the entropic method (basically counting argument). Specifically we show that a graph with maximum degree $\Delta$ has an acyclic proper edge coloring with at most $\lceil 3.74(\Delta-1)\rceil+1 $ colors, whereas, previously, the best bound was $4(\Delta-1)$. The main contribution of this work is that it comprises a probabilistic analysis of a Moser-type algorithm applied to events pertaining to dependent variables. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; Mathematics - Probability ; ,"Giotis, Ioannis ; Kirousis, Lefteris ; Psaromiligkos, Kostas I. ; Thilikos, Dimitrios M. ; "
http://arxiv.org/abs/1407.5536,Multichannel Compressive Sensing MRI Using Noiselet Encoding,"  The incoherence between measurement and sparsifying transform matrices and the restricted isometry property (RIP) of measurement matrix are two of the key factors in determining the performance of compressive sensing (CS). In CS-MRI, the randomly under-sampled Fourier matrix is used as the measurement matrix and the wavelet transform is usually used as sparsifying transform matrix. However, the incoherence between the randomly under-sampled Fourier matrix and the wavelet matrix is not optimal, which can deteriorate the performance of CS-MRI. Using the mathematical result that noiselets are maximally incoherent with wavelets, this paper introduces the noiselet unitary bases as the measurement matrix to improve the incoherence and RIP in CS-MRI, and presents a method to design the pulse sequence for the noiselet encoding. This novel encoding scheme is combined with the multichannel compressive sensing (MCS) framework to take the advantage of multichannel data acquisition used in MRI scanners. An empirical RIP analysis is presented to compare the multichannel noiselet and multichannel Fourier measurement matrices in MCS. Simulations are presented in the MCS framework to compare the performance of noiselet encoding reconstructions and Fourier encoding reconstructions at different acceleration factors. The comparisons indicate that multichannel noiselet measurement matrix has better RIP than that of its Fourier counterpart, and that noiselet encoded MCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution and can achieve higher acceleration factors. To demonstrate the feasibility of the proposed noiselet encoding scheme, two pulse sequences with tailored spatially selective RF excitation pulses was designed and implemented on a 3T scanner to acquire the data in the noiselet domain from a phantom and a human brain. ",Physics - Medical Physics ; Computer Science - Computer Vision and Pattern Recognition ; ,"Pawar, Kamlesh ; Egan, Gary F. ; Zhang, Jingxin ; "
http://arxiv.org/abs/1407.5965,Optimization Techniques on Riemannian Manifolds,"  The techniques and analysis presented in this paper provide new methods to solve optimization problems posed on Riemannian manifolds. A new point of view is offered for the solution of constrained optimization problems. Some classical optimization techniques on Euclidean space are generalized to Riemannian manifolds. Several algorithms are presented and their convergence properties are analyzed employing the Riemannian structure of the manifold. Specifically, two apparently new algorithms, which can be thought of as Newton's method and the conjugate gradient method on Riemannian manifolds, are presented and shown to possess, respectively, quadratic and superlinear convergence. Examples of each method on certain Riemannian manifolds are given with the results of numerical experiments. Rayleigh's quotient defined on the sphere is one example. It is shown that Newton's method applied to this function converges cubically, and that the Rayleigh quotient iteration is an efficient approximation of Newton's method. The Riemannian version of the conjugate gradient method applied to this function gives a new algorithm for finding the eigenvectors corresponding to the extreme eigenvalues of a symmetric matrix. Another example arises from extremizing the function $\mathop{\rm tr} {\Theta}^{\scriptscriptstyle\rm T}Q{\Theta}N$ on the special orthogonal group. In a similar example, it is shown that Newton's method applied to the sum of the squares of the off-diagonal entries of a symmetric matrix converges cubically. ",Mathematics - Optimization and Control ; Computer Science - Computational Geometry ; Computer Science - Numerical Analysis ; Mathematics - Differential Geometry ; Mathematics - Dynamical Systems ; ,"Smith, Steven Thomas ; "
http://arxiv.org/abs/1407.6116,A Genetic Algorithm for Software Design Migration from Structured to   Object Oriented Paradigm,"  The potential benefit of migrating software design from Structured to Object Oriented Paradigm is manifolded including modularity, manageability and extendability. This design migration should be automated as it will reduce the time required in manual process. Our previous work has addressed this issue in terms of optimal graph clustering problem formulated by a quadratic Integer Program (IP). However, it has been realized that solution to the IP is computationally hard and thus heuristic based methods are required to get a near optimal solution. This paper presents a Genetic Algorithm (GA) for optimal clustering with an objective of maximizing intra-cluster edges whereas minimizing the inter-cluster ones. The proposed algorithm relies on fitness based parent selection and cross-overing cluster elements to reach an optimal solution step by step. The scheme was implemented and tested against a set of real and synthetic data. The experimental results show that GA outperforms our previous works based on Greedy and Monte Carlo approaches by 40% and 49.5%. ",Computer Science - Software Engineering ; Computer Science - Neural and Evolutionary Computing ; ,"Selim, Md. ; Siddik, Saeed ; Gias, Alim Ul ; Abdullah-Al-Wadud, M. ; Khaled, Shah Mostafa ; "
http://arxiv.org/abs/1407.6169,Multiplicative Complexity of Vector Valued Boolean Functions,"  We consider the multiplicative complexity of Boolean functions with multiple bits of output, studying how large a multiplicative complexity is necessary and sufficient to provide a desired nonlinearity. For so-called $\Sigma\Pi\Sigma$ circuits, we show that there is a tight connection between error correcting codes and circuits computing functions with high nonlinearity. Combining this with known coding theory results, we show that functions with $n$ inputs and $n$ outputs with the highest possible nonlinearity must have at least $2.32n$ AND gates. We further show that one cannot prove stronger lower bounds by only appealing to the nonlinearity of a function; we show a bilinear circuit computing a function with almost optimal nonlinearity with the number of AND gates being exactly the length of such a shortest code.   Additionally we provide a function which, for general circuits, has multiplicative complexity at least $2n-3$.   Finally we study the multiplicative complexity of ""almost all"" functions. We show that every function with $n$ bits of input and $m$ bits of output can be computed using at most $2.5(1+o(1))\sqrt{m2^n}$ AND gates. ",Computer Science - Computational Complexity ; ,"Find, Magnus Gausdal ; Boyar, Joan ; "
http://arxiv.org/abs/1407.6845,Higher-Order Approximate Relational Refinement Types for Mechanism   Design and Differential Privacy,"  Mechanism design is the study of algorithm design in which the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. Unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property---incentive properties are only useful if the strategic agents also believe this fact.   Verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals.   We introduce a relational refinement type system, called $\mathsf{HOARe}^2$, for verifying mechanism design and differential privacy. We show that $\mathsf{HOARe}^2$ is sound w.r.t. a denotational semantics, and correctly models $(\epsilon,\delta)$-differential privacy; moreover, we show that it subsumes DFuzz, an existing linear dependent type system for differential privacy. Finally, we develop an SMT-based implementation of $\mathsf{HOARe}^2$ and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy. ",Computer Science - Programming Languages ; Computer Science - Computer Science and Game Theory ; ,"Barthe, Gilles ; Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Roth, Aaron ; Strub, Pierre-Yves ; "
http://arxiv.org/abs/1407.7274,Isomorphism within Naive Type Theory,"  We provide a treatment of isomorphism within a set-theoretic formulation of dependent type theory. Type expressions are assigned their natural set-theoretic compositional meaning. Types are divided into small and large types --- sets and proper classes respectively. Each proper class, such as ""group"" or ""topological space"", has an associated notion of isomorphism in correspondence with standard definitions. Isomorphism is handled by definging a groupoid structure on the space of all definable values. The values are simultaneously objects (oids) and morphism --- they are ""morphoids"". Soundness can then be proved for simple and natural inference rules deriving isomorphisms and for the substitution of isomorphics. ",Computer Science - Logic in Computer Science ; ,"McAllester, David ; "
http://arxiv.org/abs/1407.7459,A note on multipivot Quicksort,"  We analyse a generalisation of the Quicksort algorithm, where k uniformly at random chosen pivots are used for partitioning an array of n distinct keys. Specifically, the expected cost of this scheme is obtained, under the assumption of linearity of the cost needed for the partition process. The integration constants of the expected cost are computed using Vandermonde matrices. ","Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; 68P10, 68W20 ; ","Iliopoulos, Vasileios ; "
http://arxiv.org/abs/1408.0135,"New data, new possibilities: Exploring the insides of Altmetric.com","  This paper analyzes Altmetric.com, one of the most important altmetric data providers currently used. We have analyzed a set of publications with DOI number indexed in the Web of Science during the period 2011-2013 and collected their data with the Altmetric API. 19% of the original set of papers was retrieved from Altmetric.com including some altmetric data. We identified 16 different social media sources from which Altmetric.com retrieves data. However five of them cover 95.5% of the total set. Twitter (87.1%) and Mendeley (64.8%) have the highest coverage. We conclude that Altmetric.com is a transparent, rich and accurate tool for altmetric data. Nevertheless, there are still potential limitations on its exhaustiveness as well as on the selection of social media sources that need further research. ",Computer Science - Digital Libraries ; ,"Robinson-García, Nicolás ; Torres-Salinas, Daniel ; Zahedi, Zohreh ; Costas, Rodrigo ; "
http://arxiv.org/abs/1408.0652,Precision of Pulse-Coupled Oscillator Synchronization on FPGA-Based   Radios,"  The precision of synchronization algorithms based on the theory of pulse-coupled oscillators is evaluated on FPGA-based radios for the first time. Measurements show that such algorithms can reach precision in the low microsecond range when being implemented in the physical layer. Furthermore, we propose an algorithm extension accounting for phase rate deviations of the hardware and show that an improved precision below one microsecond is possible with this extension in the given setup. The resulting algorithm can thus be applied in ad hoc wireless systems for fully distributed synchronization of transmission slots or sleep cycles, in particular, if centralized synchronization is impossible. ",Computer Science - Other Computer Science ; ,"Brandner, Günther ; Klinglmayr, Johannes ; Schilcher, Udo ; Egarter, Dominik ; Bettstetter, Christian ; "
http://arxiv.org/abs/1408.0807,Polynomial size linear programs for problems in P,"  A perfect matching in an undirected graph $G=(V,E)$ is a set of vertex disjoint edges from $E$ that include all vertices in $V$. The perfect matching problem is to decide if $G$ has such a matching. Recently Rothvo{\ss} proved the striking result that the Edmonds' matching polytope has exponential extension complexity. Here for each $n=|V|$ we describe a perfect matching polytope that is different from Edmonds' polytope and define a weaker notion of extended formulation. We show that the new polytope has a weak extended formulation (WEF) $Q$ of polynomial size. For each graph $G$ with $n$ vertices we can readily construct an objective function so that solving the resulting linear program over $Q$ decides whether or not $G$ has a perfect matching. The construction is uniform in the sense that, for each $n$, a single polytope is defined for the class of all graphs with $n$ nodes. The method extends to solve poly time optimization problems, such as the weighted matching problem. In this case a logarithmic (in the weight of the optimum solution) number of optimizations are made over the constructed WEF.   The method described in the paper involves construction of a compiler that converts an algorithm given in a prescribed pseudocode into a polytope. It can therefore be used to construct a polytope for any decision problem in {\bf P} which can be solved by a given algorithm. Compared with earlier results of Dobkin-Lipton-Reiss and Valiant our method allows the construction of explicit linear programs directly from algorithms written for a standard register model, without intermediate transformations. We apply our results to obtain polynomial upper bounds on the non-negative rank of certain slack matrices related to membership testing of languages in {\bf P/Poly}. ",Computer Science - Discrete Mathematics ; ,"Avis, David ; Bremner, David ; Tiwary, Hans Raj ; Watanabe, Osamu ; "
http://arxiv.org/abs/1408.0848,Multilayer bootstrap networks,"  Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. Each layer of the network is a nonparametric density estimator. It consists of a group of k-centroids clusterings. Each clustering randomly selects data points with randomly selected features as its centroids, and learns a one-hot encoder by one-nearest-neighbor optimization. Geometrically, the nonparametric density estimator at each layer projects the input data space to a uniformly-distributed discrete feature space, where the similarity of two data points in the discrete feature space is measured by the number of the nearest centroids they share in common. The multilayer network gradually reduces the nonlinear variations of data from bottom up by building a vast number of hierarchical trees implicitly on the original data space. Theoretically, the estimation error caused by the nonparametric density estimator is proportional to the correlation between the clusterings, both of which are reduced by the randomization steps. ",Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; Statistics - Machine Learning ; ,"Zhang, Xiao-Lei ; "
http://arxiv.org/abs/1408.0948,A special role of Boolean quadratic polytopes among other combinatorial   polytopes,"  We consider several families of combinatorial polytopes associated with the following NP-complete problems: maximum cut, Boolean quadratic programming, quadratic linear ordering, quadratic assignment, set partition, set packing, stable set, 3-assignment. For comparing two families of polytopes we use the following method. We say that a family $P$ is affinely reduced to a family $Q$ if for every polytope $p\in P$ there exists $q\in Q$ such that $p$ is affinely equivalent to $q$ or to a face of $q$, where $\dim q = O((\dim p)^k)$ for some constant $k$. Under this comparison the above-mentioned families are splitted into two equivalence classes. We show also that these two classes are simpler (in the above sence) than the families of poytopes of the following problems: set covering, traveling salesman, 0-1 knapsack problem, 3-satisfiability, cubic subgraph, partial ordering. In particular, Boolean quadratic polytopes appear as faces of polytopes in every of the mentioned families. ",Computer Science - Computational Complexity ; Mathematics - Combinatorics ; ,"Maksimenko, Aleksandr ; "
http://arxiv.org/abs/1408.1025,Stable Throughput Region of Cognitive-Relay Networks with Imperfect   Sensing and Finite Relaying Buffer,"  In this letter, we obtain the stable throughput region for a cognitive relaying scheme with a finite relaying buffer and imperfect sensing. The analysis investigates the effect of the secondary user's finite relaying capabilities under different scenarios of primary, secondary and relaying links outages. Furthermore, we demonstrate the effect of miss detection and false alarm probabilities on the achievable throughput for the primary and secondary users. ",Computer Science - Networking and Internet Architecture ; ,"Alaa, Ahmed M. ; "
http://arxiv.org/abs/1408.1118,Spoke-Darts for High-Dimensional Blue-Noise Sampling,"  Blue noise sampling has proved useful for many graphics applications, but remains underexplored in high-dimensional spaces due to the difficulty of generating distributions and proving properties about them. We present a blue noise sampling method with good quality and performance across different dimensions. The method, spoke-dart sampling, shoots rays from prior samples and selects samples from these rays. It combines the advantages of two major high-dimensional sampling methods: the locality of advancing front with the dimensionality-reduction of hyperplanes, specifically line sampling. We prove that the output sampling is saturated with high probability, with bounds on distances between pairs of samples and between any domain point and its nearest sample. We demonstrate spoke-dart applications for approximate Delaunay graph construction, global optimization, and robotic motion planning. Both the blue-noise quality of the output distribution and the adaptability of the intermediate processes of our method are useful in these applications. ",Computer Science - Graphics ; ,"Mitchell, Scott A. ; Ebeida, Mohamed S. ; Awad, Muhammad A. ; Park, Chonhyon ; Patney, Anjul ; Rushdi, Ahmad A. ; Swiler, Laura P. ; Manocha, Dinesh ; Wei, Li-Yi ; "
http://arxiv.org/abs/1408.1390,On optimal approximability results for computing the strong metric   dimension,"  The strong metric dimension of a graph was first introduced by Seb\""{o} and Tannier (Mathematics of Operations Research, 29(2), 383-393, 2004) as an alternative to the (weak) metric dimension of graphs previously introduced independently by Slater (Proc. 6th Southeastern Conference on Combinatorics, Graph Theory, and Computing, 549-559, 1975) and by Harary and Melter (Ars Combinatoria, 2, 191-195, 1976), and has since been investigated in several research papers. However, the exact worst-case computational complexity of computing the strong metric dimension has remained open beyond being NP-complete. In this communication, we show that the problem of computing the strong metric dimension of a graph of $n$ nodes admits a polynomial-time $2$-approximation, admits a $O^\ast\big(2^{\,0.287\,n}\big)$-time exact computation algorithm, admits a $O\big(1.2738^k+n\,k\big)$-time exact computation algorithm if the strong metric dimension is at most $k$, does not admit a polynomial time $(2-\varepsilon)$-approximation algorithm assuming the unique games conjecture is true, does not admit a polynomial time $(10\sqrt{5}-21-\varepsilon)$-approximation algorithm assuming P$\neq$NP, does not admit a $O^\ast\big(2^{o(n)}\big)$-time exact computation algorithm assuming the exponential time hypothesis is true, and does not admit a $O^\ast\big(n^{o(k)}\big)$-time exact computation algorithm if the strong metric dimension is at most $k$ assuming the exponential time hypothesis is true. ","Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; 68Q17, 68Q25, 68R10 ; G.2.2 ; F.2.2 ; ","DasGupta, Bhaskar ; Mobasheri, Nasim ; "
http://arxiv.org/abs/1408.1868,On the structure of classical realizability models of ZF,"  The technique of ""classical realizability"" is an extension of the method of ""forcing""; it permits to extend the Curry-Howard correspondence between proofs and programs, to Zermelo-Fraenkel set theory and to build new models of ZF, called ""realizability models"". The structure of these models is, in general, much more complicated than that of the particular case of ""forcing models"". We show here that the class of constructible sets of any realizability model is an elementary extension of the constructibles of the ground model (a trivial fact in the case of forcing, since these classes are identical). It follows that Shoenfield absoluteness theorem applies to realizability models. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03E40 ; F.4.1 ; ,"Krivine, Jean-Louis ; "
http://arxiv.org/abs/1408.2071,Near-Constant-Time Distributed Algorithms on a Congested Clique,"  This paper presents constant-time and near-constant-time distributed algorithms for a variety of problems in the congested clique model. We show how to compute a 3-ruling set in expected $O(\log \log \log n)$ rounds and using this, we obtain a constant-approximation to metric facility location, also in expected $O(\log \log \log n)$ rounds. In addition, assuming an input metric space of constant doubling dimension, we obtain constant-round algorithms to compute constant-factor approximations to the minimum spanning tree and the metric facility location problems. These results significantly improve on the running time of the fastest known algorithms for these problems in the congested clique setting. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Hegeman, James W. ; Pemmaraju, Sriram V. ; Sardeshmukh, Vivek B. ; "
http://arxiv.org/abs/1408.2467,Matrix Completion under Interval Uncertainty,"  Matrix completion under interval uncertainty can be cast as matrix completion with element-wise box constraints. We present an efficient alternating-direction parallel coordinate-descent method for the problem. We show that the method outperforms any other known method on a benchmark in image in-painting in terms of signal-to-noise ratio, and that it provides high-quality solutions for an instance of collaborative filtering with 100,198,805 recommendations within 5 minutes. ",Mathematics - Optimization and Control ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; ,"Marecek, Jakub ; Richtarik, Peter ; Takac, Martin ; "
http://arxiv.org/abs/1408.3030,Distributed Graph Automata and Verification of Distributed Algorithms,"  Combining ideas from distributed algorithms and alternating automata, we introduce a new class of finite graph automata that recognize precisely the languages of finite graphs definable in monadic second-order logic. By restricting transitions to be nondeterministic or deterministic, we also obtain two strictly weaker variants of our automata for which the emptiness problem is decidable. As an application, we suggest how suitable graph automata might be useful in formal verification of distributed algorithms, using Floyd-Hoare logic. ","Computer Science - Formal Languages and Automata Theory ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Logic in Computer Science ; ","Reiter, Fabian ; "
http://arxiv.org/abs/1408.3190,On the neighbour sum distinguishing index of planar graphs,"  Let $c$ be a proper edge colouring of a graph $G=(V,E)$ with integers $1,2,\ldots,k$. Then $k\geq \Delta(G)$, while by Vizing's theorem, no more than $k=\Delta(G)+1$ is necessary for constructing such $c$. On the course of investigating irregularities in graphs, it has been moreover conjectured that only slightly larger $k$, i.e., $k=\Delta(G)+2$ enables enforcing additional strong feature of $c$, namely that it attributes distinct sums of incident colours to adjacent vertices in $G$ if only this graph has no isolated edges and is not isomorphic to $C_5$. We prove the conjecture is valid for planar graphs of sufficiently large maximum degree. In fact even stronger statement holds, as the necessary number of colours stemming from the result of Vizing is proved to be sufficient for this family of graphs. Specifically, our main result states that every planar graph $G$ of maximum degree at least $28$ which contains no isolated edges admits a proper edge colouring $c:E\to\{1,2,\ldots,\Delta(G)+1\}$ such that $\sum_{e\ni u}c(e)\neq \sum_{e\ni v}c(e)$ for every edge $uv$ of $G$. ","Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05C78, 05C15 ; ","Bonamy, Marthe ; Przybyło, Jakub ; "
http://arxiv.org/abs/1408.3310,An algorithm for canonical forms of finite subsets of $\mathbb{Z}^d$ up   to affinities,"  In this paper we describe an algorithm for the computation of canonical forms of finite subsets of $\mathbb{Z}^d$, up to affinities over $\mathbb{Z}$. For fixed dimension $d$, this algorithm has worst-case asymptotic complexity $O(n \log^2 n \, s\,\mu(s))$, where $n$ is the number of points in the given subset, $s$ is an upper bound to the size of the binary representation of any of the $n$ points, and $\mu(s)$ is an upper bound to the number of operations required to multiply two $s$-bit numbers. In particular, the problem is fixed-parameter tractable with respect to the dimension $d$. This problem arises e.g. in the context of computation of invariants of finitely presented groups with abelianized group isomorphic to $\mathbb{Z}^d$. In that context one needs to decide whether two Laurent polynomials in $d$ indeterminates, considered as elements of the group ring over the abelianized group, are equivalent with respect to a change of basis. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Group Theory ; 52C07 ; ,"Paolini, Giovanni ; "
http://arxiv.org/abs/1408.3743,Parallel generator of $q$-valued pseudorandom sequences based on   arithmetic polynomials,"  A new method for parallel generation of $q$-valued pseudorandom sequence based on the presentation of systems generating logical formulae by means of arithmetic polynomials is proposed. Fragment consisting of $k$-elements of $q$-valued pseudorandom sequence may be obtained by means of single calculation of a single recursion numerical formula. It is mentioned that the method of the ""arithmetization"" of generation may be used and further developed in order to protect the encryption gears from cryptographic onset, resulting in the initiating of mass hardware failures. The achieved results may be widely applied to the realization of perspective high-performance cryptographic facilities for information protection. ","Computer Science - Cryptography and Security ; 94A55, 68W10, 03B50, 11A07, 11B50, 94A60 ; ","Finko, Oleg ; Samoylenko, Dmitriy ; Dichenko, Sergey ; Eliseev, Nikolay ; "
http://arxiv.org/abs/1408.3869,Treewidth of graphs with balanced separations,  We prove that if every subgraph of a graph $G$ has a balanced separation of order at most $a$ then $G$ has treewidth at most $15a$. This establishes a linear dependence between the treewidth and the separation number. ,Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Dvorak, Zdenek ; Norin, Sergey ; "
http://arxiv.org/abs/1408.3877,FESTUNG: A MATLAB / GNU Octave toolbox for the discontinuous Galerkin   method. Part I: Diffusion operator,"  This is the first in a series of papers on implementing a discontinuous Galerkin method as a MATLAB / GNU Octave toolbox. The main goal is the development of techniques that deliver optimized computational performance combined with a compact, user-friendly interface. Our implementation relies on fully vectorized matrix / vector operations and is carefully documented; in addition, a direct mapping between discretization terms and code routines is maintained throughout. The present work focuses on a two-dimensional time-dependent diffusion equation with space / time-varying coefficients. The spatial discretization is based on the local discontinuous Galerkin formulation and is locally mass conservative. Approximations of orders zero through four based on orthogonal polynomials have been implemented; more spaces of arbitrary type and order can be easily accommodated by the code structure. Time discretization is performed using an implicit Euler method. ","Mathematics - Numerical Analysis ; Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Numerical Analysis ; ","Frank, Florian ; Reuter, Balthasar ; Aizinger, Vadym ; Knabner, Peter ; "
http://arxiv.org/abs/1408.3976,Static Analysis for Extracting Permission Checks of a Large Scale   Framework: The Challenges And Solutions for Analyzing Android,"  A common security architecture is based on the protection of certain resources by permission checks (used e.g., in Android and Blackberry). It has some limitations, for instance, when applications are granted more permissions than they actually need, which facilitates all kinds of malicious usage (e.g., through code injection). The analysis of permission-based framework requires a precise mapping between API methods of the framework and the permissions they require. In this paper, we show that naive static analysis fails miserably when applied with off-the-shelf components on the Android framework. We then present an advanced class-hierarchy and field-sensitive set of analyses to extract this mapping. Those static analyses are capable of analyzing the Android framework. They use novel domain specific optimizations dedicated to Android. ",Computer Science - Software Engineering ; ,"Bartel, Alexandre ; Klein, Jacques ; Monperrus, Martin ; Traon, Yves Le ; "
http://arxiv.org/abs/1408.4528,Laplace Functional Ordering of Point Processes in Large-scale Wireless   Networks,"  Stochastic orders on point processes are partial orders which capture notions like being larger or more variable. Laplace functional ordering of point processes is a useful stochastic order for comparing spatial deployments of wireless networks. It is shown that the ordering of point processes is preserved under independent operations such as marking, thinning, clustering, superposition, and random translation. Laplace functional ordering can be used to establish comparisons of several performance metrics such as coverage probability, achievable rate, and resource allocation even when closed form expressions of such metrics are unavailable. Applications in several network scenarios are also provided where tradeoffs between coverage and interference as well as fairness and peakyness are studied. Monte-Carlo simulations are used to supplement our analytical results. ",Computer Science - Information Theory ; Computer Science - Networking and Internet Architecture ; ,"Lee, Junghoon ; Tepedelenlioglu, Cihan ; "
http://arxiv.org/abs/1408.6321,Crossing Minimization for 1-page and 2-page Drawings of Graphs with   Bounded Treewidth,"  We investigate crossing minimization for 1-page and 2-page book drawings. We show that computing the 1-page crossing number is fixed-parameter tractable with respect to the number of crossings, that testing 2-page planarity is fixed-parameter tractable with respect to treewidth, and that computing the 2-page crossing number is fixed-parameter tractable with respect to the sum of the number of crossings and the treewidth of the input graph. We prove these results via Courcelle's theorem on the fixed-parameter tractability of properties expressible in monadic second order logic for graphs of bounded treewidth. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Bannister, Michael J. ; Eppstein, David ; "
http://arxiv.org/abs/1408.6771,Flat Foldings of Plane Graphs with Prescribed Angles and Edge Lengths,"  When can a plane graph with prescribed edge lengths and prescribed angles (from among $\{0,180^\circ, 360^\circ$\}) be folded flat to lie in an infinitesimally thin line, without crossings? This problem generalizes the classic theory of single-vertex flat origami with prescribed mountain-valley assignment, which corresponds to the case of a cycle graph. We characterize such flat-foldable plane graphs by two obviously necessary but also sufficient conditions, proving a conjecture made in 2001: the angles at each vertex should sum to $360^\circ$, and every face of the graph must itself be flat foldable. This characterization leads to a linear-time algorithm for testing flat foldability of plane graphs with prescribed edge lengths and angles, and a polynomial-time algorithm for counting the number of distinct folded states. ",Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; F.2.2 ; ,"Abel, Zachary ; Demaine, Erik D. ; Demaine, Martin L. ; Eppstein, David ; Lubiw, Anna ; Uehara, Ryuhei ; "
http://arxiv.org/abs/1408.6923,GPGPU Computing,"  Since the first idea of using GPU to general purpose computing, things have evolved over the years and now there are several approaches to GPU programming. GPU computing practically began with the introduction of CUDA (Compute Unified Device Architecture) by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide. A new emerging standard, OpenCL (Open Computing Language) tries to unify different GPU general computing API implementations and provides a framework for writing programs executed across heterogeneous platforms consisting of both CPUs and GPUs. OpenCL provides parallel computing using task-based and data-based parallelism. In this paper we will focus on the CUDA parallel computing architecture and programming model introduced by NVIDIA. We will present the benefits of the CUDA programming model. We will also compare the two main approaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries to unify the GPGPU computing models. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Oancea, Bogdan ; Andrei, Tudorel ; Dragoescu, Raluca Mariana ; "
http://arxiv.org/abs/1409.0264,Nash Equilbria for Quadratic Voting,"  Voters making a binary decision purchase votes from a centralized clearing house, paying the square of the number of votes purchased. The net payoff to an agent with utility $u$ who purchases $v$ votes is $\Psi (S_{n+1})u-v^{2}$, where $\Psi$ is a monotone function taking values between -1 and +1 and $S_{n+1}$ is the sum of all votes purchased by the $n+1$ voters participating in the election. The utilities of the voters are assumed to arise by random sampling from a probability distribution $F_{U}$ with compact support; each voter knows her own utility, but not those of the other voters, although she does know the sampling distribution $F_{U}$. Nash equilibria for this game are described. These results imply that the expected inefficiency of any Nash equilibrium decays like $1/n$. ","Computer Science - Computer Science and Game Theory ; Mathematics - Probability ; 91B12 (Primary), 91B52, 60F99 (Secondary) ; ","Lalley, Steven P. ; Weyl, E. Glen ; "
http://arxiv.org/abs/1409.0375,Polynomial solvability of $NP$-complete problems,"  ${ NP}$-complete problem ""Hamiltonian cycle""\ for graph $G=(V,E)$ is extended to the ""Hamiltonian Complement of the Graph""\ problem of finding the minimal cardinality set $H$ containing additional edges so that graph $G=(V,E\cup H)$ is Hamiltonian. The solving of ""Hamiltonian Complement of a Graph""\ problem is reduced to the linear programming problem {\bf P}, which has an optimal integer solution. The optimal integer solution of {\bf P} is found for any its optimal solution by solving the linear assignment problem {\bf L}. The existence of polynomial algorithms for problems {\bf P} and {\bf L} proves the polynomial solvability of ${ NP}$-complete problems. ",Computer Science - Computational Complexity ; 05C85 ; ,"Panyukov, Anatoly ; "
http://arxiv.org/abs/1409.1467,Evaluation of Position-related Information in Multipath Components for   Indoor Positioning,"  Location awareness is a key factor for a wealth of wireless indoor applications. Its provision requires the careful fusion of diverse information sources. For agents that use radio signals for localization, this information may either come from signal transmissions with respect to fixed anchors, from cooperative transmissions inbetween agents, or from radar-like monostatic transmissions. Using a-priori knowledge of a floor plan of the environment, specular multipath components can be exploited, based on a geometric-stochastic channel model. In this paper, a unified framework is presented for the quantification of this type of position-related information, using the concept of equivalent Fisher information. We derive analytical results for the Cram\'er-Rao lower bound of multipath-assisted positioning, considering bistatic transmissions between agents and fixed anchors, monostatic transmissions from agents, cooperative measurements inbetween agents, and combinations thereof, including the effect of clock offsets. Awareness of this information enables highly accurate and robust indoor positioning. Computational results show the applicability of the framework for the characterization of the localization capabilities of a given environment, quantifying the influence of different system setups, signal parameters, and the impact of path overlap. ",Computer Science - Information Theory ; ,"Leitinger, Erik ; Meissner, Paul ; Rüdisser, Christoph ; Dumphart, Gregor ; Witrisal, Klaus ; "
http://arxiv.org/abs/1409.1714,A level set based method for fixing overhangs in 3D printing,"  3D printers based on the Fused Decomposition Modeling create objects layer-by-layer dropping fused material. As a consequence, strong overhangs cannot be printed because the new-come material does not find a suitable support over the last deposed layer. In these cases, one can add some support structures (scaffolds) which make the object printable, to be removed at the end. In this paper we propose a level set method to create object-dependent support structures, specifically conceived to reduce both the amount of additional material and the printing time. We also review some open problems about 3D printing which can be of interests for the mathematical community. ","Mathematics - Numerical Analysis ; Computer Science - Graphics ; 65D17, 35F21 ; ","Cacace, Simone ; Cristiani, Emiliano ; Rocchi, Leonardo ; "
http://arxiv.org/abs/1409.2193,An Epistemic Strategy Logic,"  This paper presents an extension of temporal epistemic logic with operators that quantify over agent strategies. Unlike previous work on alternating temporal epistemic logic, the semantics works with systems whose states explicitly encode the strategy being used by each of the agents. This provides a natural way to express what agents would know were they to be aware of some of the strategies being used by other agents. A number of examples that rely upon the ability to express an agent's knowledge about the strategies being used by other agents are presented to motivate the framework, including reasoning about game theoretic equilibria, knowledge-based programs, and information theoretic computer security policies. Relationships to several variants of alternating temporal epistemic logic are discussed. The computational complexity of model checking the logic and several of its fragments are also characterized. ",Computer Science - Logic in Computer Science ; ,"Huang, Xiaowei ; van der Meyden, Ron ; "
http://arxiv.org/abs/1409.2248,Secure pseudo-random linear binary sequences generators based on   arithmetic polynoms,"  We present a new approach to constructing of pseudo-random binary sequences (PRS) generators for the purpose of cryptographic data protection, secured from the perpetrator's attacks, caused by generation of masses of hardware errors and faults. The new method is based on use of linear polynomial arithmetic for the realization of systems of boolean characteristic functions of PRS' generators. ""Arithmetizatio"" of systems of logic formulas has allowed to apply mathematical apparatus of residue systems for multisequencing of the process of PRS generation and organizing control of computing errors, caused by hardware faults. This has guaranteed high security of PRS generator's functioning and, consequently, security of tools for cryptographic data protection based on those PRSs. ","Computer Science - Cryptography and Security ; 94C10, 94A60, 11K45, 11A07 ; ","Finko, Oleg ; Dichenko, Sergey ; "
http://arxiv.org/abs/1409.2612,A simple proof of the completeness of APAL,  We provide a simple proof of the completeness of arbitrary public announcement logic APAL. The proof is an improvement over the proof found in the publication Knowable as Known after an Announcement. ,Computer Science - Logic in Computer Science ; ,"Balbiani, Philippe ; van Ditmarsch, Hans ; "
http://arxiv.org/abs/1409.2908,A Framework for Practical Parallel Fast Matrix Multiplication,"  Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; G.4 ; ","Benson, Austin R. ; Ballard, Grey ; "
http://arxiv.org/abs/1409.3176,Test Case Purification for Improving Fault Localization,"  Finding and fixing bugs are time-consuming activities in software development. Spectrum-based fault localization aims to identify the faulty position in source code based on the execution trace of test cases. Failing test cases and their assertions form test oracles for the failing behavior of the system under analysis. In this paper, we propose a novel concept of spectrum driven test case purification for improving fault localization. The goal of test case purification is to separate existing test cases into small fractions (called purified test cases) and to enhance the test oracles to further localize faults. Combining with an original fault localization technique (e.g., Tarantula), test case purification results in better ranking the program statements. Our experiments on 1800 faults in six open-source Java programs show that test case purification can effectively improve existing fault localization techniques. ",Computer Science - Software Engineering ; ,"Xuan, Jifeng ; Monperrus, Martin ; "
http://arxiv.org/abs/1409.3562,Strong converse exponent for classical-quantum channel coding,"  We determine the exact strong converse exponent of classical-quantum channel coding, for every rate above the Holevo capacity. Our form of the exponent is an exact analogue of Arimoto's, given as a transform of the Renyi capacities with parameters alpha>1. It is important to note that, unlike in the classical case, there are many inequivalent ways to define the Renyi divergence of states, and hence the R\'enyi capacities of channels. Our exponent is in terms of the Renyi capacities corresponding to a version of the Renyi divergences that has been introduced recently in [M\""uller-Lennert, Dupuis, Szehr, Fehr and Tomamichel, J. Math. Phys. 54, 122203, (2013)], and [Wilde, Winter, Yang, Commun. Math. Phys. 331, (2014)]. Our result adds to the growing body of evidence that this new version is the natural definition for the purposes of strong converse problems. ",Quantum Physics ; Computer Science - Information Theory ; Mathematical Physics ; ,"Mosonyi, Milan ; Ogawa, Tomohiro ; "
http://arxiv.org/abs/1409.3600,Select with Small Groups,"  We revisit the selection problem, namely that of computing the $i$th order statistic of $n$ given elements, in particular the classical deterministic algorithm by grouping and partition due to Blum, Floyd, Pratt, Rivest, and Tarjan (1973). While the original algorithm uses groups of odd size at least $5$ and runs in linear time, it has been perpetuated in the literature that using smaller group sizes will force the worst-case running time to become superlinear, namely $\Omega(n \log{n})$. We first point out that the arguments existent in the literature justifying the superlinear worst-case running time fall short of proving this claim. We further prove that it is possible to use group size smaller than $5$ while maintaining the worst case linear running time. To this end we introduce three simple variants of the classical algorithm, the repeated step algorithm, the shifting target algorithm, and the hyperpair algorithm, all running in linear time. ",Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Chen, Ke ; Dumitrescu, Adrian ; "
http://arxiv.org/abs/1409.3954,MIMO-MC Radar: A MIMO Radar Approach Based on Matrix Completion,"  In a typical MIMO radar scenario, transmit nodes transmit orthogonal waveforms, while each receive node performs matched filtering with the known set of transmit waveforms, and forwards the results to the fusion center. Based on the data it receives from multiple antennas, the fusion center formulates a matrix, which, in conjunction with standard array processing schemes, such as MUSIC, leads to target detection and parameter estimation. In MIMO radars with compressive sensing (MIMO-CS), the data matrix is formulated by each receive node forwarding a small number of compressively obtained samples. In this paper, it is shown that under certain conditions, in both sampling cases, the data matrix at the fusion center is low-rank, and thus can be recovered based on knowledge of a small subset of its entries via matrix completion (MC) techniques. Leveraging the low-rank property of that matrix, we propose a new MIMO radar approach, termed, MIMO-MC radar, in which each receive node either performs matched filtering with a small number of randomly selected dictionary waveforms or obtains sub-Nyquist samples of the received signal at random sampling instants, and forwards the results to a fusion center. Based on the received samples, and with knowledge of the sampling scheme, the fusion center partially fills the data matrix and subsequently applies MC techniques to estimate the full matrix. MIMO-MC radars share the advantages of the recently proposed MIMO-CS radars, i.e., high resolution with reduced amounts of data, but unlike MIMO-CS radars do not require grid discretization. The MIMO-MC radar concept is illustrated through a linear uniform array configuration, and its target estimation performance is demonstrated via simulations. ",Computer Science - Information Theory ; Statistics - Applications ; ,"Sun, Shunqiao ; Bajwa, Waheed U. ; Petropulu, Athina P. ; "
http://arxiv.org/abs/1409.4575,Stable Cosparse Recovery via \ell_p-analysis Optimization,"  In this paper we study the $\ell_p$-analysis optimization ($0<p\leq1$) problem for cosparse signal recovery. We establish a bound for recovery error via the restricted $p$-isometry property over any subspace. We further prove that the nonconvex $\ell_q$-analysis optimization can do recovery with a lower sample complexity and in a wider range of cosparsity than its convex counterpart. In addition, we develop an iteratively reweighted method to solve the optimization problem under a variational framework. Empirical results of preliminary computational experiments illustrate that the nonconvex method outperforms its convex counterpart. ",Computer Science - Information Theory ; ,"Zhang, Shubao ; Qian, Hui ; Gong, Xiaojin ; Zhou, Jianying ; "
http://arxiv.org/abs/1409.4711,Doing-it-All with Bounded Work and Communication,"  We consider the Do-All problem, where $p$ cooperating processors need to complete $t$ similar and independent tasks in an adversarial setting. Here we deal with a synchronous message passing system with processors that are subject to crash failures. Efficiency of algorithms in this setting is measured in terms of work complexity (also known as total available processor steps) and communication complexity (total number of point-to-point messages). When work and communication are considered to be comparable resources, then the overall efficiency is meaningfully expressed in terms of effort defined as work + communication. We develop and analyze a constructive algorithm that has work $O( t + p \log p\, (\sqrt{p\log p}+\sqrt{t\log t}\, ) )$ and a nonconstructive algorithm that has work $O(t +p \log^2 p)$. The latter result is close to the lower bound $\Omega(t + p \log p/ \log \log p)$ on work. The effort of each of these algorithms is proportional to its work when the number of crashes is bounded above by $c\,p$, for some positive constant $c < 1$. We also present a nonconstructive algorithm that has effort $O(t + p ^{1.77})$. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Chlebus, Bogdan S. ; Gąsieniec, Leszek ; Kowalski, Dariusz R. ; Schwarzmann, Alexander A. ; "
http://arxiv.org/abs/1409.6182,A Benchmark Suite for Template Detection and Content Extraction,"  Template detection and content extraction are two of the main areas of information retrieval applied to the Web. They perform different analyses over the structure and content of webpages to extract some part of the document. However, their objective is different. While template detection identifies the template of a webpage (usually comparing with other webpages of the same website), content extraction identifies the main content of the webpage discarding the other part. Therefore, they are somehow complementary, because the main content is not part of the template. It has been measured that templates represent between 40% and 50% of data on the Web. Therefore, identifying templates is essential for indexing tasks because templates usually contain irrelevant information such as advertisements, menus and banners. Processing and storing this information is likely to lead to a waste of resources (storage space, bandwidth, etc.). Similarly, identifying the main content is essential for many information retrieval tasks. In this paper, we present a benchmark suite to test different approaches for template detection and content extraction. The suite is public, and it contains real heterogeneous webpages that have been labelled so that different techniques can be suitable (and automatically) compared. ",Computer Science - Information Retrieval ; ,"Alarte, Julián ; Insa, David ; Silva, Josep ; Tamarit, Salvador ; "
http://arxiv.org/abs/1409.6193,Estimating topological properties of weighted networks from limited   information,"  A fundamental problem in studying and modeling economic and financial systems is represented by privacy issues, which put severe limitations on the amount of accessible information. Here we introduce a novel, highly nontrivial method to reconstruct the structural properties of complex weighted networks of this kind using only partial information: the total number of nodes and links, and the values of the strength for all nodes. The latter are used as fitness to estimate the unknown node degrees through a standard configuration model. Then, these estimated degrees and the strengths are used to calibrate an enhanced configuration model in order to generate ensembles of networks intended to represent the real system. The method, which is tested on real economic and financial networks, while drastically reducing the amount of information needed to infer network properties, turns out to be remarkably effective$-$thus representing a valuable tool for gaining insights on privacy-protected socioeconomic systems. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Social and Information Networks ; Quantitative Finance - Statistical Finance ; ,"Cimini, Giulio ; Squartini, Tiziano ; Gabrielli, Andrea ; Garlaschelli, Diego ; "
http://arxiv.org/abs/1409.6777,Impossibility of Classically Simulating One-Clean-Qubit Computation,"  Deterministic quantum computation with one quantum bit (DQC1) is a restricted model of quantum computing where the input state is the completely mixed state except for a single clean qubit, and only a single output qubit is measured at the end of the computing. It is proved that the restriction of quantum computation to the DQC1 model does not change the complexity classes NQP and SBQP. As a main consequence, it follows that the DQC1 model cannot be efficiently simulated by classical computers unless the polynomial-time hierarchy collapses to the second level (more precisely, to AM), which answers the long-standing open problem posed by Knill and Laflamme under the very plausible complexity assumption. The argument developed in this paper also weakens the complexity assumption necessary for the existing impossibility results on classical simulation of various sub-universal quantum computing models, such as the IQP model and the Boson sampling. ",Quantum Physics ; Computer Science - Computational Complexity ; ,"Fujii, Keisuke ; Kobayashi, Hirotada ; Morimae, Tomoyuki ; Nishimura, Harumichi ; Tamate, Shuhei ; Tani, Seiichiro ; "
http://arxiv.org/abs/1409.7579,Field evidence of social influence in the expression of political   preferences: the case of secessionist flags in Barcelona,"  Different models of social influence have explored the dynamics of social contagion, imitation, and diffusion of different types of traits, opinions, and conducts. However, few behavioral data indicating social influence dynamics have been obtained from direct observation in `natural' social contexts. The present research provides that kind of evidence in the case of the public expression of political preferences in the city of Barcelona, where thousands of citizens supporting the secession of Catalonia from Spain have placed a Catalan flag in their balconies. We present two different studies. 1) In July 2013 we registered the number of flags in 26% of the the city. We find that there is a large dispersion in the density of flags in districts with similar density of pro-independence voters. However, we find that the density of flags tends to be fostered in those electoral district where there is a clear majority of pro-independence vote, while it is inhibited in the opposite cases. 2) During 17 days around Catalonia's 2013 National Holiday we observed the position at balcony resolution of the flags displayed in the facades of 82 blocks. We compare the clustering of flags on the facades observed each day to equivalent random distributions and find that successive hangings of flags are not independent events but that a local influence mechanism is favoring their clustering. We also find that except for the National Holiday day the density of flags tends to be fostered in those facades where there is a clear majority of pro-independence vote. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Parravano, Antonio ; Noguera, José A. ; Tena, Jordi ; Hermida, Paula ; "
http://arxiv.org/abs/1409.8061,A New DoF Upper Bound and Its Achievability for $K$-User MIMO Y Channels,"  This work is to study the degrees of freedom (DoF) for the $K$-user MIMO Y channel. Previously, two transmission frameworks have been proposed for the DoF analysis when $N \geq 2M$, where $M$ and $N$ denote the number of antennas at each source node and the relay node respectively. The first method is named as signal group based alignment proposed by Hua et al. in [1]. The second is named as signal pattern approach introduced by Wang et al. in [2]. But both of them only studied certain antenna configurations. The maximum achievable DoF in the general case still remains unknown. In this work, we first derive a new upper bound of the DoF using the genie-aided approach. Then, we propose a more general transmission framework, generalized signal alignment (GSA), and show that the previous two methods are both special cases of GSA. With GSA, we prove that the new DoF upper bound is achievable when $\frac{N}{M} \in \left(0,2+\frac{4}{K(K-1)}\right] \cup \left[K-2, +\infty\right)$. The DoF analysis in this paper provides a major step forward towards the fundamental capacity limit of the $K$-user MIMO Y channel. It also offers a new approach of integrating interference alignment with physical layer network coding. ",Computer Science - Information Theory ; ,"Liu, Kangqi ; Tao, Meixia ; "
http://arxiv.org/abs/1409.8230,RENOIR - A Dataset for Real Low-Light Image Noise Reduction,"  Image denoising algorithms are evaluated using images corrupted by artificial noise, which may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes. We also introduce a method for estimating the true noise level in our images, since even the low noise images contain small amounts of noise. We evaluate the accuracy of our noise estimation method on real and artificial noise, and investigate the Poisson-Gaussian noise model. Finally, we use our dataset to evaluate six denoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer Perceptron, and two versions of NL-means. We show that while the Multi-Layer Perceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on gray images with synthetic noise, they lag behind on our dataset. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Anaya, Josue ; Barbu, Adrian ; "
http://arxiv.org/abs/1409.8498,Non-myopic learning in repeated stochastic games,"  In repeated stochastic games (RSGs), an agent must quickly adapt to the behavior of previously unknown associates, who may themselves be learning. This machine-learning problem is particularly challenging due, in part, to the presence of multiple (even infinite) equilibria and inherently large strategy spaces. In this paper, we introduce a method to reduce the strategy space of two-player general-sum RSGs to a handful of expert strategies. This process, called Mega, effectually reduces an RSG to a bandit problem. We show that the resulting strategy space preserves several important properties of the original RSG, thus enabling a learner to produce robust strategies within a reasonably small number of interactions. To better establish strengths and weaknesses of this approach, we empirically evaluate the resulting learning system against other algorithms in three different RSGs. ",Computer Science - Computer Science and Game Theory ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Crandall, Jacob W. ; "
http://arxiv.org/abs/1409.8580,Interference Functionals in Poisson Networks,"  We propose and prove a theorem that allows the calculation of a class of functionals on Poisson point processes that have the form of expected values of sum-products of functions. In proving the theorem, we present a variant of the Campbell-Mecke theorem from stochastic geometry. We proceed to apply our result in the calculation of expected values involving interference in wireless Poisson networks. Based on this, we derive outage probabilities for transmissions in a Poisson network with Nakagami fading. Our results extend the stochastic geometry toolbox used for the mathematical analysis of interference-limited wireless networks. ",Computer Science - Information Theory ; ,"Schilcher, Udo ; Toumpis, Stavros ; Haenggi, Martin ; Crismani, Alessandro ; Brandner, Günther ; Bettstetter, Christian ; "
http://arxiv.org/abs/1410.0446,Identification of Dynamic functional brain network states Through Tensor   Decomposition,"  With the advances in high resolution neuroimaging, there has been a growing interest in the detection of functional brain connectivity. Complex network theory has been proposed as an attractive mathematical representation of functional brain networks. However, most of the current studies of functional brain networks have focused on the computation of graph theoretic indices for static networks, i.e. long-time averages of connectivity networks. It is well-known that functional connectivity is a dynamic process and the construction and reorganization of the networks is key to understanding human cognition. Therefore, there is a growing need to track dynamic functional brain networks and identify time intervals over which the network is quasi-stationary. In this paper, we present a tensor decomposition based method to identify temporally invariant 'network states' and find a common topographic representation for each state. The proposed methods are applied to electroencephalogram (EEG) data during the study of error-related negativity (ERN). ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Machine Learning ; Quantitative Biology - Neurons and Cognition ; ,"Mahyari, Arash Golibagh ; Aviyente, Selin ; "
http://arxiv.org/abs/1410.0736,HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale   Visual Recognition,"  In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; Statistics - Machine Learning ; ,"Yan, Zhicheng ; Zhang, Hao ; Piramuthu, Robinson ; Jagadeesh, Vignesh ; DeCoste, Dennis ; Di, Wei ; Yu, Yizhou ; "
http://arxiv.org/abs/1410.2670,Entropy NAND: Early Functional Completeness in Entropy Networks,"  An observer increases in relative entropy as it receives information from what it is observing. In a system of only an observer and the observed, an increase in the relative entropy of the observer is a decrease in the relative entropy of the observed. Linking together these directional entropy disequilibriums we show that NAND and NOR functionality arise in such networks at very low levels of complexity. ","Computer Science - Information Theory ; 37A35, 81P15 ; F.1.1 ; F.1.2 ; ","Jesse, Forrest Fabian ; "
http://arxiv.org/abs/1410.2752,Spatial Straight Line Linkages by Factorization of Motion Polynomials,"  We use the recently introduced factorization of motion polynomials for constructing overconstrained spatial linkages with a straight line trajectory. Unlike previous examples, the end-effector motion is not translational and the link graph is a cycle. In particular, we obtain a number of linkages with four revolute and two prismatic joints and a remarkable linkage with seven revolute joints one of whose joints performs a Darboux motion. ",Mathematics - Metric Geometry ; Computer Science - Robotics ; Mathematics - Rings and Algebras ; 70B10 ; ,"Li, Zijia ; Schicho, Josef ; Schröcker, Hans-Peter ; "
http://arxiv.org/abs/1410.3247,An easy subexponential bound for online chain partitioning,  Bosek and Krawczyk exhibited an online algorithm for partitioning an online poset of width $w$ into $w^{14\lg w}$ chains. We improve this to $w^{6.5 \lg w + 7}$ with a simpler and shorter proof by combining the work of Bosek & Krawczyk with work of Kierstead & Smith on First-Fit chain partitioning of ladder-free posets. We also provide examples illustrating the limits of our approach. ,Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; 68W27 ; ,"Bosek, Bartłomiej ; Kierstead, Hal A. ; Krawczyk, Tomasz ; Matecki, Grzegorz ; Smith, Matthew E. ; "
http://arxiv.org/abs/1410.3351,Ricci Curvature and the Manifold Learning Problem,"  Consider a sample of $n$ points taken i.i.d from a submanifold $\Sigma$ of Euclidean space. We show that there is a way to estimate the Ricci curvature of $\Sigma$ with respect to the induced metric from the sample. Our method is grounded in the notions of Carr\'e du Champ for diffusion semi-groups, the theory of Empirical processes and local Principal Component Analysis. ","Mathematics - Differential Geometry ; Computer Science - Machine Learning ; Mathematics - Metric Geometry ; Statistics - Machine Learning ; 68T05, 58J35 ; ","Ache, Antonio G. ; Warren, Micah W. ; "
http://arxiv.org/abs/1410.3764,A lazy approach to on-line bipartite matching,"  We present a new approach, called a lazy matching, to the problem of on-line matching on bipartite graphs. Imagine that one side of a graph is given and the vertices of the other side are arriving on-line. Originally, incoming vertex is either irrevocably matched to an another element or stays forever unmatched. A lazy algorithm is allowed to match a new vertex to a group of elements (possibly empty) and afterwords, forced against next vertices, may give up parts of the group. The restriction is that all the time each element is in at most one group. We present an optimal lazy algorithm (deterministic) and prove that its competitive ratio equals $1-\pi/\cosh(\frac{\sqrt{3}}{2}\pi)\approx 0.588$. The lazy approach allows us to break the barrier of $1/2$, which is the best competitive ratio that can be guaranteed by any deterministic algorithm in the classical on-line matching. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Kozik, Jakub ; Matecki, Grzegorz ; "
http://arxiv.org/abs/1410.3877,Selection-based Approach to Cooperative Interval Games,"  Cooperative interval games are a generalized model of cooperative games in which the worth of every coalition corresponds to a closed interval representing the possible outcomes of its cooperation. Selections are all possible outcomes of the interval game with no additional uncertainty.   We introduce new selection-based classes of interval games and prove their characterization theorems and relations to existing classes based on the interval weakly better operator. We show new results regarding the core and imputations and examine a problem of equivalence for two different versions of the core, the main stability solution of cooperative games. Finally, we introduce the definition of strong imputation and strong core as universal solution concepts of interval games. ","Mathematics - Optimization and Control ; Computer Science - Computer Science and Game Theory ; 65G99, 91A12 ; ","Bok, Jan ; Hladík, Milan ; "
http://arxiv.org/abs/1410.4060,Decoupling Multivariate Polynomials Using First-Order Information,"  We present a method to decompose a set of multivariate real polynomials into linear combinations of univariate polynomials in linear forms of the input variables. The method proceeds by collecting the first-order information of the polynomials in a set of operating points, which is captured by the Jacobian matrix evaluated at the operating points. The polyadic canonical decomposition of the three-way tensor of Jacobian matrices directly returns the unknown linear relations, as well as the necessary information to reconstruct the univariate polynomials. The conditions under which this decoupling procedure works are discussed, and the method is illustrated on several numerical examples. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; ,"Dreesen, Philippe ; Ishteva, Mariya ; Schoukens, Johan ; "
http://arxiv.org/abs/1410.4536,Numerical Optimization for Symmetric Tensor Decomposition,"  We consider the problem of decomposing a real-valued symmetric tensor as the sum of outer products of real-valued vectors. Algebraic methods exist for computing complex-valued decompositions of symmetric tensors, but here we focus on real-valued decompositions, both unconstrained and nonnegative, for problems with low-rank structure. We discuss when solutions exist and how to formulate the mathematical program. Numerical results show the properties of the proposed formulations (including one that ignores symmetry) on a set of test problems and illustrate that these straightforward formulations can be effective even though the problem is nonconvex. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; ,"Kolda, Tamara G. ; "
http://arxiv.org/abs/1410.4617,A Cut Principle for Information Flow,"  We view a distributed system as a graph of active locations with unidirectional channels between them, through which they pass messages. In this context, the graph structure of a system constrains the propagation of information through it.   Suppose a set of channels is a cut set between an information source and a potential sink. We prove that, if there is no disclosure from the source to the cut set, then there can be no disclosure to the sink. We introduce a new formalization of partial disclosure, called *blur operators*, and show that the same cut property is preserved for disclosure to within a blur operator. This cut-blur property also implies a compositional principle, which ensures limited disclosure for a class of systems that differ only beyond the cut. ",Computer Science - Cryptography and Security ; ,"Guttman, Joshua D. ; Rowe, Paul D. ; "
http://arxiv.org/abs/1410.5131,An Algebra of Reversible Computation,"  Process algebra ACP based on the interleaving semantics can not be reversed. We design a reversible version of APTC called RAPTC. It has algebraic laws of reversible choice, sequence, parallelism, communication, silent step and abstraction, and also the soundness and completeness modulo strongly forward-reverse truly concurrent bisimulations and weakly forward-reverse truly concurrent bisimulations. ",Computer Science - Logic in Computer Science ; ,"Wang, Yong ; "
http://arxiv.org/abs/1410.5920,Active Regression by Stratification,"  We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of $O(1/\epsilon)$ cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches the optimal risk using piecewise constant approximations. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Sabato, Sivan ; Munos, Remi ; "
http://arxiv.org/abs/1410.6516,Coalition Structure Generation on Graphs,"  Two fundamental algorithm-design paradigms are Tree Search and Dynamic Programming. The techniques used therein have been shown to complement one another when solving the complete set partitioning problem, also known as the coalition structure generation problem [5]. Inspired by this observation, we develop in this paper an algorithm to solve the coalition structure generation problem on graphs, where the goal is to identifying an optimal partition of a graph into connected subgraphs. More specifically, we develop a new depth-first search algorithm, and combine it with an existing dynamic programming algorithm due to Vinyals et al. [9]. The resulting hybrid algorithm is empirically shown to significantly outperform both its constituent parts when the subset-evaluation function happens to have certain intuitive properties. ",Computer Science - Multiagent Systems ; ,"Rahwan, Talal ; Michalak, Tomasz P. ; "
http://arxiv.org/abs/1410.7082,Complexity of LP in Terms of the Face Lattice,"  Let $X$ be a finite set in $Z^d$. We consider the problem of optimizing linear function $f(x) = c^T x$ on $X$, where $c\in Z^d$ is an input vector. We call it a problem $X$. A problem $X$ is related with linear program $\max\limits_{x \in P} f(x)$, where polytope $P$ is a convex hull of $X$. The key parameters for evaluating the complexity of a problem $X$ are the dimension $d$, the cardinality $|X|$, and the encoding size $S(X) = \log_2 \left(\max\limits_{x\in X} \|x\|_{\infty}\right)$. We show that if the (time and space) complexity of some algorithm $A$ for solving a problem $X$ is defined only in terms of combinatorial structure of $P$ and the size $S(X)$, then for every $d$ and $n$ there exists polynomially (in $d$, $\log n$, and $S$) solvable problem $Y$ with $\dim Y = d$, $|Y| = n$, such that the algorithm $A$ requires exponential time or space for solving $Y$. ",Computer Science - Computational Complexity ; Mathematics - Combinatorics ; ,"Maksimenko, Aleksandr ; "
http://arxiv.org/abs/1410.7694,Dynamic Analysis of Digital Chaotic Maps via State-Mapping Networks,"  Chaotic dynamics is widely used to design pseudo-random number generators and for other applications such as secure communications and encryption. This paper aims to study the dynamics of discrete-time chaotic maps in the digital (i.e., finite-precision) domain. Differing from the traditional approaches treating a digital chaotic map as a black box with different explanations according to the test results of the output, the dynamical properties of such chaotic maps are first explored with a fixed-point arithmetic, using the Logistic map and the Tent map as two representative examples, from a new perspective with the corresponding state-mapping networks (SMNs). In an SMN, every possible value in the digital domain is considered as a node and the mapping relationship between any pair of nodes is a directed edge. The scale-free properties of the Logistic map's SMN are proved. The analytic results are further extended to the scenario of floating-point arithmetic and for other chaotic maps. Understanding the network structure of a chaotic map's SMN in digital computers can facilitate counteracting the undesirable degeneration of chaotic dynamics in finite-precision domains, helping also classify and improve the randomness of pseudo-random number sequences generated by iterating chaotic maps. ","Computer Science - Cryptography and Security ; Nonlinear Sciences - Chaotic Dynamics ; 37D45, 05C82 ; ","Li, Chengqing ; Feng, Bingbing ; Li, Shujun ; Kurths, Juergen ; Chen, Guanrong ; "
http://arxiv.org/abs/1410.8663,On the Inequalities of Projected Volumes and the Constructible Region,"  We study the following geometry problem: given a $2^n-1$ dimensional vector $\pi=\{\pi_S\}_{S\subseteq [n], S\ne \emptyset}$, is there an object $T\subseteq\mathbb{R}^n$ such that $\log(\mathsf{vol}(T_S))= \pi_S$, for all $S\subseteq [n]$, where $T_S$ is the projection of $T$ to the subspace spanned by the axes in $S$? If $\pi$ does correspond to an object in $\mathbb{R}^n$, we say that $\pi$ is {\em constructible}. We use $\Psi_n$ to denote the constructible region, i.e., the set of all constructible vectors in $\mathbb{R}^{2^n-1}$. In 1995, Bollob\'{a}s and Thomason showed that $\Psi_n$ is contained in a polyhedral cone, defined a class of so called uniform cover inequalities. We propose a new set of natural inequalities, called nonuniform-cover inequalities, which generalize the BT inequalities. We show that any linear inequality that all points in $\Psi_n$ satisfy must be a nonuniform-cover inequality. Based on this result and an example by Bollob\'{a}s and Thomason, we show that constructible region $\Psi_n$ is not even convex, and thus cannot be fully characterized by linear inequalities. We further show that some subclasses of the nonuniform-cover inequalities are not correct by various combinatorial constructions, which refutes a previous conjecture about $\Psi_n$. Finally, we conclude with an interesting conjecture regarding the convex hull of $\Psi_n$. ",Computer Science - Discrete Mathematics ; ,"Tan, Zihan ; Zeng, Liwei ; "
http://arxiv.org/abs/1411.0187,Construction of Capacity-Achieving Lattice Codes: Polar Lattices,"  In this paper, we propose a new class of lattices constructed from polar codes, namely polar lattices, to achieve the capacity $\frac{1}{2}\log(1+\SNR)$ of the additive white Gaussian-noise (AWGN) channel. Our construction follows the multilevel approach of Forney \textit{et al.}, where we construct a capacity-achieving polar code on each level. The component polar codes are shown to be naturally nested, thereby fulfilling the requirement of the multilevel lattice construction. We prove that polar lattices are \emph{AWGN-good}. Furthermore, using the technique of source polarization, we propose discrete Gaussian shaping over the polar lattice to satisfy the power constraint. Both the construction and shaping are explicit, and the overall complexity of encoding and decoding is $O(N\log N)$ for any fixed target error probability. ",Computer Science - Information Theory ; ,"Liu, Ling ; Yan, Yanfei ; Ling, Cong ; Wu, Xiaofu ; "
http://arxiv.org/abs/1411.0440,Modelling serendipity in a computational context,"  We understand the term serendipity to describe a creative process that develops, in context, with the active participation of a creative agent, but not entirely within that agent's control. While a system cannot be made to perform serendipitously on demand, nevertheless, we argue that its \emph{serendipity potential} can be increased by means of a suitable system architecture and other design choices. We distil a unified description of serendipitous occurrences from historical theorisations of serendipity and creativity. This takes the form of a framework with six phases: \emph{perception}, \emph{attention}, \emph{interest}, \emph{explanation}, \emph{bridge}, and \emph{valuation}. We then use this framework to organise a survey of literature in cognitive science, philosophy, and computing, which yields practical definitions of the six phases, along with heuristics for implementation. We use the resulting model to evaluate the serendipity potential of four existing systems developed by others, and two systems previously developed by two of us. Whereas most existing research that considers serendipity in a computing context deals with serendipity as a service, we relate theories of serendipity to artificial intelligence practice and the development of autonomous systems. We outline representative directions for future applications of our model in the domains of automated programming, recommender systems, and computational creativity. We conclude that it is feasible to equip computational systems with the potential for serendipity, and that this could be beneficial in varied artificial intelligence applications, particularly those designed to operate responsively in real-world contexts. ",Computer Science - Artificial Intelligence ; I.2.11 ; D.2.2 ; ,"Corneli, Joseph ; Jordanous, Anna ; Guckelsberger, Christian ; Pease, Alison ; Colton, Simon ; "
http://arxiv.org/abs/1411.0998,Jointly Private Convex Programming,"  In this paper we present an extremely general method for approximately solving a large family of convex programs where the solution can be divided between different agents, subject to joint differential privacy. This class includes multi-commodity flow problems, general allocation problems, and multi-dimensional knapsack problems, among other examples. The accuracy of our algorithm depends on the \emph{number} of constraints that bind between individuals, but crucially, is \emph{nearly independent} of the number of primal variables and hence the number of agents who make up the problem. As the number of agents in a problem grows, the error we introduce often becomes negligible.   We also consider the setting where agents are strategic and have preferences over their part of the solution. For any convex program in this class that maximizes \emph{social welfare}, there is a generic reduction that makes the corresponding optimization \emph{approximately dominant strategy truthful} by charging agents prices for resources as a function of the approximately optimal dual variables, which are themselves computed under differential privacy. Our results substantially expand the class of problems that are known to be solvable under both privacy and incentive constraints. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; ,"Hsu, Justin ; Huang, Zhiyi ; Roth, Aaron ; Wu, Zhiwei Steven ; "
http://arxiv.org/abs/1411.1124,Nearly Linear-Time Packing and Covering LP Solvers,"  Packing and covering linear programs (PC-LPs) form an important class of linear programs (LPs) across computer science, operations research, and optimization. In 1993, Luby and Nisan constructed an iterative algorithm for approximately solving PC-LPs in nearly linear time, where the time complexity scales nearly linearly in $N$, the number of nonzero entries of the matrix, and polynomially in $\varepsilon$, the (multiplicative) approximation error. Unfortunately, all existing nearly linear-time algorithms for solving PC-LPs require time at least proportional to $\varepsilon^{-2}$.   In this paper, we break this longstanding barrier by designing a packing solver that runs in time $\tilde{O}(N \varepsilon^{-1})$ and covering LP solver that runs in time $\tilde{O}(N \varepsilon^{-1.5})$. Our packing solver can be extended to run in time $\tilde{O}(N \varepsilon^{-1})$ for a class of well-behaved covering programs. In a follow-up work, Wang et al. showed that all covering LPs can be converted into well-behaved ones by a reduction that blows up the problem size only logarithmically.   At high level, these two algorithms can be described as linear couplings of several first-order descent steps. This is an application of our linear coupling technique to problems that are not amenable to blackbox applications known iterative algorithms in convex optimization. ",Computer Science - Data Structures and Algorithms ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; Mathematics - Optimization and Control ; ,"Allen-Zhu, Zeyuan ; Orecchia, Lorenzo ; "
http://arxiv.org/abs/1411.1420,Eigenvectors of Orthogonally Decomposable Functions,"  The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by the spectral theorem is a foundational result in applied mathematics. Motivated by a shared structure found in inferential problems of recent interest---namely orthogonal tensor decompositions, Independent Component Analysis (ICA), topic models, spectral clustering, and Gaussian mixture learning---we generalize the eigendecomposition from quadratic forms to a broad class of ""orthogonally decomposable"" functions. We identify a key role of convexity in our extension, and we generalize two traditional characterizations of eigenvectors: First, the eigenvectors of a quadratic form arise from the optima structure of the quadratic form on the sphere. Second, the eigenvectors are the fixed points of the power iteration.   In our setting, we consider a simple first order generalization of the power method which we call gradient iteration. It leads to efficient and easily implementable methods for basis recovery. It includes influential Machine Learning methods such as cumulant-based FastICA and the tensor power iteration for orthogonally decomposable tensors as special cases.   We provide a complete theoretical analysis of gradient iteration using the structure theory of discrete dynamical systems to show almost sure convergence and fast (super-linear) convergence rates. The analysis also extends to the case when the observed function is only approximately orthogonally decomposable, with bounds that are polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a non-linear version of the classical Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. ",Computer Science - Machine Learning ; ,"Belkin, Mikhail ; Rademacher, Luis ; Voss, James ; "
http://arxiv.org/abs/1411.1751,Playing the Wrong Game: Bounding Externalities in Diverse Populations of   Agents,"  The robustness of multiagent systems can be affected by mistakes or behavioral biases (e.g., risk-aversion, altruism, toll-sensitivity), with some agents playing the ""wrong game."" This can change the set of equilibria, and may in turn harm or improve the social welfare of agents in the system. We are interested in bounding what we call the biased price of anarchy (BPoA) in populations with diverse agent behaviors, which is the ratio between welfare in the ""wrong"" equilibrium and optimal welfare.   We study nonatomic routing games, and derive an externality bound that depends on a key topological parameter of the underlying network.   We then prove two general BPoA bounds for games with diverse populations: one that relies on the network structure and the average bias of all agents in the population, and one that is independent of the structure but depends on the maximal bias. Both types of bounds can be combined with known results to derive concrete BPoA bounds for a variety of specific behaviors (e.g., varied levels of risk-aversion). ",Computer Science - Computer Science and Game Theory ; Computer Science - Multiagent Systems ; ,"Meir, Reshef ; Parkes, David ; "
http://arxiv.org/abs/1411.2419,Beta-expansions of rational numbers in quadratic Pisot bases,"  We study rational numbers with purely periodic R\'enyi $\beta$-expansions. For bases $\beta$ satisfying $\beta^2=a\beta+b$ with $b$ dividing $a$, we give a necessary and sufficient condition for $\gamma(\beta)=1$, i.e., that all rational numbers $p/q\in[0,1)$ with $\gcd(q,b)=1$ have a purely periodic $\beta$-expansion. A simple algorithm for determining the value of $\gamma(\beta)$ for all quadratic Pisot numbers $\beta$ is described. ",Mathematics - Dynamical Systems ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; Mathematics - Number Theory ; 11A63 (11R06 37B10) ; ,"Hejda, Tomáš ; Steiner, Wolfgang ; "
http://arxiv.org/abs/1411.3010,Computational Complexity of Functions,"  Below is a translation from my Russian paper. I added references, unavailable to me in Moscow. Similar results have been also given in [Schnorr Stumpf 75] (see also [Lynch 75]). Earlier relevant work (classical theorems like Compression, Speed-up, etc.) was done in [Tseitin 56, Rabin 59, Hartmanis Stearns 65, Blum 67, Trakhtenbrot 67, Meyer Fischer 72].   I translated only the part with the statement of the results. Instead of the proof part I appended a later (1979, unpublished) proof sketch of a slightly tighter version. The improvement is based on the results of [Meyer Winklmann 78, Sipser 78]. Meyer and Winklmann extended earlier versions to machines with a separate input and working tape, thus allowing complexities smaller than the input length (down to its log). Sipser showed the space-bounded Halting Problem to require only additive constant overhead. The proof in the appendix below employs both advances to extend the original proofs to machines with a fixed alphabet and a separate input and working space. The extension has no (even logarithmic) restrictions on complexity and no overhead (beyond an additive constant). The sketch is very brief and a more detailed exposition is expected later: [Seiferas Meyer]. ",Computer Science - Computational Complexity ; ,"Levin, Leonid A. ; "
http://arxiv.org/abs/1411.3140,Social media fingerprints of unemployment,"  Recent wide-spread adoption of electronic and pervasive technologies has enabled the study of human behavior at an unprecedented level, uncovering universal patterns underlying human activity, mobility, and inter-personal communication. In the present work, we investigate whether deviations from these universal patterns may reveal information about the socio-economical status of geographical regions. We quantify the extent to which deviations in diurnal rhythm, mobility patterns, and communication styles across regions relate to their unemployment incidence. For this we examine a country-scale publicly articulated social media dataset, where we quantify individual behavioral features from over 145 million geo-located messages distributed among more than 340 different Spanish economic regions, inferred by computing communities of cohesive mobility fluxes. We find that regions exhibiting more diverse mobility fluxes, earlier diurnal rhythms, and more correct grammatical styles display lower unemployment rates. As a result, we provide a simple model able to produce accurate, easily interpretable reconstruction of regional unemployment incidence from their social-media digital fingerprints alone. Our results show that cost-effective economical indicators can be built based on publicly-available social media datasets. ","Physics - Physics and Society ; Computer Science - Social and Information Networks ; Physics - Data Analysis, Statistics and Probability ; ","Llorente, Alejandro ; Garcia-Herranz, Manuel ; Cebrian, Manuel ; Moro, Esteban ; "
http://arxiv.org/abs/1411.3444,Growing Scale-free Networks by a Mediation-Driven Attachment Rule,"  We propose a model that generates a new class of networks exhibiting power-law degree distribution with a spectrum of exponents depending on the number of links ($m$) with which incoming nodes join the existing network. Unlike the Barab\'{a}si-Albert (BA) model, each new node first picks an existing node at random, and connects not with this but with $m$ of its neighbors also picked at random. Counterintuitively enough, such a mediation-driven attachment rule results not only in preferential but super-preferential attachment, albeit in disguise. We show that for small $m$, the dynamics of our model is governed by winners take all phenomenon, and for higher $m$ it is governed by winners take some. Besides, we show that the mean of the inverse harmonic mean of degrees of the neighborhood of all existing nodes is a measure that can well qualify how straight the degree distribution is. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Social and Information Networks ; ,"Hassan, Kamrul ; Islam, Liana ; "
http://arxiv.org/abs/1411.3517,Derandomized Graph Product Results using the Low Degree Long Code,"  In this paper, we address the question of whether the recent derandomization results obtained by the use of the low-degree long code can be extended to other product settings. We consider two settings: (1) the graph product results of Alon, Dinur, Friedgut and Sudakov [GAFA, 2004] and (2) the ""majority is stablest"" type of result obtained by Dinur, Mossel and Regev [SICOMP, 2009] and Dinur and Shinkar [In Proc. APPROX, 2010] while studying the hardness of approximate graph coloring.   In our first result, we show that there exists a considerably smaller subgraph of $K_3^{\otimes R}$ which exhibits the following property (shown for $K_3^{\otimes R}$ by Alon et al.): independent sets close in size to the maximum independent set are well approximated by dictators.   The ""majority is stablest"" type of result of Dinur et al. and Dinur and Shinkar shows that if there exist two sets of vertices $A$ and $B$ in $K_3^{\otimes R}$ with very few edges with one endpoint in $A$ and another in $B$, then it must be the case that the two sets $A$ and $B$ share a single influential coordinate. In our second result, we show that a similar ""majority is stablest"" statement holds good for a considerably smaller subgraph of $K_3^{\otimes R}$. Furthermore using this result, we give a more efficient reduction from Unique Games to the graph coloring problem, leading to improved hardness of approximation results for coloring. ",Computer Science - Computational Complexity ; ,"Dinur, Irit ; Harsha, Prahladh ; Srinivasan, Srikanth ; Varma, Girish ; "
http://arxiv.org/abs/1411.4097,A Game Theoretic Model for the Formation of Navigable Small-World   Networks --- the Tradeoff between Distance and Reciprocity,"  Kleinberg proposed a family of small-world networks to explain the navigability of large-scale real-world social networks. However, the underlying mechanism that drives real networks to be navigable is not yet well understood. In this paper, we present a game theoretic model for the formation of navigable small world networks. We model the network formation as a Distance-Reciprocity Balanced (DRB) game in which people seek for both high reciprocity and long-distance relationships. We show that the game has only two Nash equilibria: One is the navigable small-world network, and the other is the random network in which each node connects with each other node with equal probability. We further show that the navigable small world is very stable --- (a) no collusion of any size would benefit from deviating from it; and (b) after an arbitrary deviations of a large random set of nodes, the network would return to the navigable small world as soon as every node takes one best-response step. In contrast, for the random network, a small group collusion or random perturbations is guaranteed to move the network to the navigable network as soon as every node takes one best-response step. Moreover, we show that navigable small world has much better social welfare than the random network, and provide the price-of-anarchy and price-of-stability results of the game. Our empirical evaluation demonstrates that the system always converges to the navigable network even when limited or no information about other players' strategies is available, and the DRB game simulated on the real-world network leads to navigability characteristic that is very close to that of the real network. Our theoretical and empirical analyses provide important new insight on the connection between distance, reciprocity and navigability in social networks. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Yang, Zhi ; Chen, Wei ; "
http://arxiv.org/abs/1411.4498,Scalable Wake-up of Multi-Channel Single-Hop Radio Networks,"  We consider single-hop radio networks with multiple channels as a model of wireless networks. There are $n$ stations connected to $b$ radio channels that do not provide collision detection. A station uses all the channels concurrently and independently. Some $k$ stations may become active spontaneously at arbitrary times. The goal is to wake up the network, which occurs when all the stations hear a successful transmission on some channel. Duration of a waking-up execution is measured starting from the first spontaneous activation. We present a deterministic algorithm for the general problem that wakes up the network in $O(k\log^{1/b} k\log n)$ time, where $k$ is unknown. We give a deterministic scalable algorithm for the special case when $b>d \log \log n$, for some constant $d>1$, which wakes up the network in $O(\frac{k}{b}\log n\log(b\log n))$ time, with $k$ unknown. This algorithm misses time optimality by at most a factor of $O(\log n(\log b +\log\log n))$, because any deterministic algorithm requires $\Omega(\frac{k}{b}\log \frac{n}{k})$ time. We give a randomized algorithm that wakes up the network within $O(k^{1/b}\ln \frac{1}{\epsilon})$ rounds with a probability that is at least $1-\epsilon$, for any $0<\epsilon<1$, where $k$ is known. We also consider a model of jamming, in which each channel in any round may be jammed to prevent a successful transmission, which happens with some known parameter probability $p$, independently across all channels and rounds. For this model, we give two deterministic algorithms for unknown~$k$: one wakes up the network in time $O(\log^{-1}(\frac{1}{p})\, k\log n\log^{1/b} k)$, and the other in time $O(\log^{-1}(\frac{1}{p}) \, \frac{k}{b} \log n\log(b\log n))$ but assuming the inequality $b>\log(128b\log n)$, both with a probability that is at least $1-1/\mbox{poly}(n)$. ",Computer Science - Data Structures and Algorithms ; ,"Chlebus, Bogdan S. ; De Marco, Gianluca ; Kowalski, Dariusz R. ; "
http://arxiv.org/abs/1411.4696,Security Analysis of the Unrestricted Identity-Based Aggregate Signature   Scheme,"  Aggregate signatures allow anyone to combine different signatures signed by different signers on different messages into a single short signature. An ideal aggregate signature scheme is an identity-based aggregate signature (IBAS) scheme that supports full aggregation since it can reduce the total transmitted data by using an identity string as a public key and anyone can freely aggregate different signatures. Constructing a secure IBAS scheme that supports full aggregation in bilinear maps is an important open problem. Recently, Yuan {\it et al.} proposed an IBAS scheme with full aggregation in bilinear maps and claimed its security in the random oracle model under the computational Diffie-Hellman assumption. In this paper, we show that there exists an efficient forgery attacker on their IBAS scheme and their security proof has a serious flaw. ",Computer Science - Cryptography and Security ; ,"Lee, Kwangsu ; Lee, Dong Hoon ; "
http://arxiv.org/abs/1411.4823,Automated Reasoning in Deontic Logic,"  Deontic logic is a very well researched branch of mathematical logic and philosophy. Various kinds of deontic logics are discussed for different application domains like argumentation theory, legal reasoning, and acts in multi-agent systems. In this paper, we show how standard deontic logic can be stepwise transformed into description logic and DL- clauses, such that it can be processed by Hyper, a high performance theorem prover which uses a hypertableau calculus. Two use cases, one from multi-agent research and one from the development of normative system are investigated. ",Computer Science - Artificial Intelligence ; ,"Furbach, Ulrich ; Schon, Claudia ; Stolzenburg, Frieder ; "
http://arxiv.org/abs/1411.4840,Dual-induced multifractality in online viewing activity,"  Although recent studies have found that the long-term correlations relating to the fat-tailed distribution of inter-event times exist in human activity, and that these correlations indicate the presence of fractality, the property of fractality and its origin have not been analyzed. We use both DFA and MFDFA to analyze the time series in online viewing activity separating from Movielens and Netflix. We find long-term correlations at both the individual and communal level, and that the extent of correlation at the individual level is determined by the activity level. These long-term correlations also indicate that there is fractality in the pattern of online viewing. And, we firstly find a multifractality that results from the combined effect of the fat-tailed distribution of inter-event times (i.e., the times between successive viewing actions of individual) and the long-term correlations in online viewing activity and verify this finding using three synthesized series. Therefore, it can be concluded that the multifractality in online viewing activity is caused by both the fat-tailed distribution of inter-event times and the long-term correlations, and that this enlarges the generic property of human activity to include not just physical space, but also cyberspace. ","Physics - Physics and Society ; Computer Science - Social and Information Networks ; Physics - Data Analysis, Statistics and Probability ; ","Qin, Yu-Hao ; Zhao, Zhi-Dan ; Cai, Shi-Min ; Gao, Liang ; Stanley, H. Eugene ; "
http://arxiv.org/abs/1411.5123,Deterministic Edge Connectivity in Near-Linear Time,"  We present a deterministic near-linear time algorithm that computes the edge-connectivity and finds a minimum cut for a simple undirected unweighted graph G with n vertices and m edges. This is the first o(mn) time deterministic algorithm for the problem. In near-linear time we can also construct the classic cactus representation of all minimum cuts.   The previous fastest deterministic algorithm by Gabow from STOC'91 took ~O(m+k^2 n), where k is the edge connectivity, but k could be Omega(n).   At STOC'96 Karger presented a randomized near linear time Monte Carlo algorithm for the minimum cut problem. As he points out, there is no better way of certifying the minimality of the returned cut than to use Gabow's slower deterministic algorithm and compare sizes.   Our main technical contribution is a near-linear time algorithm that contract vertex sets of a simple input graph G with minimum degree d, producing a multigraph with ~O(m/d) edges which preserves all minimum cuts of G with at least 2 vertices on each side.   In our deterministic near-linear time algorithm, we will decompose the problem via low-conductance cuts found using PageRank a la Brin and Page (1998), as analyzed by Andersson, Chung, and Lang at FOCS'06. Normally such algorithms for low-conductance cuts are randomized Monte Carlo algorithms, because they rely on guessing a good start vertex. However, in our case, we have so much structure that no guessing is needed. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Kawarabayashi, Ken-ichi ; Thorup, Mikkel ; "
http://arxiv.org/abs/1411.5166,Subtyping in Java is a Fractal,"  While developing their software, professional object-oriented (OO) software developers keep in their minds an image of the subtyping relation between types in their software. The goal of this paper is to present an observation about the graph of the subtyping relation in Java, namely the observation that, after the addition of generics---and of wildcards, in particular---to Java, the graph of the subtyping relation is no longer a simple directed-acyclic graph (DAG), as in pre-generics Java, but is rather a fractal. Further, this observation equally applies to other mainstream nominally-typed OO languages (such as C#, C++ and Scala) where generics and wildcards (or some other form of 'variance annotations') are standard features. Accordingly, the shape of the subtyping relation in these OO languages is more complex than a tree or a simple DAG, and indeed is also a fractal. Given the popularity of fractals, the fractal observation may help OO software developers keep a useful and intuitive mental image of their software's subtyping relation, even if it is a little more frightening, and more amazing one than before. With proper support from IDEs, the fractal observation can help OO developers in resolving type errors they may find in their code in lesser time, and with more confidence. ",Computer Science - Programming Languages ; Mathematics - Geometric Topology ; ,"AbdelGawad, Moez A. ; "
http://arxiv.org/abs/1411.5735,"Minimization of Transformed $L_1$ Penalty: Theory, Difference of Convex   Function Algorithm, and Robust Application in Compressed Sensing","  We study the minimization problem of a non-convex sparsity promoting penalty function, the transformed $l_1$ (TL1), and its application in compressed sensing (CS). The TL1 penalty interpolates $l_0$ and $l_1$ norms through a nonnegative parameter $a \in (0,+\infty)$, similar to $l_p$ with $p \in (0,1]$, and is known to satisfy unbiasedness, sparsity and Lipschitz continuity properties. We first consider the constrained minimization problem and discuss the exact recovery of $l_0$ norm minimal solution based on the null space property (NSP). We then prove the stable recovery of $l_0$ norm minimal solution if the sensing matrix $A$ satisfies a restricted isometry property (RIP). Next, we present difference of convex algorithms for TL1 (DCATL1) in computing TL1-regularized constrained and unconstrained problems in CS. The inner loop concerns an $l_1$ minimization problem on which we employ the Alternating Direction Method of Multipliers (ADMM). For the unconstrained problem, we prove convergence of DCATL1 to a stationary point satisfying the first order optimality condition. In numerical experiments, we identify the optimal value $a=1$, and compare DCATL1 with other CS algorithms on two classes of sensing matrices: Gaussian random matrices and over-sampled discrete cosine transform matrices (DCT). We find that for both classes of sensing matrices, the performance of DCATL1 algorithm (initiated with $l_1$ minimization) always ranks near the top (if not the top), and is the most robust choice insensitive to the conditioning of the sensing matrix $A$. DCATL1 is also competitive in comparison with DCA on other non-convex penalty functions commonly used in statistics with two hyperparameters. ",Computer Science - Information Theory ; ,"Zhang, Shuai ; Xin, Jack ; "
http://arxiv.org/abs/1411.5878,Salient Object Detection: A Survey,"  Detecting and segmenting salient objects in natural scenes, often referred to as salient object detection, has attracted a lot of interest in computer vision. While many models have been proposed and several applications have emerged, yet a deep understanding of achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics in salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance and suggest future research directions. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Quantitative Biology - Neurons and Cognition ; ,"Borji, Ali ; Cheng, Ming-Ming ; Hou, Qibin ; Jiang, Huaizu ; Li, Jia ; "
http://arxiv.org/abs/1411.6057,Mesoscopic analysis of online social networks - The role of negative   ties,"  A class of networks are those with both positive and negative links. In this manuscript, we studied the interplay between positive and negative ties on mesoscopic level of these networks, i.e., their community structure. A community is considered as a tightly interconnected group of actors; therefore, it does not borrow any assumption from balance theory and merely uses the well-known assumption in the community detection literature. We found that if one detects the communities based on only positive relations (by ignoring the negative ones), the majority of negative relations are already placed between the communities. In other words, negative ties do not have a major role in detecting communities of studied signed networks. Moreover, regarding the internal negative ties, we proved that most unbalanced communities are maximally balanced, and hence they cannot be partitioned into k nonempty sub-clusters with higher balancedness (k >= 2). Furthermore, we showed that although the mediator triad ++- (hostile-mediator-hostile) is underrepresented, it constitutes a considerable portion of triadic relations among communities. Hence, mediator triads should not be ignored by community detection and clustering algorithms. As a result, if one uses a clustering algorithm that operates merely based on social balance, mesoscopic structure of signed networks significantly remains hidden. ","Physics - Physics and Society ; Computer Science - Social and Information Networks ; Physics - Data Analysis, Statistics and Probability ; ","Esmailian, Pouya ; Abtahi, Seyed Ebrahim ; Jalili, Mahdi ; "
http://arxiv.org/abs/1411.6538,Efficient storage of Pareto points in biobjective mixed integer   programming,"  In biobjective mixed integer linear programs (BOMILPs), two linear objectives are minimized over a polyhedron while restricting some of the variables to be integer. Since many of the techniques for finding or approximating the Pareto set of a BOMILP use and update a subset of nondominated solutions, it is highly desirable to efficiently store this subset. We present a new data structure, a variant of a binary tree that takes as input points and line segments in $\R^2$ and stores the nondominated subset of this input. When used within an exact solution procedure, such as branch-and-bound (BB), at termination this structure contains the set of Pareto optimal solutions.   We compare the efficiency of our structure in storing solutions to that of a dynamic list which updates via pairwise comparison. Then we use our data structure in two biobjective BB techniques available in the literature and solve three classes of instances of BOMILP, one of which is generated by us. The first experiment shows that our data structure handles up to $10^7$ points or segments much more efficiently than a dynamic list. The second experiment shows that our data structure handles points and segments much more efficiently than a list when used in a BB. ","Computer Science - Data Structures and Algorithms ; Mathematics - Optimization and Control ; 90C29, 68P05, 90C11, ; E.1.7 ; ","Adelgren, Nathan ; Belotti, Pietro ; Gupte, Akshay ; "
http://arxiv.org/abs/1411.6907,Spatiotemporal Modeling of a Pervasive Game,"  Given pervasive games that maintain a virtual spatiotemporal model of the physical world, game designers must contend with space and time in the virtual and physical, but an integrated conceptual model is lacking. Because the problem domains of GIS and Pervasive Games overlap, Peuquet's Triad Representational Framework is exapted, from the domain of GIS, and applied to Pervasive Games. Using Dix et al.'s three types of space and Langran's notion of time, virtual time and space are then be mapped to the physical world and vice versa. The approach is evaluated using the pervasive game called Codename: Heroes, as case study. ",Computer Science - Other Computer Science ; ,"Nevelsteen, Kim J. L. ; "
http://arxiv.org/abs/1411.7086,Discrete Sampling: A graph theoretic approach to Orthogonal   Interpolation,"  We study the problem of finding unitary submatrices of the $N \times N$ discrete Fourier transform matrix, in the context of interpolating a discrete bandlimited signal using an orthogonal basis. This problem is related to a diverse set of questions on idempotents on $\mathbb{Z}_N$ and tiling $\mathbb{Z}_N$. In this work, we establish a graph-theoretic approach and connections to the problem of finding maximum cliques. We identify the key properties of these graphs that make the interpolation problem tractable when $N$ is a prime power, and we identify the challenges in generalizing to arbitrary $N$. Finally, we investigate some connections between graph properties and the spectral-tile direction of the Fuglede conjecture. ",Computer Science - Information Theory ; ,"Siripuram, Aditya ; Wu, William ; Osgood, Brad ; "
http://arxiv.org/abs/1411.7087,Consistency proof of a fragment of PV with substitution in bounded   arithmetic,"  This paper presents proof that Buss's $S^2_2$ can prove the consistency of a fragment of Cook and Urquhart's $\mathrm{PV}$ from which induction has been removed but substitution has been retained.   This result improves Beckmann's result, which proves the consistency of such a system without substitution in bounded arithmetic $S^1_2$.   Our proof relies on the notion of ""computation"" of the terms of $\mathrm{PV}$.   In our work, we first prove that, in the system under consideration, if an equation is proved and either its left- or right-hand side is computed, then there is a corresponding computation for its right- or left-hand side, respectively.   By carefully computing the bound of the size of the computation, the proof of this theorem inside a bounded arithmetic is obtained, from which the consistency of the system is readily proven.   This result apparently implies the separation of bounded arithmetic because Buss and Ignjatovi\'c stated that it is not possible to prove the consistency of a fragment of $\mathrm{PV}$ without induction but with substitution in Buss's $S^1_2$.   However, their proof actually shows that it is not possible to prove the consistency of the system, which is obtained by the addition of propositional logic and other axioms to a system such as ours.   On the other hand, the system that we have considered is strictly equational, which is a property on which our proof relies. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03F03, 03D15 ; F.4.1 ; ","Yamagata, Yoriyuki ; "
http://arxiv.org/abs/1411.7346,A Chasm Between Identity and Equivalence Testing with Conditional   Queries,"  A recent model for property testing of probability distributions (Chakraborty et al., ITCS 2013, Canonne et al., SICOMP 2015) enables tremendous savings in the sample complexity of testing algorithms, by allowing them to condition the sampling on subsets of the domain. In particular, Canonne, Ron, and Servedio (SICOMP 2015) showed that, in this setting, testing identity of an unknown distribution $D$ (whether $D=D^\ast$ for an explicitly known $D^\ast$) can be done with a constant number of queries, independent of the support size $n$ -- in contrast to the required $\Omega(\sqrt{n})$ in the standard sampling model. It was unclear whether the same stark contrast exists for the case of testing equivalence, where both distributions are unknown. While Canonne et al. established a $\mathrm{poly}(\log n)$-query upper bound for equivalence testing, very recently brought down to $\tilde O(\log\log n)$ by Falahatgar et al. (COLT 2015), whether a dependence on the domain size $n$ is necessary was still open, and explicitly posed by Fischer at the Bertinoro Workshop on Sublinear Algorithms (2014). We show that any testing algorithm for equivalence must make $\Omega(\sqrt{\log\log n})$ queries in the conditional sampling model. This demonstrates a gap between identity and equivalence testing, absent in the standard sampling model (where both problems have sampling complexity $n^{\Theta(1)}$).   We also obtain results on the query complexity of uniformity testing and support-size estimation with conditional samples. We answer a question of Chakraborty et al. (ITCS 2013) showing that non-adaptive uniformity testing indeed requires $\Omega(\log n)$ queries in the conditional model. For the related problem of support-size estimation, we provide both adaptive and non-adaptive algorithms, with query complexities $\mathrm{poly}(\log\log n)$ and $\mathrm{poly}(\log n)$, respectively. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Machine Learning ; Mathematics - Probability ; Mathematics - Statistics Theory ; ,"Acharya, Jayadev ; Canonne, Clément L. ; Kamath, Gautam ; "
http://arxiv.org/abs/1411.7632,Semidefinite Programming Approach to Gaussian Sequential Rate-Distortion   Trade-offs,"  Sequential rate-distortion (SRD) theory provides a framework for studying the fundamental trade-off between data-rate and data-quality in real-time communication systems. In this paper, we consider the SRD problem for multi-dimensional time-varying Gauss-Markov processes under mean-square distortion criteria. We first revisit the sensor-estimator separation principle, which asserts that considered SRD problem is equivalent to a joint sensor and estimator design problem in which data-rate of the sensor output is minimized while the estimator's performance satisfies the distortion criteria. We then show that the optimal joint design can be performed by semidefinite programming. A semidefinite representation of the corresponding SRD function is obtained. Implications of the obtained result in the context of zero-delay source coding theory and applications to networked control theory are also discussed. ",Mathematics - Optimization and Control ; Computer Science - Information Theory ; ,"Tanaka, Takashi ; Kim, Kwang-Ki K. ; Parrilo, Pablo A. ; Mitter, Sanjoy K. ; "
http://arxiv.org/abs/1411.7895,Influence of sociodemographic characteristics on human mobility,"  Human mobility has been traditionally studied using surveys that deliver snapshots of population displacement patterns. The growing accessibility to ICT information from portable digital media has recently opened the possibility of exploring human behavior at high spatio-temporal resolutions. Mobile phone records, geolocated tweets, check-ins from Foursquare or geotagged photos, have contributed to this purpose at different scales, from cities to countries, in different world areas. Many previous works lacked, however, details on the individuals' attributes such as age or gender. In this work, we analyze credit-card records from Barcelona and Madrid and by examining the geolocated credit-card transactions of individuals living in the two provinces, we find that the mobility patterns vary according to gender, age and occupation. Differences in distance traveled and travel purpose are observed between younger and older people, but, curiously, either between males and females of similar age. While mobility displays some generic features, here we show that sociodemographic characteristics play a relevant role and must be taken into account for mobility and epidemiological modelization. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Lenormand, Maxime ; Louail, Thomas ; Cantu-Ros, Oliva G. ; Picornell, Miguel ; Herranz, Ricardo ; Arias, Juan Murillo ; Barthelemy, Marc ; Miguel, Maxi San ; Ramasco, Jose J. ; "
http://arxiv.org/abs/1412.0291,Bits from Biology for Computational Intelligence,"  Computational intelligence is broadly defined as biologically-inspired computing. Usually, inspiration is drawn from neural systems. This article shows how to analyze neural systems using information theory to obtain constraints that help identify the algorithms run by such systems and the information they represent. Algorithms and representations identified information-theoretically may then guide the design of biologically inspired computing systems (BICS). The material covered includes the necessary introduction to information theory and the estimation of information theoretic quantities from neural data. We then show how to analyze the information encoded in a system about its environment, and also discuss recent methodological developments on the question of how much information each agent carries about the environment either uniquely, or redundantly or synergistically together with others. Last, we introduce the framework of local information dynamics, where information processing is decomposed into component processes of information storage, transfer, and modification -- locally in space and time. We close by discussing example applications of these measures to neural data and other complex systems. ","Quantitative Biology - Neurons and Cognition ; Computer Science - Information Theory ; Physics - Data Analysis, Statistics and Probability ; ","Wibral, Michael ; Lizier, Joseph T. ; Priesemann, Viola ; "
http://arxiv.org/abs/1412.0340,Approximate MAP Estimation for Pairwise Potentials via Baker's Technique,"  The theoretical models providing mathematical abstractions for several significant optimization problems in machine learning, combinatorial optimization, computer vision and statistical physics have intrinsic similarities. We propose a unified framework to model these computation tasks where the structures of these optimization problems are encoded by functions attached on the vertices and edges of a graph. We show that computing MAX 2-CSP admits polynomial-time approximation scheme (PTAS) on planar graphs, graphs with bounded local treewidth, $H$-minor-free graphs, geometric graphs with bounded density and graphs embeddable with bounded number of crossings per edge. This implies computing MAX-CUT, MAX-DICUT and MAX $k$-CUT admits PTASs on all these classes of graphs. Our method also gives the first PTAS for computing the ground state of ferromagnetic Edwards-Anderson model without external magnetic field on $d$-dimensional lattice graphs. These results are widely applicable in vision, graphics and machine learning. ",Computer Science - Data Structures and Algorithms ; ,"Wang, Yi-Kai ; "
http://arxiv.org/abs/1412.1538,Krylov Subspace Methods in Dynamical Sampling,"  Let $B$ be an unknown linear evolution process on $\mathbb C^d\simeq l^2(\mathbb Z_d)$ driving an unknown initial state $x$ and producing the states $\{B^\ell x, \ell = 0,1,\ldots\}$ at different time levels. The problem under consideration in this paper is to find as much information as possible about $B$ and $x$ from the measurements $Y=\{x(i)$, $Bx(i)$, $\dots$, $B^{\ell_i}x(i): i \in \Omega\subset \mathbb Z^d\}$. If $B$ is a ""low-pass"" convolution operator, we show that we can recover both $B$ and $x$, almost surely, as long as we double the amount of temporal samples needed in \cite{ADK13} to recover the signal propagated by a known operator $B$. For a general operator $B$, we can recover parts or even all of its spectrum from $Y$. As a special case of our method, we derive the centuries old Prony's method \cite{BDVMC08, P795, PP13} which recovers a vector with an $s$-sparse Fourier transform from $2s$ of its consecutive components. ","Computer Science - Information Theory ; 94A20, 94A12, 42C15, 15A29 ; ","Aldroubi, Akram ; Krishtal, Ilya ; "
http://arxiv.org/abs/1412.1547,Efficient algorithms to decide tightness,"  Tightness is a generalisation of the notion of convexity: a space is tight if and only if it is ""as convex as possible"", given its topological constraints. For a simplicial complex, deciding tightness has a straightforward exponential time algorithm, but efficient methods to decide tightness are only known in the trivial setting of triangulated surfaces.   In this article, we present a new polynomial time procedure to decide tightness for triangulations of $3$-manifolds -- a problem which previously was thought to be hard. Furthermore, we describe an algorithm to decide general tightness in the case of $4$-dimensional combinatorial manifolds which is fixed parameter tractable in the treewidth of the $1$-skeletons of their vertex links, and we present an algorithm to decide $\mathbb{F}_2$-tightness for weak pseudomanifolds $M$ of arbitrary but fixed dimension which is fixed parameter tractable in the treewidth of the dual graph of $M$. ",Computer Science - Computational Geometry ; Mathematics - Combinatorics ; F.2.2 ; G.2.1 ; ,"Bagchi, Bhaskar ; Burton, Benjamin A. ; Datta, Basudeb ; Singh, Nitin ; Spreer, Jonathan ; "
http://arxiv.org/abs/1412.1695,Convolutional codes from unit schemes,"  Convolutional codes are constructed, designed and analysed using row and/or block structures of unit algebraic schemes. Infinite series of such codes and of codes with specific properties are derived. Properties are shown algebraically and algebraic decoding methods are derived. For a given rate and given error-correction capability at each component, convolutional codes with these specifications and with efficient decoding algorithms are constructed. Explicit prototype examples are given but in general large lengths and large error capability are achievable. Convolutional codes with efficient decoding algorithms at or near the maximum free distances attainable for the parameters are constructible. Unit memory convolutional codes of maximum possible free distance are designed with practical algebraic decoding algorithms.   LDPC (low density parity check) convolutional codes with efficient decoding schemes are constructed and analysed by the methods. Self-dual and dual-containing convolutional codes may also be designed by the methods; dual-containing codes enables the construction of quantum codes. ","Mathematics - Rings and Algebras ; Computer Science - Discrete Mathematics ; Computer Science - Information Theory ; 94B10, 11T71, 16S99 ; ","Hurley, Ted ; "
http://arxiv.org/abs/1412.1866,Integer-Programming Ensemble of Temporal-Relations Classifiers,"  The extraction and understanding of temporal events and their relations are major challenges in natural language processing. Processing text on a sentence-by-sentence or expression-by-expression basis often fails, in part due to the challenge of capturing the global consistency of the text. We present an ensemble method, which reconciles the outputs of multiple classifiers of temporal expressions across the text using integer programming. Computational experiments show that the ensemble improves upon the best individual results from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval). ",Computer Science - Computation and Language ; Computer Science - Machine Learning ; Mathematics - Optimization and Control ; ,"Kerr, Catherine ; Hoare, Terri ; Carroll, Paula ; Marecek, Jakub ; "
http://arxiv.org/abs/1412.2114,"Chases and Escapes, and Optimization Problems","  We propose a new approach for solving combinatorial optimization problem by utilizing the mechanism of chases and escapes, which has a long history in mathematics. In addition to the well-used steepest descent and neighboring search, we perform a chase and escape game on the ""landscape"" of the cost function. We have created a concrete algorithm for the Traveling Salesman Problem. Our preliminary test indicates a possibility that this new fusion of chases and escapes problem into combinatorial optimization search is fruitful. ",Computer Science - Artificial Intelligence ; ,"Ohira, Toru ; "
http://arxiv.org/abs/1412.2231,Generalized Singular Value Thresholding,"  This work studies the Generalized Singular Value Thresholding (GSVT) operator ${\text{Prox}}_{g}^{{\sigma}}(\cdot)$, \begin{equation*}   {\text{Prox}}_{g}^{{\sigma}}(B)=\arg\min\limits_{X}\sum_{i=1}^{m}g(\sigma_{i}(X)) + \frac{1}{2}||X-B||_{F}^{2}, \end{equation*} associated with a nonconvex function $g$ defined on the singular values of $X$. We prove that GSVT can be obtained by performing the proximal operator of $g$ (denoted as $\text{Prox}_g(\cdot)$) on the singular values since $\text{Prox}_g(\cdot)$ is monotone when $g$ is lower bounded. If the nonconvex $g$ satisfies some conditions (many popular nonconvex surrogate functions, e.g., $\ell_p$-norm, $0<p<1$, of $\ell_0$-norm are special cases), a general solver to find $\text{Prox}_g(b)$ is proposed for any $b\geq0$. GSVT greatly generalizes the known Singular Value Thresholding (SVT) which is a basic subroutine in many convex low rank minimization methods. We are able to solve the nonconvex low rank minimization problem by using GSVT in place of SVT. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Lu, Canyi ; Zhu, Changbo ; Xu, Chunyan ; Yan, Shuicheng ; Lin, Zhouchen ; "
http://arxiv.org/abs/1412.2526,Exploiting Packing Components in General-Purpose Integer Programming   Solvers,"  The problem of packing boxes into a large box is often a part of a larger problem. For example in furniture supply chain applications, one needs to decide what trucks to use to transport furniture between production sites and distribution centers and stores, such that the furniture fits inside. Such problems are often formulated and sometimes solved using general-purpose integer programming solvers.   This chapter studies the problem of identifying a compact formulation of the multi-dimensional packing component in a general instance of integer linear programming, reformulating it using the discretisation of Allen--Burke--Marecek, and and solving the extended reformulation. Results on instances of up to 10000000 boxes are reported. ",Mathematics - Optimization and Control ; Computer Science - Data Structures and Algorithms ; ,"Marecek, Jakub ; "
http://arxiv.org/abs/1412.2684,HyperSpectral classification with adaptively weighted L1-norm   regularization and spatial postprocessing,"  Sparse regression methods have been proven effective in a wide range of signal processing problems such as image compression, speech coding, channel equalization, linear regression and classification. In this paper a new convex method of hyperspectral image classification is developed based on the sparse unmixing algorithm SUnSAL for which a pixel adaptive L1-norm regularization term is introduced. To further enhance class separability, the algorithm is kernelized using an RBF kernel and the final results are improved by a combination of spatial pre and post-processing operations. It is shown that the proposed method is competitive with state of the art algorithms such as SVM-CK, KSOMP-CK and KSSP-CK. ",Mathematics - Optimization and Control ; Computer Science - Computer Vision and Pattern Recognition ; ,"Aldea, Victor Stefan ; "
http://arxiv.org/abs/1412.2877,Autonomous Load Disaggregation Approach based on Active Power   Measurements,"  With the help of smart metering valuable information of the appliance usage can be retrieved. In detail, non-intrusive load monitoring (NILM), also called load disaggregation, tries to identify appliances in the power draw of an household. In this paper an unsupervised load disaggregation approach is proposed that works without a priori knowledge about appliances. The proposed algorithm works autonomously in real time. The number of used appliances and the corresponding appliance models are learned in operation and are progressively updated. The proposed algorithm is considering each useful and suitable detected power state. The algorithm tries to detect power states corresponding to on/off appliances as well as to multi-state appliances based on active power measurements in 1s resolution. We evaluated the novel introduced load disaggregation approach on real world data by testing the possibility to disaggregate energy demand on appliance level. ",Computer Science - Other Computer Science ; ,"Egarter, Dominik ; Elmenreich, Wilfried ; "
http://arxiv.org/abs/1412.3347,Computational Aspects of the Colorful Carath\'eodory Theorem,"  Let $C_1,\dots,C_{d+1}\subset \mathbb{R}^d$ be $d+1$ point sets, each containing the origin in its convex hull. We call these sets color classes, and we call a sequence $p_1, \dots, p_{d+1}$ with $p_i \in C_i$, for $i = 1, \dots, d+1$, a colorful choice. The colorful Carath\'eodory theorem guarantees the existence of a colorful choice that also contains the origin in its convex hull. The computational complexity of finding such a colorful choice (CCP) is unknown. This is particularly interesting in the light of polynomial-time reductions from several related problems, such as computing centerpoints, to CCP.   We define a novel notion of approximation that is compatible with the polynomial-time reductions to CCP: a sequence that contains at most $k$ points from each color class is called a $k$-colorful choice. We present an algorithm that for any fixed $\varepsilon > 0$, outputs an $\lceil \epsilon d\rceil$-colorful choice containing the origin in its convex hull in polynomial time.   Furthermore, we consider a related problem of CCP: in the nearest colorful polytope problem (NCP), we are given sets $C_1,\dots,C_n\subset\mathbb{R}^d$ that do not necessarily contain the origin in their convex hulls. The goal is to find a colorful choice whose convex hull minimizes the distance to the origin. We show that computing a local optimum for NCP is PLS-complete, while computing a global optimum is NP-hard. ",Computer Science - Computational Geometry ; ,"Mulzer, Wolfgang ; Stein, Yannik ; "
http://arxiv.org/abs/1412.3701,How Much Lookahead is Needed to Win Infinite Games?,"  Delay games are two-player games of infinite duration in which one player may delay her moves to obtain a lookahead on her opponent's moves. For $\omega$-regular winning conditions it is known that such games can be solved in doubly-exponential time and that doubly-exponential lookahead is sufficient.   We improve upon both results by giving an exponential time algorithm and an exponential upper bound on the necessary lookahead. This is complemented by showing EXPTIME-hardness of the solution problem and tight exponential lower bounds on the lookahead. Both lower bounds already hold for safety conditions. Furthermore, solving delay games with reachability conditions is shown to be PSPACE-complete.   This is a corrected version of the paper https://arxiv.org/abs/1412.3701v4 published originally on August 26, 2016. ",Computer Science - Computer Science and Game Theory ; Computer Science - Formal Languages and Automata Theory ; ,"Klein, Felix ; Zimmermann, Martin ; "
http://arxiv.org/abs/1412.4171,Dynamics of Information Diffusion and Social Sensing,"  Statistical inference using social sensors is an area that has witnessed remarkable progress and is relevant in applications including localizing events for targeted advertising, marketing, localization of natural disasters and predicting sentiment of investors in financial markets. This chapter presents a tutorial description of four important aspects of sensing-based information diffusion in social networks from a communications/signal processing perspective. First, diffusion models for information exchange in large scale social networks together with social sensing via social media networks such as Twitter is considered. Second, Bayesian social learning models and risk averse social learning is considered with applications in finance and online reputation systems. Third, the principle of revealed preferences arising in micro-economics theory is used to parse datasets to determine if social sensors are utility maximizers and then determine their utility functions. Finally, the interaction of social sensors with YouTube channel owners is studied using time series analysis methods. All four topics are explained in the context of actual experimental datasets from health networks, social media and psychological experiments. Also, algorithms are given that exploit the above models to infer underlying events based on social sensing. The overview, insights, models and algorithms presented in this chapter stem from recent developments in network science, economics and signal processing. At a deeper level, this chapter considers mean field dynamics of networks, risk averse Bayesian social learning filtering and quickest change detection, data incest in decision making over a directed acyclic graph of social sensors, inverse optimization problems for utility function estimation (revealed preferences) and statistical modeling of interacting social sensors in YouTube social networks. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Krishnamurthy, Vikram ; Hoiles, William ; "
http://arxiv.org/abs/1412.4198,An Ordinal Minimax Theorem,"  In the early 1950s Lloyd Shapley proposed an ordinal and set-valued solution concept for zero-sum games called \emph{weak saddle}. We show that all weak saddles of a given zero-sum game are interchangeable and equivalent. As a consequence, every such game possesses a unique set-based value. ",Computer Science - Computer Science and Game Theory ; ,"Brandt, Felix ; Brill, Markus ; Suksompong, Warut ; "
http://arxiv.org/abs/1412.4586,Generalized Vietoris Bisimulations,"  We introduce and study bisimulations for coalgebras on Stone spaces [14]. Our notion of bisimulation is sound and complete for behavioural equivalence, and generalizes Vietoris bisimulations [4]. The main result of our paper is that bisimulation for a $\mathbf{Stone}$ coalgebra is the topological closure of bisimulation for the underlying $\mathbf{Set}$ coalgebra. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; ,"Enqvist, Sebastian ; Sourabh, Sumit ; "
http://arxiv.org/abs/1412.5204,How many queries are needed to distinguish a truncated random   permutation from a random function?,"  An oracle chooses a function $f$ from the set of $n$ bits strings to itself, which is either a randomly chosen permutation or a randomly chosen function. When queried by an $n$-bit string $w$, the oracle computes $f(w)$, truncates the $m$ last bits, and returns only the first $n-m$ bits of $f(w)$. How many queries does a querying adversary need to submit in order to distinguish the truncated permutation from the (truncated) function?   In 1998, Hall et al. showed an algorithm for determining (with high probability) whether or not $f$ is a permutation, using $O(2^{\frac{m+n}{2}})$ queries. They also showed that if $m < n/7$, a smaller number of queries will not suffice. For $m > n/7$, their method gives a weaker bound. In this note, we first show how a modification of the approximation method used by Hall et al. can solve the problem completely. It extends the result to practically any $m$, showing that $\Omega(2^{\frac{m+n}{2}})$ queries are needed to get a non-negligible distinguishing advantage. However, more surprisingly, a better bound for the distinguishing advantage can be obtained from a result of Stam published, in a different context, already in 1978. We also show that, at least in some cases, Stam's bound is tight. ",Computer Science - Cryptography and Security ; ,"Gilboa, Shoni ; Gueron, Shay ; Morris, Ben ; "
http://arxiv.org/abs/1412.5374,Maximal Correlation Secrecy,"  This paper shows that the Hirschfeld-Gebelein-R\'enyi maximal correlation between the message and the ciphertext provides good secrecy guarantees for cryptosystems that use short keys. We first establish a bound on the eavesdropper's advantage in guessing functions of the message in terms of maximal correlation and the R\'enyi entropy of the message. This result implies that maximal correlation is stronger than the notion of entropic security introduced by Russell and Wang. We then show that a small maximal correlation $\rho$ can be achieved via a randomly generated cipher with key length $\approx2\log(1/\rho)$, independent of the message length, and by a stream cipher with key length $2\log(1/\rho)+\log n+2$ for a message of length $n$. We establish a converse showing that these ciphers are close to optimal. This is in contrast to entropic security for which there is a gap between the lower and upper bounds. Finally, we show that a small maximal correlation implies secrecy with respect to several mutual information based criteria but is not necessarily implied by them. Hence, maximal correlation is a stronger and more practically relevant measure of secrecy than mutual information. ",Computer Science - Information Theory ; Computer Science - Cryptography and Security ; ,"Li, Cheuk Ting ; Gamal, Abbas El ; "
http://arxiv.org/abs/1412.5466,Enumerative Coding for Line Polar Grassmannians with applications to   codes,"  A $k$-polar Grassmannian is the geometry having as pointset the set of all $k$-dimensional subspaces of a vector space $V$ which are totally isotropic for a given non-degenerate bilinear form $\mu$ defined on $V.$ Hence it can be regarded as a subgeometry of the ordinary $k$-Grassmannian. In this paper we deal with orthogonal line Grassmannians and with symplectic line Grassmannians, i.e. we assume $k=2$ and $\mu$ a non-degenerate symmetric or alternating form. We will provide a method to efficiently enumerate the pointsets of both orthogonal and symplectic line Grassmannians. This has several nice applications; among them, we shall discuss an efficient encoding/decoding/error correction strategy for line polar Grassmann codes of both types. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; 14M15, 94B27, 94B05 ; ","Cardinali, Ilaria ; Giuzzi, Luca ; "
http://arxiv.org/abs/1412.5669,The Timestamp of Timed Automata,"  Given a member A of the class of non-deterministic timed automata with silent transitions (eNTA), we show how one can effectively compute its timestamp: the set of all pairs of time values and the corresponding actions of all observable timed traces of A, and also a deterministic timed automaton with the same timestamp as that of A. The timestamp is eventually periodic and is constructed via a finite periodic augmented region automaton. A consequence of this construction is the periodicity of the language of timed automata with respect to suffixes. Applications include the decidability of the 1-bounded language inclusion problem for the class eNTA, and a partial method, not bounded by time or number of steps, for the general language non-inclusion problem for eNTA. ",Computer Science - Formal Languages and Automata Theory ; F.1.1 ; D.2.4 ; ,"Rosenmann, Amnon ; "
http://arxiv.org/abs/1412.5718,Revisiting Non-Progressive Influence Models: Scalable Influence   Maximization,"  While influence maximization in social networks has been studied extensively in computer science community for the last decade the focus has been on the progressive influence models, such as independent cascade (IC) and Linear threshold (LT) models, which cannot capture the reversibility of choices. In this paper, we present the Heat Conduction (HC) model which is a non-progressive influence model with real-world interpretations. We show that HC unifies, generalizes, and extends the existing nonprogressive models, such as the Voter model [1] and non-progressive LT [2]. We then prove that selecting the optimal seed set of influential nodes is NP-hard for HC but by establishing the submodularity of influence spread, we can tackle the influence maximization problem with a scalable and provably near-optimal greedy algorithm. We are the first to present a scalable solution for influence maximization under nonprogressive LT model, as a special case of the HC model. In sharp contrast to the other greedy influence maximization methods, our fast and efficient C2GREEDY algorithm benefits from two analytically computable steps: closed-form computation for finding the influence spread as well as the greedy seed selection. Through extensive experiments on several large real and synthetic networks, we show that C2GREEDY outperforms the state-of-the-art methods, in terms of both influence spread and scalability. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Golnari, Golshan ; Asiaee, Amir ; Banerjee, Arindam ; Zhang, Zhi-Li ; "
http://arxiv.org/abs/1412.5831,Deducing Truth from Correlation,"  This work is motivated by a question at the heart of unsupervised learning approaches: Assume we are collecting a number K of (subjective) opinions about some event E from K different agents. Can we infer E from them? Prima facie this seems impossible, since the agents may be lying. We model this task by letting the events be distributed according to some distribution p and the task is to estimate p under unknown noise. Again, this is impossible without additional assumptions. We report here the finding of very natural such assumptions - the availability of multiple copies of the true data, each under independent and invertible (in the sense of matrices) noise, is already sufficient: If the true distribution and the observations are modeled on the same finite alphabet, then the number of such copies needed to determine p to the highest possible precision is exactly three! This result can be seen as a counterpart to independent component analysis. Therefore, we call our approach 'dependent component analysis'. In addition, we present generalizations of the model to different alphabet sizes at in- and output. A second result is found: the 'activation' of invertibility through multiple parallel uses. ",Computer Science - Information Theory ; ,"Nötzel, Janis ; Swetly, Walter ; "
http://arxiv.org/abs/1412.5902,"Nearest Descent, In-Tree, and Clustering","  In this paper, we propose a physically inspired graph-theoretical clustering method, which first makes the data points organized into an attractive graph, called In-Tree, via a physically inspired rule, called Nearest Descent (ND). In particular, the rule of ND works to select the nearest node in the descending direction of potential as the parent node of each node, which is in essence different from the classical Gradient Descent or Steepest Descent. The constructed In-Tree proves a very good candidate for clustering due to its particular features and properties. In the In-Tree, the original clustering problem is reduced to a problem of removing a very few of undesired edges from this graph. Pleasingly, the undesired edges in In-Tree are so distinguishable that they can be easily determined in either automatic or interactive way, which is in stark contrast to the cases in the widely used Minimal Spanning Tree and k-nearest-neighbor graph. The cluster number in the proposed method can be easily determined based on some intermediate plots, and the cluster assignment for each node is easily made by quickly searching its root node in each sub-graph (also an In-Tree). The proposed method is extensively evaluated on both synthetic and real-world datasets. Overall, the proposed clustering method is a density-based one, but shows significant differences and advantages in comparison to the traditional ones. The proposed method is simple yet efficient and reliable, and is applicable to various datasets with diverse shapes, attributes and any high dimensionality ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; ,"Qiu, Teng ; Yang, Kaifu ; Li, Chaoyi ; Li, Yongjie ; "
http://arxiv.org/abs/1412.6466,Finding 2-Edge and 2-Vertex Strongly Connected Components in Quadratic   Time,"  We present faster algorithms for computing the 2-edge and 2-vertex strongly connected components of a directed graph, which are straightforward generalizations of strongly connected components. While in undirected graphs the 2-edge and 2-vertex connected components can be found in linear time, in directed graphs only rather simple $O(m n)$-time algorithms were known. We use a hierarchical sparsification technique to obtain algorithms that run in time $O(n^2)$. For 2-edge strongly connected components our algorithm gives the first running time improvement in 20 years. Additionally we present an $O(m^2 / \log{n})$-time algorithm for 2-edge strongly connected components, and thus improve over the $O(m n)$ running time also when $m = O(n)$. Our approach extends to k-edge and k-vertex strongly connected components for any constant k with a running time of $O(n^2 \log^2 n)$ for edges and $O(n^3)$ for vertices. ",Computer Science - Data Structures and Algorithms ; ,"Henzinger, Monika ; Krinninger, Sebastian ; Loitzenbauer, Veronika ; "
http://arxiv.org/abs/1412.6622,Deep metric learning using Triplet network,"  Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Statistics - Machine Learning ; ,"Hoffer, Elad ; Ailon, Nir ; "
http://arxiv.org/abs/1412.6761,New results on classical and quantum counter automata,"  We show that one-way quantum one-counter automaton with zero-error is more powerful than its probabilistic counterpart on promise problems. Then, we obtain a similar separation result between Las Vegas one-way probabilistic one-counter automaton and one-way deterministic one-counter automaton.   We also obtain new results on classical counter automata regarding language recognition. It was conjectured that one-way probabilistic one blind-counter automata cannot recognize Kleene closure of equality language [A. Yakaryilmaz: Superiority of one-way and realtime quantum machines. RAIRO - Theor. Inf. and Applic. 46(4): 615-641 (2012)]. We show that this conjecture is false, and also show several separation results for blind/non-blind counter automata. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Quantum Physics ; ,"Nakanishi, Masaki ; Yakaryılmaz, Abuzer ; Gainutdinova, Aida ; "
http://arxiv.org/abs/1412.6808,Learning the nonlinear geometry of high-dimensional data: Models and   algorithms,"  Modern information processing relies on the axiom that high-dimensional data lie near low-dimensional geometric structures. This paper revisits the problem of data-driven learning of these geometric structures and puts forth two new nonlinear geometric models for data describing ""related"" objects/phenomena. The first one of these models straddles the two extremes of the subspace model and the union-of-subspaces model, and is termed the metric-constrained union-of-subspaces (MC-UoS) model. The second one of these models---suited for data drawn from a mixture of nonlinear manifolds---generalizes the kernel subspace model, and is termed the metric-constrained kernel union-of-subspaces (MC-KUoS) model. The main contributions of this paper in this regard include the following. First, it motivates and formalizes the problems of MC-UoS and MC-KUoS learning. Second, it presents algorithms that efficiently learn an MC-UoS or an MC-KUoS underlying data of interest. Third, it extends these algorithms to the case when parts of the data are missing. Last, but not least, it reports the outcomes of a series of numerical experiments involving both synthetic and real data that demonstrate the superiority of the proposed geometric models and learning algorithms over existing approaches in the literature. These experiments also help clarify the connections between this work and the literature on (subspace and kernel k-means) clustering. ",Statistics - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; ,"Wu, Tong ; Bajwa, Waheed U. ; "
http://arxiv.org/abs/1412.7172,Rational Groupthink,"  We study how long-lived rational agents learn from repeatedly observing each others' actions. We find that in the long run, information aggregation fails, and the fraction of private information transmitted goes to zero as the number of agents gets large. With Normal signals, in the long-run, agents learn less from observing the actions of any number of other agents than they learn from seeing three other agents' signals. We identify rational groupthink---in which agents ignore their private signals and choose the same action for long periods of time---as the cause of this failure of information aggregation. ",Computer Science - Computer Science and Game Theory ; Economics - Theoretical Economics ; Mathematics - Probability ; ,"Harel, Matan ; Mossel, Elchanan ; Strack, Philipp ; Tamuz, Omer ; "
http://arxiv.org/abs/1412.7646,Sub-linear Time Support Recovery for Compressed Sensing using   Sparse-Graph Codes,"  We study the support recovery problem for compressed sensing, where the goal is to reconstruct the a high-dimensional $K$-sparse signal $\mathbf{x}\in\mathbb{R}^N$, from low-dimensional linear measurements with and without noise. Our key contribution is a new compressed sensing framework through a new family of carefully designed sparse measurement matrices associated with minimal measurement costs and a low-complexity recovery algorithm. The measurement matrix in our framework is designed based on the well-crafted sparsification through capacity-approaching sparse-graph codes, where the sparse coefficients can be recovered efficiently in a few iterations by performing simple error decoding over the observations. We formally connect this general recovery problem with sparse-graph decoding in packet communication systems, and analyze our framework in terms of the measurement cost, time complexity and recovery performance. In the noiseless setting, our framework can recover any arbitrary $K$-sparse signal in $O(K)$ time using $2K$ measurements asymptotically with high probability. In the noisy setting, when the sparse coefficients take values in a finite and quantized alphabet, our framework can achieve the same goal in time $O(K\log(N/K))$ using $O(K\log(N/K))$ measurements obtained from measurement matrix with elements $\{-1,0,1\}$. When the sparsity $K$ is sub-linear in the signal dimension $K=O(N^\delta)$ for some $0<\delta<1$, our results are order-optimal in terms of measurement costs and run-time, both of which are sub-linear in the signal dimension $N$. The sub-linear measurement cost and run-time can also be achieved with continuous-valued sparse coefficients, with a slight increment in the logarithmic factors. This offers the desired scalability of our framework that can potentially enable real-time or near-real-time processing for massive datasets featuring sparsity. ",Computer Science - Information Theory ; ,"Li, Xiao ; Yin, Dong ; Pawar, Sameer ; Pedarsani, Ramtin ; Ramchandran, Kannan ; "
http://arxiv.org/abs/1412.7725,Automatic Photo Adjustment Using Deep Neural Networks,"  Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Graphics ; Computer Science - Machine Learning ; Electrical Engineering and Systems Science - Image and Video Processing ; ,"Yan, Zhicheng ; Zhang, Hao ; Wang, Baoyuan ; Paris, Sylvain ; Yu, Yizhou ; "
http://arxiv.org/abs/1412.7839,"Cloud K-SVD: A Collaborative Dictionary Learning Algorithm for Big,   Distributed Data","  This paper studies the problem of data-adaptive representations for big, distributed data. It is assumed that a number of geographically-distributed, interconnected sites have massive local data and they are interested in collaboratively learning a low-dimensional geometric structure underlying these data. In contrast to previous works on subspace-based data representations, this paper focuses on the geometric structure of a union of subspaces (UoS). In this regard, it proposes a distributed algorithm---termed cloud K-SVD---for collaborative learning of a UoS structure underlying distributed data of interest. The goal of cloud K-SVD is to learn a common overcomplete dictionary at each individual site such that every sample in the distributed data can be represented through a small number of atoms of the learned dictionary. Cloud K-SVD accomplishes this goal without requiring exchange of individual samples between sites. This makes it suitable for applications where sharing of raw data is discouraged due to either privacy concerns or large volumes of data. This paper also provides an analysis of cloud K-SVD that gives insights into its properties as well as deviations of the dictionaries learned at individual sites from a centralized solution in terms of different measures of local/global data and topology of interconnections. Finally, the paper numerically illustrates the efficacy of cloud K-SVD on real and synthetic distributed data. ",Computer Science - Machine Learning ; Computer Science - Information Theory ; Statistics - Machine Learning ; ,"Raja, Haroon ; Bajwa, Waheed U. ; "
http://arxiv.org/abs/1412.7998,Propositional Logics of Dependence,"  In this paper, we study logics of dependence on the propositional level. We prove that several interesting propositional logics of dependence, including propositional dependence logic, propositional intuitionistic dependence logic as well as propositional inquisitive logic, are expressively complete and have disjunctive or conjunctive normal forms. We provide deduction systems and prove the completeness theorems for these logics. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03B60 ; ,"Yang, Fan ; Väänänen, Jouko ; "
http://arxiv.org/abs/1412.8324,A Constructive Proof on the Compositionality of Linearizability,"  Linearizability is the strongest correctness property for both shared memory and message passing systems. One of its useful features is the compositionality: a history (execution) is linearizable if and only if each object (component) subhistory is linearizable. In this paper, we propose a new hierarchical system model to address challenges in modular development of cloud systems. Object are defined by induction from the most fundamental atomic Boolean registers, and histories are represented as countable well-ordered structures of events to deal with both finite and infinite executions. Then, we present a new constructive proof on the compositionality theorem of linearizability inspired by Multiway Merge. This proof deduces a theoretically efficient algorithm which generates linearization in O(N*logP) running time with O(N) space, where P and N are process/event numbers respectively. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Lin, Haoxiang ; "
http://arxiv.org/abs/1412.8358,Odd graph and its applications to the strong edge coloring,"  A strong edge coloring of a graph is a proper edge coloring in which every color class is an induced matching. The strong chromatic index $\chi_s'(G)$ of a graph $G$ is the minimum number of colors in a strong edge coloring of $G$. Let $\Delta \geq 4$ be an integer. In this note, we study the odd graphs and show the existence of some special walks. By using these results and Chang's ideas in [Discuss. Math. Graph Theory 34 (4) (2014) 723--733], we show that every planar graph with maximum degree at most $\Delta$ and girth at least $10 \Delta - 4$ has a strong edge coloring with $2\Delta - 1$ colors. In addition, we prove that if $G$ is a graph with girth at least $2\Delta - 1$ and mad$(G) < 2 + \frac{1}{3\Delta - 2}$, where $\Delta$ is the maximum degree and $\Delta \geq 4$, then $\chi_s'(G) \leq 2\Delta - 1$, if $G$ is a subcubic graph with girth at least $8$ and mad$(G) < 2 + \frac{2}{23}$, then $\chi_s'(G) \leq 5$. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Wang, Tao ; Zhao, Xiaodan ; "
http://arxiv.org/abs/1412.8591,Maze Solving Automatons for Self-Healing of Open Interconnects: Modular   Add-on for Circuit Boards,  We present the circuit board integration of a self-healing mechanism to repair open faults. The electric field driven mechanism physically restores fractured interconnects in electronic circuits and has the ability to solve mazes. The repair is performed by conductive particles dispersed in an insulating fluid. We demonstrate the integration of the healing module onto printed circuit boards and the ability of maze solving. We model and perform experiments on the influence of the geometry of the conductive particles as well as the terminal impedances of the route on the healing efficiency. The typical heal rate is 10 $\mu$m/s with healed route having resistance of 100 $\Omega$ to 20 k$\Omega$ depending on the materials and concentrations used. ,Computer Science - Emerging Technologies ; Condensed Matter - Soft Condensed Matter ; ,"Nair, Aswathi ; Raghunandan, Karthik ; Yaswanth, Vaddi ; Shridharan, Sreelal ; Sambandan, Sanjiv ; "
http://arxiv.org/abs/1501.00433,On the Uniform Computational Content of Computability Theory,"  We demonstrate that the Weihrauch lattice can be used to classify the uniform computational content of computability-theoretic properties as well as the computational content of theorems in one common setting. The properties that we study include diagonal non-computability, hyperimmunity, complete consistent extensions of Peano arithmetic, 1-genericity, Martin-L\""of randomness, and cohesiveness. The theorems that we include in our case study are the low basis theorem of Jockusch and Soare, the Kleene-Post theorem, and Friedberg's jump inversion theorem. It turns out that all the aforementioned properties and many theorems in computability theory, including all theorems that claim the existence of some Turing degree, have very little uniform computational content: they are located outside of the upper cone of binary choice (also known as LLPO); we call problems with this property indiscriminative. Since practically all theorems from classical analysis whose computational content has been classified are discriminative, our observation could yield an explanation for why theorems and results in computability theory typically have very few direct consequences in other disciplines such as analysis. A notable exception in our case study is the low basis theorem which is discriminative. This is perhaps why it is considered to be one of the most applicable theorems in computability theory. In some cases a bridge between the indiscriminative world and the discriminative world of classical mathematics can be established via a suitable residual operation and we demonstrate this in the case of the cohesiveness problem and the problem of consistent complete extensions of Peano arithmetic. Both turn out to be the quotient of two discriminative problems. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Brattka, Vasco ; Hendtlass, Matthew ; Kreuzer, Alexander P. ; "
http://arxiv.org/abs/1501.00960,Characterizing the Google Books corpus: Strong limits to inferences of   socio-cultural and linguistic evolution,"  It is tempting to treat frequency trends from the Google Books data sets as indicators of the ""true"" popularity of various words and phrases. Doing so allows us to draw quantitatively strong conclusions about the evolution of cultural perception of a given topic, such as time or gender. However, the Google Books corpus suffers from a number of limitations which make it an obscure mask of cultural popularity. A primary issue is that the corpus is in effect a library, containing one of each book. A single, prolific author is thereby able to noticeably insert new phrases into the Google Books lexicon, whether the author is widely read or not. With this understood, the Google Books corpus remains an important data set to be considered more lexicon-like than text-like. Here, we show that a distinct problematic feature arises from the inclusion of scientific texts, which have become an increasingly substantive portion of the corpus throughout the 1900s. The result is a surge of phrases typical to academic articles but less common in general, such as references to time in the form of citations. We highlight these dynamics by examining and comparing major contributions to the statistical divergence of English data sets between decades in the period 1800--2000. We find that only the English Fiction data set from the second version of the corpus is not heavily affected by professional texts, in clear contrast to the first version of the fiction data set and both unfiltered English data sets. Our findings emphasize the need to fully characterize the dynamics of the Google Books corpus before using these data sets to draw broad conclusions about cultural and linguistic evolution. ",Physics - Physics and Society ; Condensed Matter - Statistical Mechanics ; Computer Science - Computation and Language ; Statistics - Applications ; ,"Pechenick, Eitan Adam ; Danforth, Christopher M. ; Dodds, Peter Sheridan ; "
http://arxiv.org/abs/1501.01042,Augur: a decentralized oracle and prediction market platform,"  Augur is a trustless, decentralized oracle and platform for prediction markets. The outcomes of Augur's prediction markets are chosen by users that hold Augur's native Reputation token, who stake their tokens on the actual observed outcome and, in return, receive settlement fees from the markets. Augur's incentive structure is designed to ensure that honest, accurate reporting of outcomes is always the most profitable option for Reputation token holders. Token holders can post progressively-larger Reputation bonds to dispute proposed market outcomes. If the size of these bonds reaches a certain threshold, Reputation splits into multiple versions, one for each possible outcome of the disputed market; token holders must then exchange their Reputation tokens for one of these versions. Versions of Reputation which do not correspond to the real-world outcome will become worthless, as no one will participate in prediction markets unless they are confident that the markets will resolve correctly. Therefore, token holders will select the only version of Reputation which they know will continue to have value: the version that corresponds to reality. ",Computer Science - Cryptography and Security ; ,"Peterson, Jack ; Krug, Joseph ; Zoltu, Micah ; Williams, Austin K. ; Alexander, Stephanie ; "
http://arxiv.org/abs/1501.02741,Salient Object Detection: A Benchmark,"  We extensively compare, qualitatively and quantitatively, 40 state-of-the-art models (28 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over 6 challenging datasets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted just two years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for state-of-the-art models, provide useful hints towards constructing more challenging large scale datasets and better saliency models. Finally, we propose probable solutions for tackling several open problems such as evaluation scores and dataset bias, which also suggest future research directions in the rapidly-growing field of salient object detection. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Borji, Ali ; Cheng, Ming-Ming ; Jiang, Huaizu ; Li, Jia ; "
http://arxiv.org/abs/1501.03043,Functionals and hardware,"  Functionals are an important research subject in Mathematics and Computer Science as well as a challenge in Information Technologies where the current programming paradigm states that only symbolic computations are possible on higher order objects, i.e. functionals are terms, and computation is term rewriting. The idea explored in the paper is that functionals correspond to generic mechanisms for management of connections in arrays consisting of first order functional units. Functionals are higher order abstractions that are useful for the management of such large arrays. Computations on higher order objects comprise dynamic configuration of connections between first order elementary functions in the arrays. Once the functionals are considered as the generic mechanisms, they have a grounding in hardware. A conceptual framework for constructing such mechanisms is presented, and their hardware realization is discussed. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D ; F.4.1 ; ,"Ambroszkiewicz, Stanislaw ; "
http://arxiv.org/abs/1501.03347,Dirichlet Process Parsimonious Mixtures for clustering,"  The parsimonious Gaussian mixture models, which exploit an eigenvalue decomposition of the group covariance matrices of the Gaussian mixture, have shown their success in particular in cluster analysis. Their estimation is in general performed by maximum likelihood estimation and has also been considered from a parametric Bayesian prospective. We propose new Dirichlet Process Parsimonious mixtures (DPPM) which represent a Bayesian nonparametric formulation of these parsimonious Gaussian mixture models. The proposed DPPM models are Bayesian nonparametric parsimonious mixture models that allow to simultaneously infer the model parameters, the optimal number of mixture components and the optimal parsimonious mixture structure from the data. We develop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of the developed DPMM models and provide a Bayesian model selection framework by using Bayes factors. We apply them to cluster simulated data and real data sets, and compare them to the standard parsimonious mixture models. The obtained results highlight the effectiveness of the proposed nonparametric parsimonious mixture models as a good nonparametric alternative for the parametric parsimonious models. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Statistics - Methodology ; ,"Chamroukhi, Faicel ; Bartcus, Marius ; Glotin, Hervé ; "
http://arxiv.org/abs/1501.03872,The Dead Cryptographers Society Problem,"  This paper defines The Dead Cryptographers Society Problem - DCS (where several great cryptographers created many polynomial-time Deterministic Turing Machines (DTMs) of a specific type, ran them on their proper descriptions concatenated with some arbitrary strings, deleted them and left only the results from those running, after they died: if those DTMs only permute and sometimes invert the bits on input, is it possible to decide the language formed by such resulting strings within polynomial time?), proves some facts about its computational complexity, and discusses some possible uses on Cryptography, such as into distance keys distribution, online reverse auction and secure communication. ","Computer Science - Computational Complexity ; Computer Science - Cryptography and Security ; 94A60 (Primary), 94A62 (Secondary) ; ","Barbosa, André Luiz ; "
http://arxiv.org/abs/1501.04147,Categorified Reeb Graphs,"  The Reeb graph is a construction which originated in Morse theory to study a real valued function defined on a topological space. More recently, it has been used in various applications to study noisy data which creates a desire to define a measure of similarity between these structures. Here, we exploit the fact that the category of Reeb graphs is equivalent to the category of a particular class of cosheaf. Using this equivalency, we can define an `interleaving' distance between Reeb graphs which is stable under the perturbation of a function. Along the way, we obtain a natural construction for smoothing a Reeb graph to reduce its topological complexity. The smoothed Reeb graph can be constructed in polynomial time. ",Computer Science - Computational Geometry ; ,"de Silva, Vin ; Munch, Elizabeth ; Patel, Amit ; "
http://arxiv.org/abs/1501.04318,Clustering based on the In-tree Graph Structure and Affinity Propagation,"  A recently proposed clustering method, called the Nearest Descent (ND), can organize the whole dataset into a sparsely connected graph, called the In-tree. This ND-based Intree structure proves able to reveal the clustering structure underlying the dataset, except one imperfect place, that is, there are some undesired edges in this In-tree which require to be removed. Here, we propose an effective way to automatically remove the undesired edges in In-tree via an effective combination of the In-tree structure with affinity propagation (AP). The key for the combination is to add edges between the reachable nodes in In-tree before using AP to remove the undesired edges. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed method. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Statistics - Machine Learning ; ,"Qiu, Teng ; Li, Yongjie ; "
http://arxiv.org/abs/1501.04343,Algorithms for Scheduling Malleable Cloud Tasks,"  Due to the ubiquity of batch data processing in cloud computing, the related problem of scheduling malleable batch tasks and its extensions have received significant attention recently. In this paper, we consider a fundamental model where a set of n tasks is to be processed on C identical machines and each task is specified by a value, a workload, a deadline and a parallelism bound. Within the parallelism bound, the number of machines assigned to a task can vary over time without affecting its workload. For this model, we obtain two core results: a sufficient and necessary condition such that a set of tasks can be finished by their deadlines on C machines, and an algorithm to produce such a schedule. These core results provide a conceptual tool and an optimal scheduling algorithm that enable proposing new algorithmic analysis and design and improving existing algorithms under various objectives. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; ","Wu, Xiaohu ; Loiseau, Patrick ; "
http://arxiv.org/abs/1501.04706,A Novel Implementation of QuickHull Algorithm on the GPU,"  We present a novel GPU-accelerated implementation of the QuickHull algorihtm for calculating convex hulls of planar point sets. We also describe a practical solution to demonstrate how to efficiently implement a typical Divide-and-Conquer algorithm on the GPU. We highly utilize the parallel primitives provided by the library Thrust such as the parallel segmented scan for better efficiency and simplicity. To evaluate the performance of our implementation, we carry out four groups of experimental tests using two groups of point sets in two modes on the GPU K20c. Experimental results indicate that: our implementation can achieve the speedups of up to 10.98x over the state-of-art CPU-based convex hull implementation Qhull [16]. In addition, our implementation can find the convex hull of 20M points in about 0.2 seconds. ",Computer Science - Computational Geometry ; Computer Science - Graphics ; ,"Zhang, Jiayin ; Mei, Gang ; Xu, Nengxiong ; Zhao, Kunyang ; "
http://arxiv.org/abs/1501.04836,Subtropical Real Root Finding,"  We describe a new incomplete but terminating method for real root finding for large multivariate polynomials. We take an abstract view of the polynomial as the set of exponent vectors associated with sign information on the coefficients. Then we employ linear programming to heuristically find roots. There is a specialized variant for roots with exclusively positive coordinates, which is of considerable interest for applications in chemistry and systems biology. An implementation of our method combining the computer algebra system Reduce with the linear programming solver Gurobi has been successfully applied to input data originating from established mathematical models used in these areas. We have solved several hundred problems with up to more than 800000 monomials in up to 10 variables with degrees up to 12. Our method has failed due to its incompleteness in less than 8 percent of the cases. ",Computer Science - Symbolic Computation ; ,"Sturm, Thomas ; "
http://arxiv.org/abs/1501.05098,Better Answers to Real Questions,"  We consider existential problems over the reals. Extended quantifier elimination generalizes the concept of regular quantifier elimination by providing in addition answers, which are descriptions of possible assignments for the quantified variables. Implementations of extended quantifier elimination via virtual substitution have been successfully applied to various problems in science and engineering. So far, the answers produced by these implementations included infinitesimal and infinite numbers, which are hard to interpret in practice. We introduce here a post-processing procedure to convert, for fixed parameters, all answers into standard real numbers. The relevance of our procedure is demonstrated by application of our implementation to various examples from the literature, where it significantly improves the quality of the results. ",Computer Science - Symbolic Computation ; Computer Science - Logic in Computer Science ; ,"Kosta, Marek ; Sturm, Thomas ; Dolzmann, Andreas ; "
http://arxiv.org/abs/1501.05151,Recursive Bayesian Filtering in Circular State Spaces,"  For recursive circular filtering based on circular statistics, we introduce a general framework for estimation of a circular state based on different circular distributions, specifically the wrapped normal distribution and the von Mises distribution. We propose an estimation method for circular systems with nonlinear system and measurement functions. This is achieved by relying on efficient deterministic sampling techniques. Furthermore, we show how the calculations can be simplified in a variety of important special cases, such as systems with additive noise as well as identity system or measurement functions. We introduce several novel key components, particularly a distribution-free prediction algorithm, a new and superior formula for the multiplication of wrapped normal densities, and the ability to deal with non-additive system noise. All proposed methods are thoroughly evaluated and compared to several state-of-the-art solutions. ",Computer Science - Systems and Control ; Computer Science - Robotics ; ,"Kurz, Gerhard ; Gilitschenski, Igor ; Hanebeck, Uwe D. ; "
http://arxiv.org/abs/1501.05260,An Algebra of Reversible Quantum Computing,"  We extend the algebra of reversible computation to support quantum computing. Since the algebra is based on true concurrency, it is reversible for quantum computing and it has a sound and complete theory. ",Computer Science - Logic in Computer Science ; ,"Wang, Yong ; "
http://arxiv.org/abs/1501.05354,A speed and departure time optimization algorithm for the   Pollution-Routing Problem,"  We propose a new speed and departure time optimization algorithm for the Pollution-Routing Problem (PRP), which runs in quadratic time and returns a certified optimal schedule. This algorithm is embedded into an iterated local search-based metaheuristic to achieve a combined speed, scheduling and routing optimization. The start of the working day is set as a decision variable for individual routes, thus enabling a better assignment of human resources to required demands. Some routes that were evaluated as unprofitable can now appear as viable candidates later in the day, leading to a larger search space and further opportunities of distance optimization via better service consolidation. Extensive computational experiments on available PRP benchmark instances demonstrate the good performance of the algorithms. The flexible departure times from the depot contribute to reduce the operational costs by 8.36% on the considered instances. ",Computer Science - Data Structures and Algorithms ; ,"Kramer, Raphael ; Maculan, Nelson ; Subramanian, Anand ; Vidal, Thibaut ; "
http://arxiv.org/abs/1501.06076,Fourier Analysis of MAC Polarization,  One problem with MAC polar codes that are based on MAC polarization is that they may not achieve the entire capacity region. The reason behind this problem is that MAC polarization sometimes induces a loss in the capacity region. This paper provides a single letter necessary and sufficient condition which characterizes all the MACs that do not lose any part of their capacity region by polarization. ,Computer Science - Information Theory ; ,"Nasser, Rajai ; Telatar, Emre ; "
http://arxiv.org/abs/1501.06297,Geodesic convolutional neural networks on Riemannian manifolds,"  Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract ""patches"", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use GCNN to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Masci, Jonathan ; Boscaini, Davide ; Bronstein, Michael M. ; Vandergheynst, Pierre ; "
http://arxiv.org/abs/1501.06654,Compressive Sampling of Ensembles of Correlated Signals,"  We propose several sampling architectures for the efficient acquisition of an ensemble of correlated signals. We show that without prior knowledge of the correlation structure, each of our architectures (under different sets of assumptions) can acquire the ensemble at a sub-Nyquist rate. Prior to sampling, the analog signals are diversified using simple, implementable components. The diversification is achieved by injecting types of ""structured randomness"" into the ensemble, the result of which is subsampled. For reconstruction, the ensemble is modeled as a low-rank matrix that we have observed through an (undetermined) set of linear equations. Our main results show that this matrix can be recovered using standard convex programming techniques when the total number of samples is on the order of the intrinsic degree of freedom of the ensemble --- the more heavily correlated the ensemble, the fewer samples are needed.   To motivate this study, we discuss how such ensembles arise in the context of array processing. ",Computer Science - Information Theory ; ,"Ahmed, Ali ; Romberg, Justin ; "
http://arxiv.org/abs/1501.07496,Implementation of an Automatic Syllabic Division Algorithm from Speech   Files in Portuguese Language,"  A new algorithm for voice automatic syllabic splitting in the Portuguese language is proposed, which is based on the envelope of the speech signal of the input audio file. A computational implementation in MatlabTM is presented and made available at the URL http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its straightforwardness, the proposed method is very attractive for embedded systems (e.g. i-phones). It can also be used as a screen to assist more sophisticated methods. Voice excerpts containing more than one syllable and identified by the same envelope are named as super-syllables and they are subsequently separated. The results indicate which samples corresponds to the beginning and end of each detected syllable. Preliminary tests were performed to fifty words at an identification rate circa 70% (further improvements may be incorporated to treat particular phonemes). This algorithm is also useful in voice command systems, as a tool in the teaching of Portuguese language or even for patients with speech pathology. ",Computer Science - Sound ; Computer Science - Computation and Language ; Computer Science - Data Structures and Algorithms ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Da Silva, E. L. F. ; de Oliveira, H. M. ; "
http://arxiv.org/abs/1501.07584,Efficient Divide-And-Conquer Classification Based on Feature-Space   Decomposition,"  This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively. ",Computer Science - Machine Learning ; ,"Guo, Qi ; Chen, Bo-Wei ; Jiang, Feng ; Ji, Xiangyang ; Kung, Sun-Yuan ; "
http://arxiv.org/abs/1501.07637,Simple Mechanisms for a Subadditive Buyer and Applications to Revenue   Monotonicity,"  We study the revenue maximization problem of a seller with n heterogeneous items for sale to a single buyer whose valuation function for sets of items is unknown and drawn from some distribution D. We show that if D is a distribution over subadditive valuations with independent items, then the better of pricing each item separately or pricing only the grand bundle achieves a constant-factor approximation to the revenue of the optimal mechanism. This includes buyers who are k-demand, additive up to a matroid constraint, or additive up to constraints of any downwards-closed set system (and whose values for the individual items are sampled independently), as well as buyers who are fractionally subadditive with item multipliers drawn independently. Our proof makes use of the core-tail decomposition framework developed in prior work showing similar results for the significantly simpler class of additive buyers [LY13, BILW14].   In the second part of the paper, we develop a connection between approximately optimal simple mechanisms and approximate revenue monotonicity with respect to buyers' valuations. Revenue non-monotonicity is the phenomenon that sometimes strictly increasing buyers' values for every set can strictly decrease the revenue of the optimal mechanism [HR12]. Using our main result, we derive a bound on how bad this degradation can be (and dub such a bound a proof of approximate revenue monotonicity); we further show that better bounds on approximate monotonicity imply a better analysis of our simple mechanisms. ",Computer Science - Computer Science and Game Theory ; ,"Rubinstein, Aviad ; Weinberg, S. Matthew ; "
http://arxiv.org/abs/1502.00112,Bar recursion in classical realisability : dependent choice and   continuum hypothesis,"  This paper is about the bar recursion operator in the context of classical realizability. After the pioneering work of Berardi, Bezem & Coquand [1], T. Streicher has shown [10], by means of their bar recursion operator, that the realizability models of ZF, obtained from usual models of $\lambda$-calculus (Scott domains, coherent spaces, . . .), satisfy the axiom of dependent choice. We give a proof of this result, using the tools of classical realizability. Moreover, we show that these realizability models satisfy the well ordering of $\mathbb{R}$ and the continuum hypothesis These formulas are therefore realized by closed $\lambda_c$-terms. This allows to obtain programs from proofs of arithmetical formulas using all these axioms. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03E40 ; F.4.1 ; ,"Krivine, Jean-Louis ; "
http://arxiv.org/abs/1502.01187,Reversibility of d-State Finite Cellular Automata,  This paper investigates reversibility properties of 1-dimensional 3-neighborhood d-state finite cellular automata (CAs) of length n under periodic boundary condition. A tool named reachability tree has been developed from de Bruijn graph which represents all possible reachable configurations of an n-cell CA. This tool has been used to test reversibility of CAs. We have identified a large set of reversible CAs using this tool by following some greedy strategies. ,Computer Science - Formal Languages and Automata Theory ; ,"Bhattacharjee, Kamalika ; Das, Sukanta ; "
http://arxiv.org/abs/1502.01410,On the Lexical Distinguishability of Source Code,"  Natural language is robust against noise. The meaning of many sentences survives the loss of words, sometimes many of them. Some words in a sentence, however, cannot be lost without changing the meaning of the sentence. We call these words ""wheat"" and the rest ""chaff"". The word ""not"" in the sentence ""I do not like rain"" is wheat and ""do"" is chaff. For human understanding of the purpose and behavior of source code, we hypothesize that the same holds. To quantify the extent to which we can separate code into ""wheat"" and ""chaff"", we study a large (100M LOC), diverse corpus of real-world projects in Java. Since methods represent natural, likely distinct units of code, we use the ~9M Java methods in the corpus to approximate a universe of ""sentences."" We extract their wheat by computing the function's minimal distinguishing subset (Minset). Our results confirm that functions contain work offers the first quantitative evidence for recent promising work on keyword-based programming and insight into how to develop a powerful, alternative programming model. ",Computer Science - Software Engineering ; ,"Velez, Martin ; Qiu, Dong ; Zhou, You ; Barr, Earl T. ; Su, Zhendong ; "
http://arxiv.org/abs/1502.01494,Code generator matrices as RNG conditioners,  We quantify precisely the distribution of the output of a binary random number generator (RNG) after conditioning with a binary linear code generator matrix by showing the connection between the Walsh spectrum of the resulting random variable and the weight distribution of the code. Previously known bounds on the performance of linear binary codes as entropy extractors can be derived by considering generator matrices as a selector of a subset of that spectrum. We also extend this framework to the case of non-binary codes. ,"Computer Science - Information Theory ; 65C10, 60B99, 11T71, 94B99 ; E.4 ; G.3 ; ","Tomasi, Alessandro ; Meneghetti, Alessio ; Sala, Massimiliano ; "
http://arxiv.org/abs/1502.01566,A Matrix Laurent Series-based Fast Fourier Transform for Blocklengths   N=4 (mod 8),"  General guidelines for a new fast computation of blocklength 8m+4 DFTs are presented, which is based on a Laurent series involving matrices. Results of non-trivial real multiplicative complexity are presented for blocklengths N=64, achieving lower multiplication counts than previously published FFTs. A detailed description for the cases m=1 and m=2 is presented. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Electrical Engineering and Systems Science - Signal Processing ; ,"de Oliveira, H. M. ; de Souza, R. M. Campello ; de Oliveira, R. C. ; "
http://arxiv.org/abs/1502.01865,Lower Bounds for Monotone Counting Circuits,"  A {+,x}-circuit counts a given multivariate polynomial f, if its values on 0-1 inputs are the same as those of f; on other inputs the circuit may output arbitrary values. Such a circuit counts the number of monomials of f evaluated to 1 by a given 0-1 input vector (with multiplicities given by their coefficients). A circuit decides $f$ if it has the same 0-1 roots as f. We first show that some multilinear polynomials can be exponentially easier to count than to compute them, and can be exponentially easier to decide than to count them. Then we give general lower bounds on the size of counting circuits. ",Computer Science - Computational Complexity ; ,"Jukna, Stasys ; "
http://arxiv.org/abs/1502.02253,Data Bits in Karnaugh Map and Increasing Map Capability in Error   Correcting,"  To provide reliable communication in data transmission, ability of correcting errors is of prime importance. This paper intends to suggest an easy algorithm to detect and correct errors in transmission codes using the well-known Karnaugh map. Referring to past research done and proving new theorems and also using a suggested simple technique taking advantage of the easy concept of Karnaugh map, we offer an algorithm to reduce the number of occupied squares in the map and therefore, reduce substantially the execution time for placing data bits in Karnaugh map. Based on earlier papers, we first propose an algorithm for correction of two simultaneous errors in a code. Then, defining specifications for empty squares of the map, we limit the choices for selection of new squares. In addition, burst errors in sending codes is discussed, and systematically code words for correcting them will be made. ",Computer Science - Information Theory ; ,"Pezeshkpour, Pouya ; Tabandeh, Mahmoud ; "
http://arxiv.org/abs/1502.02272,Rigorous Deductive Argumentation for Socially Relevant Issues,"  The most important problems for society are describable only in vague terms, dependent on subjective positions, and missing highly relevant data. This thesis is intended to revive and further develop the view that giving non-trivial, rigorous deductive arguments concerning such problems -without eliminating the complications of vagueness, subjectivity, and uncertainty- is, though very difficult, not problematic in principle, does not require the invention of new logics -classical first-order logic will do- and is something that more mathematically-inclined people should be pursuing. The framework of interpreted formal proofs is presented for formalizing and criticizing rigorous deductive arguments about vague, subjective, and uncertain issues, and its adequacy is supported largely by a number of major examples. This thesis also documents progress towards a web system for collaboratively authoring and criticizing such arguments, which is the ultimate goal of this project. ","Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03Bxx, 03B52, 03B10, 03Axx ; I.2.3 ; F.4.m ; I.2.4 ; F.4.0 ; ","Wehr, Robert Dustin ; "
http://arxiv.org/abs/1502.02348,MATLAB based language for generating randomized multiple choice   questions,"  In this work we describe a simple MATLAB based language which allows to create randomized multiple choice questions with minimal effort. This language has been successfully tested at Flinders University by the author in a number of mathematics topics including Numerical Analysis, Abstract Algebra and Partial Differential Equations. ",Computer Science - Computers and Society ; ,"Azamov, Nurulla ; "
http://arxiv.org/abs/1502.02481,Dynamic DFS Tree in Undirected Graphs: breaking the $O(m)$ barrier,"  Depth first search (DFS) tree is a fundamental data structure for solving various problems in graphs. It is well known that it takes $O(m+n)$ time to build a DFS tree for a given undirected graph $G=(V,E)$ on $n$ vertices and $m$ edges. We address the problem of maintaining a DFS tree when the graph is undergoing {\em updates} (insertion and deletion of vertices or edges). We present the following results for this problem.   (a) Fault tolerant DFS tree: There exists a data structure of size ${O}(m ~polylog~ n)$ such that given any set ${\cal F}$ of failed vertices or edges, a DFS tree of the graph $G\setminus {\cal F}$ can be reported in ${O}(n|{\cal F}| ~polylog~ n)$ time.   (b) Fully dynamic DFS tree: There exists a fully dynamic algorithm for maintaining a DFS tree that takes worst case ${O}(\sqrt{mn} ~polylog~ n)$ time per update for any arbitrary online sequence of updates.   (c) Incremental DFS tree: Given any arbitrary online sequence of edge insertions, we can maintain a DFS tree in ${O}(n ~polylog~ n)$ worst case time per edge insertion.   These are the first $o(m)$ worst case time results for maintaining a DFS tree in a dynamic environment. Moreover, our fully dynamic algorithm provides, in a seamless manner, the first deterministic algorithm with $O(1)$ query time and $o(m)$ worst case update time for the dynamic subgraph connectivity, biconnectivity, and 2-edge connectivity. ",Computer Science - Data Structures and Algorithms ; ,"Baswana, Surender ; Chaudhury, Shreejit Ray ; Choudhary, Keerti ; Khan, Shahbaz ; "
http://arxiv.org/abs/1502.02511,Measurement Scale Effect on Prediction of Soil Water Retention Curve and   Saturated Hydraulic Conductivity,"  Soil water retention curve (SWRC) and saturated hydraulic conductivity (SHC) are key hydraulic properties for unsaturated zone hydrology and groundwater. In particular, SWRC provides useful information on entry pore-size distribution, and SHC is required for flow and transport modeling in the hydrologic cycle. Not only the SWRC and SHC measurements are time-consuming, but also scale dependent. This means as soil column volume increases, variability of the SWRC and SHC decreases. Although prediction of the SWRC and SHC from available parameters, such as textural data, organic matter, and bulk density have been under investigation for decades, up to now no research has focused on the effect of measurement scale on the soil hydraulic properties pedotransfer functions development. In the literature, several data mining approaches have been applied, such as multiple linear regression, artificial neural networks, group method of data handling. However, in this study we develop pedotransfer functions using a novel approach called contrast pattern aided regression (CPXR) and compare it with the multiple linear regression method. For this purpose, two databases including 210 and 213 soil samples are collected to develop and evaluate pedotransfer functions for the SWRC and SHC, respectively, from the UNSODA database. The 10-fold cross-validation method is applied to evaluate the accuracy and reliability of the proposed regression-based models. Our results show that including measurement scale parameters, such as sample internal diameter and length could substantially improve the accuracy of the SWRC and SHC pedotransfer functions developed using the CPXR method, while this is not the case when MLR is used. Moreover, the CPXR method yields remarkably more accurate soil water retention curve and saturated hydraulic conductivity predictions than the MLR approach. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Databases ; ","Ghanbarian, Behzad ; Taslimitehrani, Vahid ; Dong, Guozhu ; Pachepsky, Yakov A. ; "
http://arxiv.org/abs/1502.02800,Fast integer multiplication using generalized Fermat primes,"  For almost 35 years, Sch{\""o}nhage-Strassen's algorithm has been the fastest algorithm known for multiplying integers, with a time complexity O(n $\times$ log n $\times$ log log n) for multiplying n-bit inputs. In 2007, F{\""u}rer proved that there exists K > 1 and an algorithm performing this operation in O(n $\times$ log n $\times$ K log n). Recent work by Harvey, van der Hoeven, and Lecerf showed that this complexity estimate can be improved in order to get K = 8, and conjecturally K = 4. Using an alternative algorithm, which relies on arithmetic modulo generalized Fermat primes, we obtain conjecturally the same result K = 4 via a careful complexity analysis in the deterministic multitape Turing model. ",Computer Science - Symbolic Computation ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; ,"Covanov, Svyatoslav ; Thomé, Emmanuel ; "
http://arxiv.org/abs/1502.02908,Fast event-based epidemiological simulations on national scales,"  We present a computational modeling framework for data-driven simulations and analysis of infectious disease spread in large populations. For the purpose of efficient simulations, we devise a parallel solution algorithm targeting multi-socket shared memory architectures. The model integrates infectious dynamics as continuous-time Markov chains and available data such as animal movements or aging are incorporated as externally defined events. To bring out parallelism and accelerate the computations, we decompose the spatial domain and optimize cross-boundary communication using dependency-aware task scheduling. Using registered livestock data at a high spatio-temporal resolution, we demonstrate that our approach not only is resilient to varying model configurations, but also scales on all physical cores at realistic work loads. Finally, we show that these very features enable the solution of inverse problems on national scales. ","Quantitative Biology - Populations and Evolution ; Computer Science - Distributed, Parallel, and Cluster Computing ; ","Bauer, Pavol ; Engblom, Stefan ; Widgren, Stefan ; "
http://arxiv.org/abs/1502.03097,"Contextuality, Cohomology and Paradox","  Contextuality is a key feature of quantum mechanics that provides an important non-classical resource for quantum information and computation. Abramsky and Brandenburger used sheaf theory to give a general treatment of contextuality in quantum theory [New Journal of Physics 13 (2011) 113036]. However, contextual phenomena are found in other fields as well, for example database theory. In this paper, we shall develop this unified view of contextuality. We provide two main contributions: firstly, we expose a remarkable connection between contexuality and logical paradoxes; secondly, we show that an important class of contextuality arguments has a topological origin. More specifically, we show that ""All-vs-Nothing"" proofs of contextuality are witnessed by cohomological obstructions. ",Quantum Physics ; Computer Science - Logic in Computer Science ; Mathematics - Algebraic Topology ; ,"Abramsky, Samson ; Barbosa, Rui Soares ; Kishida, Kohei ; Lal, Raymond ; Mansfield, Shane ; "
http://arxiv.org/abs/1502.03371,The Z Transform over Finite Fields,"  Finite field transforms have many applications and, in many cases, can be implemented with a low computational complexity. In this paper, the Z Transform over a finite field is introduced and some of its properties are presented. ",Mathematics - Number Theory ; Computer Science - Numerical Analysis ; Electrical Engineering and Systems Science - Signal Processing ; ,"de Souza, R. M. Campello ; de Oliveira, H. M. ; Silva, D. ; "
http://arxiv.org/abs/1502.03387,A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation   Recording,  This paper presents a new approach for a vocoder design based on full frequency masking by octaves in addition to a technique for spectral filling via beta probability distribution. Some psycho-acoustic characteristics of human hearing - inaudibility masking in frequency and phase - are used as a basis for the proposed algorithm. The results confirm that this technique may be useful to save bandwidth in applications requiring intelligibility. It is recommended for the legal eavesdropping of long voice conversations. ,Computer Science - Sound ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Filho, R. F. B. Sotero ; de Oliveira, H. M. ; de Souza, R. M. Campello ; "
http://arxiv.org/abs/1502.03951,Varieties,"  This text is devoted to the theory of varieties, which provides an important tool, based in universal algebra, for the classification of regular languages. In the introductory section, we present a number of examples that illustrate and motivate the fundamental concepts. We do this for the most part without proofs, and often without precise definitions, leaving these to the formal development of the theory that begins in Section 2. Our presentation of the theory draws heavily on the work of Gehrke, Grigorieff and Pin (2008) on the equational theory of lattices of regular languages. In the subsequent sections we consider in more detail aspects of varieties that were only briefly evoked in the introduction: Decidability, operations on languages, and characterizations in formal logic. ","Computer Science - Formal Languages and Automata Theory ; 68Q70, 20M07 ; F.4.3 ; ","Straubing, Howard ; Weil, Pascal ; "
http://arxiv.org/abs/1502.04052,Computer-aided verification in mechanism design,"  In mechanism design, the gold standard solution concepts are dominant strategy incentive compatibility and Bayesian incentive compatibility. These solution concepts relieve the (possibly unsophisticated) bidders from the need to engage in complicated strategizing. While incentive properties are simple to state, their proofs are specific to the mechanism and can be quite complex. This raises two concerns. From a practical perspective, checking a complex proof can be a tedious process, often requiring experts knowledgeable in mechanism design. Furthermore, from a modeling perspective, if unsophisticated agents are unconvinced of incentive properties, they may strategize in unpredictable ways.   To address both concerns, we explore techniques from computer-aided verification to construct formal proofs of incentive properties. Because formal proofs can be automatically checked, agents do not need to manually check the properties, or even understand the proof. To demonstrate, we present the verification of a sophisticated mechanism: the generic reduction from Bayesian incentive compatible mechanism design to algorithm design given by Hartline, Kleinberg, and Malekian. This mechanism presents new challenges for formal verification, including essential use of randomness from both the execution of the mechanism and from the prior type distributions. As an immediate consequence, our work also formalizes Bayesian incentive compatibility for the entire family of mechanisms derived via this reduction. Finally, as an intermediate step in our formalization, we provide the first formal verification of incentive compatibility for the celebrated Vickrey-Clarke-Groves mechanism. ",Computer Science - Computer Science and Game Theory ; Computer Science - Logic in Computer Science ; ,"Barthe, Gilles ; Gaboardi, Marco ; Arias, Emilio Jesús Gallego ; Hsu, Justin ; Roth, Aaron ; Strub, Pierre-Yves ; "
http://arxiv.org/abs/1502.04147,Bayesian Incentive-Compatible Bandit Exploration,"  Individual decision-makers consume information revealed by the previous decision makers, and produce information that may help in future decisions. This phenomenon is common in a wide range of scenarios in the Internet economy, as well as in other domains such as medical decisions. Each decision-maker would individually prefer to ""exploit"": select an action with the highest expected reward given her current information. At the same time, each decision-maker would prefer previous decision-makers to ""explore"", producing information about the rewards of various actions. A social planner, by means of carefully designed information disclosure, can incentivize the agents to balance the exploration and exploitation so as to maximize social welfare.   We formulate this problem as a multi-armed bandit problem (and various generalizations thereof) under incentive-compatibility constraints induced by the agents' Bayesian priors. We design an incentive-compatible bandit algorithm for the social planner whose regret is asymptotically optimal among all bandit algorithms (incentive-compatible or not). Further, we provide a black-box reduction from an arbitrary multi-arm bandit algorithm to an incentive-compatible one, with only a constant multiplicative increase in regret. This reduction works for very general bandit setting that incorporate contexts and arbitrary auxiliary feedback. ",Computer Science - Computer Science and Game Theory ; ,"Mansour, Yishay ; Slivkins, Aleksandrs ; Syrgkanis, Vasilis ; "
http://arxiv.org/abs/1502.04382,Temporal Network Optimization Subject to Connectivity Constraints,"  In this work we consider \emph{temporal networks}, i.e. networks defined by a \emph{labeling} $\lambda$ assigning to each edge of an \emph{underlying graph} $G$ a set of \emph{discrete} time-labels. The labels of an edge, which are natural numbers, indicate the discrete time moments at which the edge is available. We focus on \emph{path problems} of temporal networks. In particular, we consider \emph{time-respecting} paths, i.e. paths whose edges are assigned by $\lambda$ a strictly increasing sequence of labels. We begin by giving two efficient algorithms for computing shortest time-respecting paths on a temporal network. We then prove that there is a \emph{natural analogue of Menger's theorem} holding for arbitrary temporal networks. Finally, we propose two \emph{cost minimization parameters} for temporal network design. One is the \emph{temporality} of $G$, in which the goal is to minimize the maximum number of labels of an edge, and the other is the \emph{temporal cost} of $G$, in which the goal is to minimize the total number of labels used. Optimization of these parameters is performed subject to some \emph{connectivity constraint}. We prove several lower and upper bounds for the temporality and the temporal cost of some very basic graph families such as rings, directed acyclic graphs, and trees. ","Computer Science - Discrete Mathematics ; 68R10, 68Q17, 68Q25 ; ","Mertzios, George B. ; Michail, Othon ; Spirakis, Paul G. ; "
http://arxiv.org/abs/1502.04634,The exp-log normal form of types,"  Lambda calculi with algebraic data types lie at the core of functional programming languages and proof assistants, but conceal at least two fundamental theoretical problems already in the presence of the simplest non-trivial data type, the sum type. First, we do not know of an explicit and implemented algorithm for deciding the beta-eta-equality of terms---and this in spite of the first decidability results proven two decades ago. Second, it is not clear how to decide when two types are essentially the same, i.e. isomorphic, in spite of the meta-theoretic results on decidability of the isomorphism.   In this paper, we present the exp-log normal form of types---derived from the representation of exponential polynomials via the unary exponential and logarithmic functions---that any type built from arrows, products, and sums, can be isomorphically mapped to. The type normal form can be used as a simple heuristic for deciding type isomorphism, thanks to the fact that it is a systematic application of the high-school identities.   We then show that the type normal form allows to reduce the standard beta-eta equational theory of the lambda calculus to a specialized version of itself, while preserving the completeness of equality on terms. We end by describing an alternative representation of normal terms of the lambda calculus with sums, together with a Coq-implemented converter into/from our new term calculus. The difference with the only other previously implemented heuristic for deciding interesting instances of eta-equality by Balat, Di Cosmo, and Fiore, is that we exploit the type information of terms substantially and this often allows us to obtain a canonical representation of terms without performing sophisticated term analyses. ",Computer Science - Logic in Computer Science ; Computer Science - Programming Languages ; Mathematics - Logic ; ,"Ilik, Danko ; "
http://arxiv.org/abs/1502.05058,Tensor Spectral Clustering for Partitioning Higher-order Network   Structures,"  Spectral graph theory-based methods represent an important class of tools for studying the structure of networks. Spectral methods are based on a first-order Markov chain derived from a random walk on the graph and thus they cannot take advantage of important higher-order network substructures such as triangles, cycles, and feed-forward loops. Here we propose a Tensor Spectral Clustering (TSC) algorithm that allows for modeling higher-order network structures in a graph partitioning framework. Our TSC algorithm allows the user to specify which higher-order network structures (cycles, feed-forward loops, etc.) should be preserved by the network clustering. Higher-order network structures of interest are represented using a tensor, which we then partition by developing a multilinear spectral method. Our framework can be applied to discovering layered flows in networks as well as graph anomaly detection, which we illustrate on synthetic networks. In directed networks, a higher-order structure of particular interest is the directed 3-cycle, which captures feedback loops in networks. We demonstrate that our TSC algorithm produces large partitions that cut fewer directed 3-cycles than standard spectral clustering algorithms. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Benson, Austin R. ; Gleich, David F. ; Leskovec, Jure ; "
http://arxiv.org/abs/1502.05183,Practically-Self-Stabilizing Virtual Synchrony,"  Virtual synchrony is an important abstraction that is proven to be extremely useful when implemented over asynchronous, typically large, message-passing distributed systems. Fault tolerant design is a key criterion for the success of such implementations. This is because large distributed systems can be highly available as long as they do not depend on the full operational status of every system participant. Namely, they employ redundancy in numbers to overcome non-optimal behavior of participants and to gain global robustness and high availability.   Self-stabilizing systems can tolerate transient faults that drive the system to an arbitrary unpredicted configuration. Such systems automatically regain consistency from any such arbitrary configuration, and then produce the desired system behavior. Practically self-stabilizing systems ensure the desired system behavior for practically infinite number of successive steps e.g., $2^{64}$ steps.   We present the first practically self-stabilizing virtual synchrony algorithm. The algorithm is a combination of several new techniques that may be of independent interest. In particular, we present a new counter algorithm that establishes an efficient practically unbounded counter, that in turn can be directly used to implement a self-stabilizing Multiple-Writer Multiple-Reader (MWMR) register emulation. Other components include self-stabilizing group membership, self-stabilizing multicast, and self-stabilizing emulation of replicated state machine. As we base the replicated state machine implementation on virtual synchrony, rather than consensus, the system progresses in more extreme asynchronous executions in relation to consensus-based replicated state machine. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Dolev, Shlomi ; Georgiou, Chryssis ; Marcoullis, Ioannis ; Schiller, Elad Michael ; "
http://arxiv.org/abs/1502.05472,On the Effects of Low-Quality Training Data on Information Extraction   from Clinical Reports,"  In the last five years there has been a flurry of work on information extraction from clinical documents, i.e., on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice, mentions of concepts relevant to such practice. Most of this literature is about methods based on supervised learning, i.e., methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors, no work has been devoted to investigating the effect of the quality of training data on the learning process. Low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated. In this paper we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain. We do this by comparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who has also annotated the test data, and by whose judgment we must abide), with the accuracy deriving from training data annotated by a different coder. The results indicate that, although the disagreement between the two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not always statistically significant. ",Computer Science - Machine Learning ; Computer Science - Computation and Language ; Computer Science - Information Retrieval ; ,"Marcheggiani, Diego ; Sebastiani, Fabrizio ; "
http://arxiv.org/abs/1502.05491,Optimizing Text Quantifiers for Multivariate Loss Functions,"  We address the problem of \emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods. ",Computer Science - Machine Learning ; Computer Science - Information Retrieval ; ,"Esuli, Andrea ; Sebastiani, Fabrizio ; "
http://arxiv.org/abs/1502.05507,On asymptotically good ramp secret sharing schemes,  Asymptotically good sequences of linear ramp secret sharing schemes have been intensively studied by Cramer et al. in terms of sequences of pairs of nested algebraic geometric codes. In those works the focus is on full privacy and full reconstruction. In this paper we analyze additional parameters describing the asymptotic behavior of partial information leakage and possibly also partial reconstruction giving a more complete picture of the access structure for sequences of linear ramp secret sharing schemes. Our study involves a detailed treatment of the (relative) generalized Hamming weights of the considered codes. ,"Computer Science - Information Theory ; 94A62, 94B27, 94B65 ; ","Geil, Olav ; Martin, Stefano ; Martínez-Peñas, Umberto ; Matsumoto, Ryutaroh ; Ruano, Diego ; "
http://arxiv.org/abs/1502.05632,Capturing k-ary Existential Second Order Logic with k-ary   Inclusion-Exclusion Logic,"  In this paper we analyze k-ary inclusion-exclusion logic, INEX[k], which is obtained by extending first order logic with k-ary inclusion and exclusion atoms. We show that every formula of INEX[k] can be expressed with a formula of k-ary existential second order logic, ESO[k]. Conversely, every formula of ESO[k] with at most k-ary free relation variables can be expressed with a formula of INEX[k]. From this it follows that, on the level of sentences, INEX[k] captures the expressive power of ESO[k].   We also introduce several useful operators that can be expressed in INEX[k]. In particular, we define inclusion and exclusion quantifiers and so-called term value preserving disjunction which is essential for the proofs of the main results in this paper. Furthermore, we present a novel method of relativization for team semantics and analyze the duality of inclusion and exclusion atoms. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; F.4.1 ; ,"Rönnholm, Raine ; "
http://arxiv.org/abs/1502.05767,Automatic differentiation in machine learning: a survey,"  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply ""autodiff"", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names ""dynamic computational graphs"" and ""differentiable programming"". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms ""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" as these are encountered more and more in machine learning settings. ","Computer Science - Symbolic Computation ; Computer Science - Machine Learning ; Statistics - Machine Learning ; 68W30, 65D25, 68T05 ; G.1.4 ; I.2.6 ; ","Baydin, Atilim Gunes ; Pearlmutter, Barak A. ; Radul, Alexey Andreyevich ; Siskind, Jeffrey Mark ; "
http://arxiv.org/abs/1502.05880,A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast   Fourier and Hartley Transforms,"  This paper describes a flexible architecture for implementing a new fast computation of the discrete Fourier and Hartley transforms, which is based on a matrix Laurent series. The device calculates the transforms based on a single bit selection operator. The hardware structure and synthesis are presented, which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E device. ",Computer Science - Numerical Analysis ; Computer Science - Discrete Mathematics ; Electrical Engineering and Systems Science - Signal Processing ; ,"de Oliveira, R. C. ; de Oliveira, H. M. ; de Souza, R. M. Campello ; Santos, E. J. P. ; "
http://arxiv.org/abs/1502.06464,Rectified Factor Networks,"  We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods. ",Computer Science - Machine Learning ; Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Neural and Evolutionary Computing ; Statistics - Machine Learning ; ,"Clevert, Djork-Arné ; Mayr, Andreas ; Unterthiner, Thomas ; Hochreiter, Sepp ; "
http://arxiv.org/abs/1502.06761,Minimal Distance of Propositional Models,"  We investigate the complexity of three optimization problems in Boolean propositional logic related to information theory: Given a conjunctive formula over a set of relations, find a satisfying assignment with minimal Hamming distance to a given assignment that satisfies the formula ($\mathsf{NeareastOtherSolution}$, $\mathsf{NOSol}$) or that does not need to satisfy it ($\mathsf{NearestSolution}$, $\mathsf{NSol}$). The third problem asks for two satisfying assignments with a minimal Hamming distance among all such assignments ($\mathsf{MinSolutionDistance}$, $\mathsf{MSD}$).   For all three problems we give complete classifications with respect to the relations admitted in the formula. We give polynomial time algorithms for several classes of constraint languages. For all other cases we prove hardness or completeness regarding APX, APX, NPO, or equivalence to well-known hard optimization problems. ",Computer Science - Computational Complexity ; ,"Behrisch, Mike ; Hermann, Miki ; Mengel, Stefan ; Salzer, Gernot ; "
http://arxiv.org/abs/1502.07209,Exploiting Feature and Class Relationships in Video Categorization with   Regularized Deep Neural Networks,"  In this paper, we study the challenging problem of categorizing videos according to high-level semantics such as the existence of a particular human action or a complex event. Although extensive efforts have been devoted in recent years, most existing works combined multiple video features using simple fusion strategies and neglected the utilization of inter-class semantic relationships. This paper proposes a novel unified framework that jointly exploits the feature relationships and the class relationships for improved categorization performance. Specifically, these two types of relationships are estimated and utilized by rigorously imposing regularizations in the learning process of a deep neural network (DNN). Such a regularized DNN (rDNN) can be efficiently realized using a GPU-based implementation with an affordable training cost. Through arming the DNN with better capability of harnessing both the feature and the class relationships, the proposed rDNN is more suitable for modeling video semantics. With extensive experimental evaluations, we show that rDNN produces superior performance over several state-of-the-art approaches. On the well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain very competitive results: 66.9\% and 73.5\% respectively in terms of mean average precision. In addition, to substantially evaluate our rDNN and stimulate future research on large scale video categorization, we collect and release a new benchmark dataset, called FCVID, which contains 91,223 Internet videos and 239 manually annotated categories. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Multimedia ; ,"Jiang, Yu-Gang ; Wu, Zuxuan ; Wang, Jun ; Xue, Xiangyang ; Chang, Shih-Fu ; "
http://arxiv.org/abs/1502.07331,Highly corrupted image inpainting through hypoelliptic diffusion,"  We present a new image inpainting algorithm, the Averaging and Hypoelliptic Evolution (AHE) algorithm, inspired by the one presented in [SIAM J. Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014] and based upon a semi-discrete variation of the Citti-Petitot-Sarti model of the primary visual cortex V1. The AHE algorithm is based on a suitable combination of sub-Riemannian hypoelliptic diffusion and ad-hoc local averaging techniques. In particular, we focus on reconstructing highly corrupted images (i.e. where more than the 80% of the image is missing), for which we obtain reconstructions comparable with the state-of-the-art. ",Computer Science - Computer Vision and Pattern Recognition ; Mathematics - Analysis of PDEs ; ,"Boscain, Ugo ; Chertovskih, Roman ; Gauthier, Jean-Paul ; Prandi, Dario ; Remizov, Alexey ; "
http://arxiv.org/abs/1502.07481,Cluster Synchronization of Coupled Systems with Nonidentical Linear   Dynamics,"  This paper considers the cluster synchronization problem of generic linear dynamical systems whose system models are distinct in different clusters. These nonidentical linear models render control design and coupling conditions highly correlated if static couplings are used for all individual systems. In this paper, a dynamic coupling structure, which incorporates a global weighting factor and a vanishing auxiliary control variable, is proposed for each agent and is shown to be a feasible solution. Lower bounds on the global and local weighting factors are derived under the condition that every interaction subgraph associated with each cluster admits a directed spanning tree. The spanning tree requirement is further shown to be a necessary condition when the clusters connect acyclicly with each other. Simulations for two applications, cluster heading alignment of nonidentical ships and cluster phase synchronization of nonidentical harmonic oscillators, illustrate essential parts of the derived theoretical results. ",Computer Science - Systems and Control ; ,"Liu, Zhongchang ; Wong, Wing Shing ; "
http://arxiv.org/abs/1502.07884,Characterising Modal Definability of Team-Based Logics via the Universal   Modality,"  We study model and frame definability of various modal logics. Let ML(A+) denote the fragment of modal logic extended with the universal modality in which the universal modality occurs only positively. We show that a class of Kripke models is definable in ML(A+) if and only if the class is elementary and closed under disjoint unions and surjective bisimulations. We also characterise the definability of ML(A+) in the spirit of the well-known Goldblatt--Thomason theorem. We show that an elementary class F of Kripke frames is definable in ML(A+) if and only if F is closed under taking generated subframes and bounded morphic images, and reflects ultrafilter extensions and finitely generated subframes. In addition we study frame definability relative to finite transitive frames and give an analogous characterisation of ML(A+)-definability relative to finite transitive frames. Finally, we initiate the study of model and frame definability in team-based logics. We study (extended) modal dependence logic, (extended) modal inclusion logic, and modal team logic. We establish strict linear hierarchies with respect to model definability and frame definability, respectively. We show that, with respect to model and frame definability, the before mentioned team-based logics, except modal dependence logic, either coincide with ML(A+) or plain modal logic ML. Thus as a corollary we obtain model theoretic characterisation of model and frame definability for the team-based logics. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Sano, Katsuhiko ; Virtema, Jonni ; "
http://arxiv.org/abs/1502.08010,Tropical differential equations,"  Tropical differential equations are introduced and an algorithm is designed which tests solvability of a system of tropical linear differential equations within the complexity polynomial in the size of the system and in its coefficients. Moreover, we show that there exists a minimal solution, and the algorithm constructs it (in case of solvability). This extends a similar complexity bound established for tropical linear systems. In case of tropical linear differential systems in one variable a polynomial complexity algorithm for testing its solvability is designed.   We prove also that the problem of solvability of a system of tropical non-linear differential equations in one variable is $NP$-hard, and this problem for arbitrary number of variables belongs to $NP$. Similar to tropical algebraic equations, a tropical differential equation expresses the (necessary) condition on the dominant term in the issue of solvability of a differential equation in power series. ",Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; 14T05 ; I.1.2 ; ,"Grigoriev, Dima ; "
http://arxiv.org/abs/1503.00207,Knowledge-aided Two-dimensional Autofocus for Spotlight SAR Polar Format   Imagery,"  Conventional two-dimensional (2-D) autofocus algorithms blindly estimate the phase error in the sense that they do not exploit any a priori information on the structure of the 2-D phase error. As such, they often suffer from low computational efficiency and lack of data redundancy to accurately estimate the 2-D phase error. In this paper, a knowledge-aided (KA) 2-D autofocus algorithm which is based on exploiting a priori knowledge about the 2-D phase error structure, is presented. First, as a prerequisite of the proposed KA method, the analytical structure of residual 2-D phase error in SAR imagery is investigated in the polar format algorithm (PFA) framework. Then, by incorporating this a priori information, a novel 2-D autofocus approach is proposed. The new method only requires an estimate of azimuth phase error and/or residual range cell migration, while the 2-D phase error can then be computed directly from the estimated azimuth phase error or residual range cell migration. This 2-D autofocus method can also be applied to refocus moving targets in PFA imagery. Experimental results clearly demonstrate the effectiveness and robustness of the proposed method. ",Computer Science - Information Theory ; ,"Mao, Xinhua ; "
http://arxiv.org/abs/1503.00244,23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and   Management,"  The global influence of Big Data is not only growing but seemingly endless. The trend is leaning towards knowledge that is attained easily and quickly from massive pools of Big Data. Today we are living in the technological world that Dr. Usama Fayyad and his distinguished research fellows discussed in the introductory explanations of Knowledge Discovery in Databases (KDD) predicted nearly two decades ago. Indeed, they were precise in their outlook on Big Data analytics. In fact, the continued improvement of the interoperability of machine learning, statistics, database building and querying fused to create this increasingly popular science- Data Mining and Knowledge Discovery. The next generation computational theories are geared towards helping to extract insightful knowledge from even larger volumes of data at higher rates of speed. As the trend increases in popularity, the need for a highly adaptive solution for knowledge discovery will be necessary. In this research paper, we are introducing the investigation and development of 23 bit-questions for a Metaknowledge template for Big Data Processing and clustering purposes. This research aims to demonstrate the construction of this methodology and proves the validity and the beneficial utilization that brings Knowledge Discovery from Big Data. ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; Computer Science - Machine Learning ; ,"Bari, Nima ; Vichr, Roman ; Kowsari, Kamran ; Berkovich, Simon Y. ; "
http://arxiv.org/abs/1503.00245,Novel Metaknowledge-based Processing Technique for Multimedia Big Data   clustering challenges,"  Past research has challenged us with the task of showing relational patterns between text-based data and then clustering for predictive analysis using Golay Code technique. We focus on a novel approach to extract metaknowledge in multimedia datasets. Our collaboration has been an on-going task of studying the relational patterns between datapoints based on metafeatures extracted from metaknowledge in multimedia datasets. Those selected are significant to suit the mining technique we applied, Golay Code algorithm. In this research paper we summarize findings in optimization of metaknowledge representation for 23-bit representation of structured and unstructured multimedia data in order to ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Information Retrieval ; Computer Science - Multimedia ; ,"Bari, Nima ; Vichr, Roman ; Kowsari, Kamran ; Berkovich, Simon Y. ; "
http://arxiv.org/abs/1503.00491,Utility-Theoretic Ranking for Semi-Automated Text Classification,"  \emph{Semi-Automated Text Classification} (SATC) may be defined as the task of ranking a set $\mathcal{D}$ of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of $\mathcal{D}$ with the goal of increasing the overall labelling accuracy of $\mathcal{D}$, the expected increase is maximized. An obvious SATC strategy is to rank $\mathcal{D}$ so that the documents that the classifier has labelled with the lowest confidence are top-ranked. In this work we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of \emph{validation gain}, defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error. ",Computer Science - Machine Learning ; ,"Berardi, Giacomo ; Esuli, Andrea ; Sebastiani, Fabrizio ; "
http://arxiv.org/abs/1503.00941,Approximation Algorithms for Computing Maximin Share Allocations,"  We study the problem of computing maximin share guarantees, a recently introduced fairness notion. Given a set of $n$ agents and a set of goods, the maximin share of a single agent is the best that she can guarantee to herself, if she would be allowed to partition the goods in any way she prefers, into $n$ bundles, and then receive her least desirable bundle. The objective then in our problem is to find a partition, so that each agent is guaranteed her maximin share. In settings with indivisible goods, such allocations are not guaranteed to exist, so we resort to approximation algorithms. Our main result is a $2/3$-approximation, that runs in polynomial time for any number of agents. This improves upon the algorithm of Procaccia and Wang, which also produces a $2/3$-approximation but runs in polynomial time only for a constant number of agents. To achieve this, we redesign certain parts of their algorithm. Furthermore, motivated by the apparent difficulty, both theoretically and experimentally, in finding lower bounds on the existence of approximate solutions, we undertake a probabilistic analysis. We prove that in randomly generated instances, with high probability there exists a maximin share allocation. This can be seen as a justification of the experimental evidence reported in relevant works. Finally, we provide further positive results for two special cases that arise from previous works. The first one is the intriguing case of $3$ agents, for which it is already known that exact maximin share allocations do not always exist (contrary to the case of $2$ agents). We provide a $7/8$-approximation algorithm, improving the previously known result of $3/4$. The second case is when all item values belong to $\{0, 1, 2\}$, extending the $\{0, 1\}$ setting studied in Bouveret and Lema\^itre. We obtain an exact algorithm for any number of agents in this case. ",Computer Science - Computer Science and Game Theory ; F.2.2 ; G.2.1 ; ,"Amanatidis, Georgios ; Markakis, Evangelos ; Nikzad, Afshin ; Saberi, Amin ; "
http://arxiv.org/abs/1503.01239,Joint Active Learning with Feature Selection via CUR Matrix   Decomposition,"  This paper presents an unsupervised learning approach for simultaneous sample and feature selection, which is in contrast to existing works which mainly tackle these two problems separately. In fact the two tasks are often interleaved with each other: noisy and high-dimensional features will bring adverse effect on sample selection, while informative or representative samples will be beneficial to feature selection. Specifically, we propose a framework to jointly conduct active learning and feature selection based on the CUR matrix decomposition. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the features are highly representative. In particular, our method runs in one-shot without the procedure of iterative sample selection for progressive labeling. Thus, our model is especially suitable when there are few labeled samples or even in the absence of supervision, which is a particular challenge for existing methods. As the joint learning problem is NP-hard, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experimental results on publicly available datasets corroborate the efficacy of our method compared with the state-of-the-art. ",Computer Science - Machine Learning ; ,"Li, Changsheng ; Wang, Xiangfeng ; Dong, Weishan ; Yan, Junchi ; Liu, Qingshan ; Zha, Hongyuan ; "
http://arxiv.org/abs/1503.01334,Faster quantum mixing for slowly evolving sequences of Markov chains,"  Markov chain methods are remarkably successful in computational physics, machine learning, and combinatorial optimization. The cost of such methods often reduces to the mixing time, i.e., the time required to reach the steady state of the Markov chain, which scales as $\delta^{-1}$, the inverse of the spectral gap. It has long been conjectured that quantum computers offer nearly generic quadratic improvements for mixing problems. However, except in special cases, quantum algorithms achieve a run-time of $\mathcal{O}(\sqrt{\delta^{-1}} \sqrt{N})$, which introduces a costly dependence on the Markov chain size $N,$ not present in the classical case. Here, we re-address the problem of mixing of Markov chains when these form a slowly evolving sequence. This setting is akin to the simulated annealing setting and is commonly encountered in physics, material sciences and machine learning. We provide a quantum memory-efficient algorithm with a run-time of $\mathcal{O}(\sqrt{\delta^{-1}} \sqrt[4]{N})$, neglecting logarithmic terms, which is an important improvement for large state spaces. Moreover, our algorithms output quantum encodings of distributions, which has advantages over classical outputs. Finally, we discuss the run-time bounds of mixing algorithms and show that, under certain assumptions, our algorithms are optimal. ",Quantum Physics ; Computer Science - Artificial Intelligence ; Computer Science - Data Structures and Algorithms ; ,"Orsucci, Davide ; Briegel, Hans J. ; Dunjko, Vedran ; "
http://arxiv.org/abs/1503.01404,Complete intersection vanishing ideals on sets of clutter type over   finite fields,  In this paper we give a classification of complete intersection vanishing ideals on parameterized sets of clutter type over finite fields. ,"Mathematics - Commutative Algebra ; Computer Science - Information Theory ; Mathematics - Algebraic Geometry ; Mathematics - Combinatorics ; 14M10, 14G15, 13P25, 13P10, 11T71, 94B27, 94B05 ; ","Tochimani, Azucena ; Villarreal, Rafael H. ; "
http://arxiv.org/abs/1503.01628,Minimal classes of graphs of unbounded clique-width defined by finitely   many forbidden induced subgraphs,"  We discover new hereditary classes of graphs that are minimal (with respect to set inclusion) of unbounded clique-width. The new examples include split permutation graphs and bichain graphs. Each of these classes is characterised by a finite list of minimal forbidden induced subgraphs. These, therefore, disprove a conjecture due to Daligault, Rao and Thomasse from 2010 claiming that all such minimal classes must be defined by infinitely many forbidden induced subgraphs.   In the same paper, Daligault, Rao and Thomasse make another conjecture that every hereditary class of unbounded clique-width must contain a labelled infinite antichain. We show that the two example classes we consider here satisfy this conjecture. Indeed, they each contain a canonical labelled infinite antichain, which leads us to propose a stronger conjecture: that every hereditary class of graphs that is minimal of unbounded clique-width contains a canonical labelled infinite antichain. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Atminas, A. ; Brignall, R. ; Lozin, V. ; Stacho, J. ; "
http://arxiv.org/abs/1503.02196,Higher Weights of Affine Grassmann Codes and Their Duals,"  We consider the question of determining the higher weights or the generalized Hamming weights of affine Grassmann codes and their duals. Several initial as well as terminal higher weights of affine Grassmann codes of an arbitrary level are determined explicitly. In the case of duals of these codes, we give a formula for many initial as well as terminal higher weights. As a special case, we obtain an alternative simpler proof of the formula of Beelen et al for the minimum distance of the dual of an affine Grasmann code. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; Primary 15A03, 11T06 05E99 Secondary 11T71 ; ","Datta, Mrinmoy ; Ghorpade, Sudhir R. ; "
http://arxiv.org/abs/1503.02577,New Algorithms for Computing a Single Component of the Discrete Fourier   Transform,"  This paper introduces the theory and hardware implementation of two new algorithms for computing a single component of the discrete Fourier transform. In terms of multiplicative complexity, both algorithms are more efficient, in general, than the well known Goertzel Algorithm. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Electrical Engineering and Systems Science - Signal Processing ; Statistics - Methodology ; ,"Silva Jr., G. Jerônimo da ; de Souza, R. M. Campello ; de Oliveira, H. M. ; "
http://arxiv.org/abs/1503.02951,Mean Field Games in Nudge Systems for Societal Networks,"  We consider the general problem of resource sharing in societal networks, consisting of interconnected communication, transportation, energy and other networks important to the functioning of society. Participants in such network need to take decisions daily, both on the quantity of resources to use as well as the periods of usage. With this in mind, we discuss the problem of incentivizing users to behave in such a way that society as a whole benefits. In order to perceive societal level impact, such incentives may take the form of rewarding users with lottery tickets based on good behavior, and periodically conducting a lottery to translate these tickets into real rewards. We will pose the user decision problem as a mean field game (MFG), and the incentives question as one of trying to select a good mean field equilibrium (MFE). In such a framework, each agent (a participant in the societal network) takes a decision based on an assumed distribution of actions of his/her competitors, and the incentives provided by the social planner. The system is said to be at MFE if the agent's action is a sample drawn from the assumed distribution. We will show the existence of such an MFE under different settings, and also illustrate how to choose an attractive equilibrium using as an example demand-response in energy networks. ",Computer Science - Computer Science and Game Theory ; ,"Li, Jian ; Xia, Bainan ; Geng, Xinbo ; Ming, Hao ; Shakkottai, Srinivas ; Subramanian, Vijay ; Xie, Le ; "
http://arxiv.org/abs/1503.02985,SybilFrame: A Defense-in-Depth Framework for Structure-Based Sybil   Detection,"  Sybil attacks are becoming increasingly widespread, and pose a significant threat to online social systems; a single adversary can inject multiple colluding identities in the system to compromise security and privacy. Recent works have leveraged the use of social network-based trust relationships to defend against Sybil attacks. However, existing defenses are based on oversimplified assumptions, which do not hold in real world social graphs. In this work, we propose SybilFrame, a defense-in-depth framework for mitigating the problem of Sybil attacks when the oversimplified assumptions are relaxed. Our framework is able to incorporate prior information about users and edges in the social graph. We validate our framework on synthetic and real world network topologies, including a large-scale Twitter dataset with 20M nodes and 265M edges, and demonstrate that our scheme performs an order of magnitude better than previous structure-based approaches. ",Computer Science - Social and Information Networks ; Computer Science - Cryptography and Security ; ,"Gao, Peng ; Gong, Neil Zhenqiang ; Kulkarni, Sanjeev ; Thomas, Kurt ; Mittal, Prateek ; "
http://arxiv.org/abs/1503.03169,"Dynamic Partitioning of Physical Memory Among Virtual Machines,   ASMI:Architectural Support for Memory Isolation","  Cloud computing relies on secure and efficient virtualization. Software level security solutions compromise the performance of virtual machines (VMs), as a large amount of computational power would be utilized for running the security modules. Moreover, software solutions are only as secure as the level that they work on. For example a security module on a hypervisor cannot provide security in the presence of an infected hypervisor. It is a challenge for virtualization technology architects to enhance the security of VMs without degrading their performance. Currently available server machines are not fully equipped to support a secure VM environment without compromising on performance. A few hardware modifications have been introduced by manufactures like Intel and AMD to provide a secure VM environment with low performance degradation. In this paper we propose a novel memory architecture model named \textit{ Architectural Support for Memory Isolation(ASMI)}, that can achieve a true isolated physical memory region to each VM without degrading performance. Along with true memory isolation, ASMI is designed to provide lower memory access times, better utilization of available memory, support for DMA isolation and support for platform independence for users of VMs. ",Computer Science - Hardware Architecture ; ,"R, Jithin ; Chandran, Priya ; "
http://arxiv.org/abs/1503.03185,Testing Randomness by Matching Pennies,"  In the game of Matching Pennies, Alice and Bob each hold a penny, and at every tick of the clock they simultaneously display the head or the tail sides of their coins. If they both display the same side, then Alice wins Bob's penny; if they display different sides, then Bob wins Alice's penny. To avoid giving the opponent a chance to win, both players seem to have nothing else to do but to randomly play heads and tails with equal frequencies. However, while not losing in this game is easy, not missing an opportunity to win is not. Randomizing your own moves can be made easy. Recognizing when the opponent's moves are not random can be arbitrarily hard.   The notion of randomness is central in game theory, but it is usually taken for granted. The notion of outsmarting is not central in game theory, but it is central in the practice of gaming. We pursue the idea that these two notions can be usefully viewed as two sides of the same coin. ","Computer Science - Computer Science and Game Theory ; 91A26, 68Q32 ; I.2.6 ; ","Pavlovic, Dusko ; Seidel, Peter-Michael ; Yahia, Muzamil ; "
http://arxiv.org/abs/1503.03605,An improved return-mapping scheme for nonsmooth yield surfaces: PART I -   the Haigh-Westergaard coordinates,"  The paper is devoted to the numerical solution of elastoplastic constitutive initial value problems. An improved form of the implicit return-mapping scheme for nonsmooth yield surfaces is proposed that systematically builds on a subdifferential formulation of the flow rule. The main advantage of this approach is that the treatment of singular points, such as apices or edges at which the flow direction is multivalued involves only a uniquely defined set of non-linear equations, similarly to smooth yield surfaces. This paper (PART I) is focused on isotropic models containing: $a)$ yield surfaces with one or two apices (singular points) laying on the hydrostatic axis; $b)$ plastic pseudo-potentials that are independent of the Lode angle; $c)$ nonlinear isotropic hardening (optionally). It is shown that for some models the improved integration scheme also enables to a priori decide about a type of the return and investigate existence, uniqueness and semismoothness of discretized constitutive operators in implicit form. Further, the semismooth Newton method is introduced to solve incremental boundary-value problems. The paper also contains numerical examples related to slope stability with available Matlab implementation. ","Computer Science - Computational Engineering, Finance, and Science ; ","Sysala, Stanislav ; Cermak, Martin ; Koudelka, Tomas ; Kruis, Jaroslav ; Zeman, Jan ; Blaheta, Radim ; "
http://arxiv.org/abs/1503.04099,Algorithms and complexity for Turaev-Viro invariants,"  The Turaev-Viro invariants are a powerful family of topological invariants for distinguishing between different 3-manifolds. They are invaluable for mathematical software, but current algorithms to compute them require exponential time.   The invariants are parameterised by an integer $r \geq 3$. We resolve the question of complexity for $r=3$ and $r=4$, giving simple proofs that computing Turaev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \#P-hard. Moreover, we give an explicit fixed-parameter tractable algorithm for arbitrary $r$, and show through concrete implementation and experimentation that this algorithm is practical---and indeed preferable---to the prior state of the art for real computation. ","Mathematics - Geometric Topology ; Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Computer Science - Mathematical Software ; 57M27, 57Q15, 68Q17 ; F.2.2 ; G.2.1 ; G.4 ; ","Burton, Benjamin A. ; Maria, Clément ; Spreer, Jonathan ; "
http://arxiv.org/abs/1503.04424,Bridging Social Media via Distant Supervision,"  Microblog classification has received a lot of attention in recent years. Different classification tasks have been investigated, most of them focusing on classifying microblogs into a small number of classes (five or less) using a training set of manually annotated tweets. Unfortunately, labelling data is tedious and expensive, and finding tweets that cover all the classes of interest is not always straightforward, especially when some of the classes do not frequently arise in practice. In this paper we study an approach to tweet classification based on distant supervision, whereby we automatically transfer labels from one social medium to another for a single-label multi-class classification task. In particular, we apply YouTube video classes to tweets linking to these videos. This provides for free a virtually unlimited number of labelled instances that can be used as training data. The classification experiments we have run show that training a tweet classifier via these automatically labelled data achieves substantially better performance than training the same classifier with a limited amount of manually labelled data; this is advantageous, given that the automatically labelled data come at no cost. Further investigation of our approach shows its robustness when applied with different numbers of classes and across different languages. ",Computer Science - Information Retrieval ; ,"Magdy, Walid ; Sajjad, Hassan ; El-Ganainy, Tarek ; Sebastiani, Fabrizio ; "
http://arxiv.org/abs/1503.04500,A Residual Based Sparse Approximate Inverse Preconditioning Procedure   for Large Sparse Linear Systems,"  The SPAI algorithm, a sparse approximate inverse preconditioning technique for large sparse linear systems, proposed by Grote and Huckle [SIAM J. Sci. Comput., 18 (1997), pp.~838--853.], is based on the F-norm minimization and computes a sparse approximate inverse $M$ of a large sparse matrix $A$ adaptively. However, SPAI may be costly to seek the most profitable indices at each loop and $M$ may be ineffective for preconditioning. In this paper, we propose a residual based sparse approximate inverse preconditioning procedure (RSAI), which, unlike SPAI, is based on only the {\em dominant} rather than all information on the current residual and augments sparsity patterns adaptively during the loops. RSAI is less costly to seek indices and is more effective to capture a good approximate sparsity pattern of $A^{-1}$ than SPAI. To control the sparsity of $M$ and reduce computational cost, we develop a practical RSAI($tol$) algorithm that drops small nonzero entries adaptively during the process. Numerical experiments are reported to demonstrate that RSAI($tol$) is at least competitive with SPAI and can be considerably more efficient and effective than SPAI. They also indicate that RSAI($tol$) is comparable to the PSAI($tol$) algorithm proposed by one of the authors in 2009. ",Mathematics - Numerical Analysis ; Computer Science - Numerical Analysis ; 65F10 ; ,"Jia, Zhongxiao ; Kang, Wenjie ; "
http://arxiv.org/abs/1503.04522,Really Natural Linear Indexed Type Checking,"  Recent works have shown the power of linear indexed type systems for enforcing complex program properties. These systems combine linear types with a language of type-level indices, allowing more fine-grained analyses. Such systems have been fruitfully applied in diverse domains, including implicit complexity and differential privacy. A natural way to enhance the expressiveness of this approach is by allowing the indices to depend on runtime information, in the spirit of dependent types. This approach is used in DFuzz, a language for differential privacy. The DFuzz type system relies on an index language supporting real and natural number arithmetic over constants and variables. Moreover, DFuzz uses a subtyping mechanism to make types more flexible. By themselves, linearity, dependency, and subtyping each require delicate handling when performing type checking or type inference; their combination increases this challenge substantially, as the features can interact in non-trivial ways. In this paper, we study the type-checking problem for DFuzz. We show how we can reduce type checking for (a simple extension of) DFuzz to constraint solving over a first-order theory of naturals and real numbers which, although undecidable, can often be handled in practice by standard numeric solvers. ",Computer Science - Logic in Computer Science ; ,"de Amorim, Arthur Azevedo ; Arias, Emilio Jesús Gallego ; Gaboardi, Marco ; Hsu, Justin ; "
http://arxiv.org/abs/1503.05496,IMP with exceptions over decorated logic,"  In this paper, we facilitate the reasoning about impure programming languages, by annotating terms with `decorations' that describe what computational (side) effect evaluation of a term may involve. In a point-free categorical language,called the `decorated logic', we formalize the mutable state and the exception effects first separately, exploiting anice duality between them, and then combined. The combined decorated logic is used as the target language forthe denotational semantics of the IMP+Exc imperative programming language, and allows us to prove equivalencesbetween programs written in IMP+Exc. The combined logic is encoded in Coq, and this encoding is used to certifysome program equivalence proofs. ",Computer Science - Logic in Computer Science ; ,"Ekici, Burak ; "
http://arxiv.org/abs/1503.05656,Cost-Effective Conceptual Design Using Taxonomies,"  It is known that annotating named entities in unstructured and semi-structured data sets by their concepts improves the effectiveness of answering queries over these data sets. As every enterprise has a limited budget of time or computational resources, it has to annotate a subset of concepts in a given domain whose costs of annotation do not exceed the budget. We call such a subset of concepts a {\it conceptual design} for the annotated data set. We focus on finding a conceptual design that provides the most effective answers to queries over the annotated data set, i.e., a {\it cost-effective conceptual design}. Since, it is often less time-consuming and costly to annotate general concepts than specific concepts, we use information on superclass/subclass relationships between concepts in taxonomies to find a cost-effective conceptual design. We quantify the amount by which a conceptual design with concepts from a taxonomy improves the effectiveness of answering queries over an annotated data set. If the taxonomy is a tree, we prove that the problem is NP-hard and propose an efficient approximation and pseudo-polynomial time algorithms for the problem. We further prove that if the taxonomy is a directed acyclic graph, given some generally accepted hypothesis, it is not possible to find any approximation algorithm with reasonably small approximation ratio for the problem. Our empirical study using real-world data sets, taxonomies, and query workloads shows that our framework effectively quantifies the amount by which a conceptual design improves the effectiveness of answering queries. It also indicates that our algorithms are efficient for a design-time task with pseudo-polynomial algorithm being generally more effective than the approximation algorithm. ",Computer Science - Databases ; ,"Vakilian, Ali ; Chodpathumwan, Yodsawalai ; Termehchy, Arash ; Nayyeri, Amir ; "
http://arxiv.org/abs/1503.06126,Polynomial complexity recognizing a tropical linear variety,  A polynomial complexity algorithm is designed which tests whether a point belongs to a given tropical linear variety. ,Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; 15T05 ; I.1.2 ; ,"Grigoriev, Dima ; "
http://arxiv.org/abs/1503.06483,Construction of FuzzyFind Dictionary using Golay Coding Transformation   for Searching Applications,"  Searching through a large volume of data is very critical for companies, scientists, and searching engines applications due to time complexity and memory complexity. In this paper, a new technique of generating FuzzyFind Dictionary for text mining was introduced. We simply mapped the 23 bits of the English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more FuzzyFind Dictionary, and reflecting the presence or absence of particular letters. This representation preserves closeness of word distortions in terms of closeness of the created binary vectors within Hamming distance of 2 deviations. This paper talks about the Golay Coding Transformation Hash Table and how it can be used on a FuzzyFind Dictionary as a new technology for using in searching through big data. This method is introduced by linear time complexity for generating the dictionary and constant time complexity to access the data and update by new data sets, also updating for new data sets is linear time depends on new data points. This technique is based on searching only for letters of English that each segment has 23 bits, and also we have more than 23-bit and also it could work with more segments as reference table. ",Computer Science - Databases ; Computer Science - Artificial Intelligence ; Computer Science - Data Structures and Algorithms ; Computer Science - Information Retrieval ; Computer Science - Machine Learning ; ,"Kowsari, Kamran ; Yammahi, Maryam ; Bari, Nima ; Vichr, Roman ; Alsaby, Faisal ; Berkovich, Simon Y. ; "
http://arxiv.org/abs/1503.06822,Tree spanners of bounded degree graphs,"  A tree $t$-spanner of a graph $G$ is a spanning tree of $G$ such that the distance between pairs of vertices in the tree is at most $t$ times their distance in $G$. Deciding tree $t$-spanner admissible graphs has been proved to be tractable for $t<3$ and NP-complete for $t>3$, while the complexity status of this problem is unresolved when $t=3$. For every $t>2$ and $b>0$, an efficient dynamic programming algorithm to decide tree $t$-spanner admissibility of graphs with vertex degrees less than $b$ is presented. Only for $t=3$, the algorithm remains efficient, when graphs $G$ with degrees less than $b\log |V(G)|$ are examined. ",Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; ,"Papoutsakis, Ioannis ; "
http://arxiv.org/abs/1503.07759,Large-scale Biological Meta-database Management,"  Up-to-date meta-databases are vital for the analysis of biological data. However,the current exponential increase in biological data leads to exponentially increasing meta-database sizes. Large-scale meta-database management is therefore an important challenge for production platforms providing services for biological data analysis. In particular, there is often a need either to run an analysis with a particular version of a meta-database, or to rerun an analysis with an updated meta-database. We present our GeStore approach for biological meta-database management. It provides efficient storage and runtime generation of specific meta-database versions, and efficient incremental updates for biological data analysis tools. The approach is transparent to the tools, and we provide a framework that makes it easy to integrate GeStore with biological data analysis frameworks. We present the GeStore system, an evaluation of the performance characteristics of the system, and an evaluation of the benefits for a biological data analysis workflow. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Databases ; ","Pedersen, Edvard ; Bongo, Lars Ailo ; "
http://arxiv.org/abs/1503.08370,Global Bandits,"  Multi-armed bandits (MAB) model sequential decision making problems, in which a learner sequentially chooses arms with unknown reward distributions in order to maximize its cumulative reward. Most of the prior work on MAB assumes that the reward distributions of each arm are independent. But in a wide variety of decision problems -- from drug dosage to dynamic pricing -- the expected rewards of different arms are correlated, so that selecting one arm provides information about the expected rewards of other arms as well. We propose and analyze a class of models of such decision problems, which we call {\em global bandits}. In the case in which rewards of all arms are deterministic functions of a single unknown parameter, we construct a greedy policy that achieves {\em bounded regret}, with a bound that depends on the single true parameter of the problem. Hence, this policy selects suboptimal arms only finitely many times with probability one. For this case we also obtain a bound on regret that is {\em independent of the true parameter}; this bound is sub-linear, with an exponent that depends on the informativeness of the arms. We also propose a variant of the greedy policy that achieves $\tilde{\mathcal{O}}(\sqrt{T})$ worst-case and $\mathcal{O}(1)$ parameter dependent regret. Finally, we perform experiments on dynamic pricing and show that the proposed algorithms achieve significant gains with respect to the well-known benchmarks. ",Computer Science - Machine Learning ; ,"Atan, Onur ; Tekin, Cem ; van der Schaar, Mihaela ; "
http://arxiv.org/abs/1503.08381,Towards Easier and Faster Sequence Labeling for Natural Language   Processing: A Search-based Probabilistic Online Learning Framework (SAPO),"  There are two major approaches for sequence labeling. One is the probabilistic gradient-based methods such as conditional random fields (CRF) and neural networks (e.g., RNN), which have high accuracy but drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other is the search-based learning methods such as structured perceptron and margin infused relaxed algorithm (MIRA), which have fast training but also drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and ""easy"" solution, a search-based probabilistic online learning method, to address most of those issues. The method is ""easy"", because the optimization algorithm at the training stage is as simple as the decoding algorithm at the test stage. This method searches the output candidates, derives probabilities, and conducts efficient online learning. We show that this method with fast training and theoretical guarantee of convergence, which is easy to implement, can support search-based optimization and obtain top accuracy. Experiments on well-known tasks show that our method has better accuracy than CRF and BiLSTM\footnote{The SAPO code is released at \url{https://github.com/lancopku/SAPO}.}. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; ,"Sun, Xu ; Ma, Shuming ; Zhang, Yi ; Ren, Xuancheng ; "
http://arxiv.org/abs/1503.08925,Geometry of Interaction for MALL via Hughes-vanGlabbeek Proof-Nets,"  This paper presents, for the first time, a Geometry of Interaction (GoI) interpretation inspired from Hughes-vanGlabbeek (HvG) proof-nets for multiplicative additive linear logic (MALL). Our GoI dynamically captures HvG's geometric correctness criterion-the toggling cycle condition-in terms of algebraic operators. Our new ingredient is a scalar extension of the *-algebra in Girard's *-ring of partial isometries over a boolean polynomial ring with literals of eigenweights as indeterminates. In order to capture feedback arising from cuts, we construct a finer grained execution formula. The expansion of this execution formula is longer than that for collections of slices for multiplicative GoI, hence it is harder to prove termination. Our GoI gives a dynamical, semantical account of boolean valuations (in particular, pruning sub-proofs), conversion of weights (in particular, alpha-conversion), and additive (co)contraction, peculiar to additive proof-theory. Termination of our execution formula is shown to correspond to HvG's toggling criterion. The slice-wise restriction of our execution formula (by collapsing the boolean structure) yields the well known correspondence, explicit or implicit in previous works on multiplicative GoI, between the convergence of execution formulas and acyclicity of proof-nets. Feedback arising from the execution formula by restricting to the boolean polynomial structure yields autonomous definability of eigenweights among cuts from the rest of the eigenweights. ",Computer Science - Logic in Computer Science ; Mathematics - Logic ; ,"Hamano, Masahiro ; "
http://arxiv.org/abs/1504.00169,Complete Simulation of Automata Networks,"  Consider a finite set $A$ and an integer $n \geq 1$. This paper studies the concept of complete simulation in the context of semigroups of transformations of $A^n$, also known as finite state-homogeneous automata networks. For $m \geq n$, a transformation of $A^m$ is \emph{$n$-complete of size $m$} if it may simulate every transformation of $A^n$ by updating one coordinate (or register) at a time. Using tools from memoryless computation, it is established that there is no $n$-complete transformation of size $n$, but there is such a transformation of size $n+1$. By studying the the time of simulation of various $n$-complete transformations, it is conjectured that the maximal time of simulation of any $n$-complete transformation is at least $2n$. A transformation of $A^m$ is \emph{sequentially $n$-complete of size $m$} if it may sequentially simulate every finite sequence of transformations of $A^n$; in this case, minimal examples and bounds for the size and time of simulation are determined. It is also shown that there is no $n$-complete transformation that updates all the registers in parallel, but that there exists a sequentally $n$-complete transformation that updates all but one register in parallel. This illustrates the strengths and weaknesses of parallel models of computation, such as cellular automata. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Mathematics - Group Theory ; ,"Bridoux, Florian ; Castillo-Ramirez, Alonso ; Gadouleau, Maximilien ; "
http://arxiv.org/abs/1504.00222,On the Exact and Approximate Eigenvalue Distribution for Sum of Wishart   Matrices,"  The sum of Wishart matrices has an important role in multiuser communication employing multiantenna elements, such as multiple-input multiple-output (MIMO) multiple access channel (MAC), MIMO Relay channel, and other multiuser channels where the mathematical model is best described using random matrices. In this paper, the distribution of linear combination of complex Wishart distributed matrices has been studied. We present a new closed form expression for the marginal distribution of the eigenvalues of a weighted sum of K complex central Wishart matrices having covariance matrices proportional to the identity matrix. The expression is general and allows for any set of linear coefficients. As an application example, we have used the marginal distribution expression to obtain the ergodic sum-rate capacity for the MIMO-MAC network, and the cut-set upper bound for the MIMO-Relay case, both as closed form expressions. We also present a very simple expression to approximate the sum of Wishart matrices by one equivalent Wishart matrix. All of our results are validated by means of Monte Carlo simulations. As expected, the agreement between the exact eigenvalue distribution and simulations is perfect, whereas for the approximate solution the difference is indistinguishable. ","Computer Science - Information Theory ; 94A15, 94A17, 15A18, 15B52 ; ","Kumar, S. ; Pivaro, G. F. ; Fraidenraich, G. ; Dias, C. F. ; "
http://arxiv.org/abs/1504.00495,Exploring the complex pattern of information spreading in online blog   communities,"  Information spreading in online social communities has attracted tremendous attention due to its utmost practical values in applications. Despite that several individual-level diffusion data have been investigated, we still lack the detailed understanding of the spreading pattern of information. Here, by comparing information flows and social links in a blog community, we find that the diffusion processes are induced by three different spreading mechanisms: social spreading, self-promotion and broadcast. Although numerous previous studies have employed epidemic spreading models to simulate information diffusion, we observe that such models fail to reproduce the realistic diffusion pattern. In respect to users behaviors, strikingly, we find that most users would stick to one specific diffusion mechanism. Moreover, our observations indicate that the social spreading is not only crucial for the structure of diffusion trees, but also capable of inducing more subsequent individuals to acquire the information. Our findings suggest new directions for modeling of information diffusion in social systems and could inform design of efficient propagation strategies based on users behaviors. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Pei, Sen ; Muchnik, Lev ; Tang, Shaoting ; Zheng, Zhiming ; Makse, Hernan A. ; "
http://arxiv.org/abs/1504.01019,On the Total-Power Capacity of Regular-LDPC Codes with Iterative   Message-Passing Decoders,"  Motivated by recently derived fundamental limits on total (transmit + decoding) power for coded communication with VLSI decoders, this paper investigates the scaling behavior of the minimum total power needed to communicate over AWGN channels as the target bit-error-probability tends to zero. We focus on regular-LDPC codes and iterative message-passing decoders. We analyze scaling behavior under two VLSI complexity models of decoding. One model abstracts power consumed in processing elements (""node model""), and another abstracts power consumed in wires which connect the processing elements (""wire model""). We prove that a coding strategy using regular-LDPC codes with Gallager-B decoding achieves order-optimal scaling of total power under the node model. However, we also prove that regular-LDPC codes and iterative message-passing decoders cannot meet existing fundamental limits on total power under the wire model. Further, if the transmit energy-per-bit is bounded, total power grows at a rate that is worse than uncoded transmission. Complementing our theoretical results, we develop detailed physical models of decoding implementations using post-layout circuit simulations. Our theoretical and numerical results show that approaching fundamental limits on total power requires increasing the complexity of both the code design and the corresponding decoding algorithm as communication distance is increased or error-probability is lowered. ",Computer Science - Information Theory ; ,"Ganesan, Karthik ; Grover, Pulkit ; Rabaey, Jan ; Goldsmith, Andrea ; "
http://arxiv.org/abs/1504.01442,The Effect of Recency to Human Mobility,"  In recent years, we have seen scientists attempt to model and explain human dynamics and, in particular, human movement. Many aspects of our complex life are affected by human movements such as disease spread and epidemics modeling, city planning, wireless network development, and disaster relief, to name a few. Given the myriad of applications it is clear that a complete understanding of how people move in space can lead to huge benefits to our society. In most of the recent works, scientists have focused on the idea that people movements are biased towards frequently-visited locations. According to them, human movement is based on an exploration/exploitation dichotomy in which individuals choose new locations (exploration) or return to frequently-visited locations (exploitation). In this work, we focus on the concept of recency. We propose a model in which exploitation in human movement also considers recently-visited locations and not solely frequently-visited locations. We test our hypothesis against different empirical data of human mobility and show that our proposed model is able to better explain the human trajectories in these datasets. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Barbosa, Hugo ; Neto, Fernando Buarque de Lima ; Evsukoff, Alexandre ; Menezes, Ronaldo ; "
http://arxiv.org/abs/1504.01708,Reactive Synthesis Without Regret,"  Two-player zero-sum games of infinite duration and their quantitative versions are used in verification to model the interaction between a controller (Eve) and its environment (Adam). The question usually addressed is that of the existence (and computability) of a strategy for Eve that can maximize her payoff against any strategy of Adam. In this work, we are interested in strategies of Eve that minimize her regret, i.e. strategies that minimize the difference between her actual payoff and the payoff she could have achieved if she had known the strategy of Adam in advance. We give algorithms to compute the strategies of Eve that ensure minimal regret against an adversary whose choice of strategy is (i) unrestricted, (ii) limited to positional strategies, or (iii) limited to word strategies. We also establish relations between the latter version and other problems studied in the literature. ",Computer Science - Computer Science and Game Theory ; Computer Science - Formal Languages and Automata Theory ; Computer Science - Logic in Computer Science ; F.1.1 ; ,"Hunter, Paul ; Pérez, Guillermo A. ; Raskin, Jean-François ; "
http://arxiv.org/abs/1504.01709,"Copyless Cost-Register Automata: Structure, Expressiveness, and Closure   Properties","  Cost register automata (CRA) and its subclass, copyless CRA, were recently proposed by Alur et al. as a new model for computing functions over strings. We study some structural properties, expressiveness, and closure properties of copyless CRA. We show that copyless CRA are strictly less expressive than weighted automata and are not closed under reverse operation. To find a better class we impose restrictions on copyless CRA, which ends successfully with a new robust computational model that is closed under reverse and other extensions. ",Computer Science - Formal Languages and Automata Theory ; ,"Mazowiecki, Filip ; Riveros, Cristian ; "
http://arxiv.org/abs/1504.01782,Profit Maximization for Geographical Dispersed Green Data Centers,"  This paper aims at maximizing the profit associated with running geographically dispersed green data centers, which offer multiple classes of service. To this end, we formulate an optimization framework which relies on the accuracy of the G/D/1 queue in characterizing the workload distribution, and taps on the merits of the workload decomposition into green and brown workload served by green and brown energy resources. Moreover, we take into account of not only the Service Level Agreements (SLAs) between the data centers and clients but also different deregulated electricity markets of data centers located at different regions. We prove the convexity of our optimization problem and the performance of the proposed workload distribution strategy is evaluated via simulations. ",Computer Science - Networking and Internet Architecture ; ,"Kiani, Abbas ; Ansari, Nirwan ; "
http://arxiv.org/abs/1504.02141,Detecting Falls with X-Factor Hidden Markov Models,"  Identification of falls while performing normal activities of daily living (ADL) is important to ensure personal safety and well-being. However, falling is a short term activity that occurs infrequently. This poses a challenge to traditional classification algorithms, because there may be very little training data for falls (or none at all). This paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal ADL. We propose three `X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls using ""inflated"" output covariances (observation models). To estimate the inflated covariances, we propose a novel cross validation method to remove ""outliers"" from the normal ADL that serve as proxies for the unseen falls and allow learning the XHMMs using only normal activities. We tested the proposed XHMM approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data. We show that the traditional method of choosing a threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem. We also show that supervised classification methods perform poorly when very limited fall data are available during the training phase. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; ,"Khan, Shehroz S. ; Karg, Michelle E. ; Kulic, Dana ; Hoey, Jesse ; "
http://arxiv.org/abs/1504.03342,A Survey on Privacy and Security in Online Social Networks,"  Online Social Networks (OSN) are a permanent presence in today's personal and professional lives of a huge segment of the population, with direct consequences to offline activities. Built on a foundation of trust-users connect to other users with common interests or overlapping personal trajectories-online social networks and the associated applications extract an unprecedented volume of personal information. Unsurprisingly, serious privacy and security risks emerged, positioning themselves along two main types of attacks: attacks that exploit the implicit trust embedded in declared social relationships; and attacks that harvest user's personal information for ill-intended use. This article provides an overview of the privacy and security issues that emerged so far in OSNs. We introduce a taxonomy of privacy and security attacks in OSNs, we overview existing solutions to mitigate those attacks, and outline challenges still to overcome. ",Computer Science - Social and Information Networks ; Computer Science - Cryptography and Security ; ,"Kayes, Imrul ; Iamnitchi, Adriana ; "
http://arxiv.org/abs/1504.03856,Sparse multivariate polynomial interpolation in the basis of Schubert   polynomials,"  Schubert polynomials were discovered by A. Lascoux and M. Sch\""utzenberger in the study of cohomology rings of flag manifolds in 1980's. These polynomials generalize Schur polynomials, and form a linear basis of multivariate polynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials, which generalize skew Schur polynomials, and expand in the Schubert basis with the generalized Littlewood-Richardson coefficients.   In this paper we initiate the study of these two families of polynomials from the perspective of computational complexity theory. We first observe that skew Schubert polynomials, and therefore Schubert polynomials, are in $\CountP$ (when evaluating on non-negative integral inputs) and $\VNP$.   Our main result is a deterministic algorithm that computes the expansion of a polynomial $f$ of degree $d$ in $\Z[x_1, \dots, x_n]$ in the basis of Schubert polynomials, assuming an oracle computing Schubert polynomials. This algorithm runs in time polynomial in $n$, $d$, and the bit size of the expansion. This generalizes, and derandomizes, the sparse interpolation algorithm of symmetric polynomials in the Schur basis by Barvinok and Fomin (Advances in Applied Mathematics, 18(3):271--285). In fact, our interpolation algorithm is general enough to accommodate any linear basis satisfying certain natural properties.   Applications of the above results include a new algorithm that computes the generalized Littlewood-Richardson coefficients. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; Mathematics - Combinatorics ; ,"Mukhopadhyay, Priyanka ; Qiao, Youming ; "
http://arxiv.org/abs/1504.03872,Extended Formulations for Independence Polytopes of Regular Matroids,"  We show that the independence polytope of every regular matroid has an extended formulation of size quadratic in the size of its ground set. This generalizes a similar statement for (co-)graphic matroids, which is a simple consequence of Martin's extended formulation for the spanning-tree polytope. In our construction, we make use of Seymour's decomposition theorem for regular matroids. As a consequence, the extended formulations can be computed in polynomial time. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 52Bxx ; ,"Kaibel, Volker ; Lee, Jon ; Walter, Matthias ; Weltge, Stefan ; "
http://arxiv.org/abs/1504.03957,Optimal Hierarchical Radio Resource Management for HetNets with Flexible   Backhaul,"  Providing backhaul connectivity for macro and pico base stations (BSs) constitutes a significant share of infrastructure costs in future heterogeneous networks (HetNets). To address this issue, the emerging idea of flexible backhaul is proposed. Under this architecture, not all the pico BSs are connected to the backhaul, resulting in a significant reduction in the infrastructure costs. In this regard, pico BSs without backhaul connectivity need to communicate with their nearby BSs in order to have indirect accessibility to the backhaul. This makes the radio resource management (RRM) in such networks more complex and challenging. In this paper, we address the problem of cross-layer RRM in HetNets with flexible backhaul. We formulate this problem as a two-timescale non-convex stochastic optimization which jointly optimizes flow control, routing, interference mitigation and link scheduling in order to maximize a generic network utility. By exploiting a hidden convexity of this non-convex problem, we propose an iterative algorithm which converges to the global optimal solution. The proposed algorithm benefits from low complexity and low signalling, which makes it scalable. Moreover, due to the proposed two-timescale design, it is robust to the backhaul signalling latency as well. Simulation results demonstrate the significant performance gain of the proposed solution over various baselines. ",Computer Science - Information Theory ; ,"Omidvar, Naeimeh ; Liu, An ; Lau, Vincent ; Zhang, Fan ; Tsang, Danny H. K. ; Pakravan, Mohammad Reza ; "
http://arxiv.org/abs/1504.04073,The Parametric Closure Problem,"  We define the parametric closure problem, in which the input is a partially ordered set whose elements have linearly varying weights and the goal is to compute the sequence of minimum-weight lower sets of the partial order as the weights vary. We give polynomial time solutions to many important special cases of this problem including semiorders, reachability orders of bounded-treewidth graphs, partial orders of bounded width, and series-parallel partial orders. Our result for series-parallel orders provides a significant generalization of a previous result of Carlson and Eppstein on bicriterion subtree problems. ",Computer Science - Data Structures and Algorithms ; F.2.2 ; ,"Eppstein, David ; "
http://arxiv.org/abs/1504.04217,Quantum and classical coin-flipping protocols based on bit-commitment   and their point games,"  We focus on a family of quantum coin-flipping protocols based on bit-commitment. We discuss how the semidefinite programming formulations of cheating strategies can be reduced to optimizing a linear combination of fidelity functions over a polytope. These turn out to be much simpler semidefinite programs which can be modelled using second-order cone programming problems. We then use these simplifications to construct their point games as developed by Kitaev. We also study the classical version of these protocols and use linear optimization to formulate optimal cheating strategies. We then construct the point games for the classical protocols as well using the analysis for the quantum case.   We discuss the philosophical connections between the classical and quantum protocols and their point games as viewed from optimization theory. In particular, we observe an analogy between a spectrum of physical theories (from classical to quantum) and a spectrum of convex optimization problems (from linear programming to semidefinite programming, through second-order cone programming). In this analogy, classical systems correspond to linear programming problems and the level of quantum features in the system is correlated to the level of sophistication of the semidefinite programming models on the optimization side.   Concerning security analysis, we use the classical point games to prove that every classical protocol of this type allows exactly one of the parties to entirely determine the coin-flip. Using the relationships between the quantum and classical protocols, we show that only ""classical"" protocols can saturate Kitaev's lower bound for strong coin-flipping. Moreover, if the product of Alice and Bob's optimal cheating probabilities is 1/2, then one party can cheat with probability 1. This rules out quantum protocols of this type from attaining the optimal level of security. ",Quantum Physics ; Computer Science - Cryptography and Security ; Mathematics - Optimization and Control ; ,"Nayak, Ashwin ; Sikora, Jamie ; Tunçel, Levent ; "
http://arxiv.org/abs/1504.04867,Information Hiding as a Challenge for Malware Detection,  Information hiding techniques are increasingly utilized by the current malware to hide its existence and communication attempts. In this paper we highlight this new trend by reviewing the most notable examples of malicious software that shows this capability. ,Computer Science - Cryptography and Security ; ,"Mazurczyk, Wojciech ; Caviglione, Luca ; "
http://arxiv.org/abs/1504.04869,Equitable total coloring of corona of cubic graphs,"  The minimum number of total independent partition sets of $V \cup E$ of a graph $G=(V,E)$ is called the \emph{total chromatic number} of $G$, denoted by $\chi''(G)$. If the difference between cardinalities of any two total independent sets is at most one, then the minimum number of total independent partition sets of $V \cup E$ is called the \emph{equitable total chromatic number}, and is denoted by $\chi''_=(G)$.   In this paper we consider equitable total coloring of coronas of cubic graphs, $G \circ H$. It turns out that, independly on the values of equitable total chromatic number of factors $G$ and $H$, equitable total chromatic number of corona $G \circ H$ is equal to $\Delta(G \circ H) +1$. Thereby, we confirm Total Coloring Conjecture (TCC), posed by Behzad in 1964, and Equitable Total Coloring Conjecture (ETCC), posed by Wang in 2002, for coronas of cubic graphs. As a direct consequence we get that all coronas of cubic graphs are of Type 1. ","Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 05C15, 05C76 ; ","Furmańczyk, Hanna ; Zuazua, Rita ; "
http://arxiv.org/abs/1504.05895,Semantic Enrichment of Mobile Phone Data Records Using Background   Knowledge,"  Every day, billions of mobile network events (i.e. CDRs) are generated by cellular phone operator companies. Latent in this data are inspiring insights about human actions and behaviors, the discovery of which is important because context-aware applications and services hold the key to user-driven, intelligent services, which can enhance our everyday lives such as social and economic development, urban planning, and health prevention. The major challenge in this area is that interpreting such a big stream of data requires a deep understanding of mobile network events' context through available background knowledge. This article addresses the issues in context awareness given heterogeneous and uncertain data of mobile network events missing reliable information on the context of this activity. The contribution of this research is a model from a combination of logical and statistical reasoning standpoints for enabling human activity inference in qualitative terms from open geographical data that aimed at improving the quality of human behaviors recognition tasks from CDRs. We use open geographical data, Openstreetmap (OSM), as a proxy for predicting the content of human activity in the area. The user study performed in Trento shows that predicted human activities (top level) match the survey data with around 93% overall accuracy. The extensive validation for predicting a more specific economic type of human activity performed in Barcelona, by employing credit card transaction data. The analysis identifies that appropriately normalized data on points of interest (POI) is a good proxy for predicting human economical activities, with 84% accuracy on average. So the model is proven to be efficient for predicting the context of human activity, when its total level could be efficiently observed from cell phone data records, missing contextual information however. ",Computer Science - Artificial Intelligence ; Computer Science - Information Theory ; 68 ; H.1.2 ; H.2.8 ; H.3.3 ; I.2.3 ; I.2.4 ; G.3 ; ,"Dashdorj, Zolzaya ; Sobolevsky, Stanislav ; Serafini, Luciano ; Antonelli, Fabrizio ; Ratti, Carlo ; "
http://arxiv.org/abs/1504.06043,Stability of Stochastic Approximations with `Controlled Markov' Noise   and Temporal Difference Learning,"  We are interested in understanding stability (almost sure boundedness) of stochastic approximation algorithms (SAs) driven by a `controlled Markov' process. Analyzing this class of algorithms is important, since many reinforcement learning (RL) algorithms can be cast as SAs driven by a `controlled Markov' process. In this paper, we present easily verifiable sufficient conditions for stability and convergence of SAs driven by a `controlled Markov' process. Many RL applications involve continuous state spaces. While our analysis readily ensures stability for such continuous state applications, traditional analyses do not. As compared to literature, our analysis presents a two-fold generalization (a) the Markov process may evolve in a continuous state space and (b) the process need not be ergodic under any given stationary policy. Temporal difference learning (TD) is an important policy evaluation method in reinforcement learning. The theory developed herein, is used to analyze generalized $TD(0)$, an important variant of TD. Our theory is also used to analyze a TD formulation of supervised learning for forecasting problems. ","Computer Science - Systems and Control ; Statistics - Machine Learning ; 62L20, 93E03, 93E35, 34A60 ; ","Ramaswamy, Arunselvan ; Bhatnagar, Shalabh ; "
http://arxiv.org/abs/1504.06234,Acyclic chromatic index of triangle-free 1-planar graphs,"  An acyclic edge coloring of a graph $G$ is a proper edge coloring such that every cycle is colored with at least three colors. The acyclic chromatic index $\chiup_{a}'(G)$ of a graph $G$ is the least number of colors in an acyclic edge coloring of $G$. It was conjectured that $\chiup'_{a}(G)\leq \Delta(G) + 2$ for any simple graph $G$ with maximum degree $\Delta(G)$. A graph is {\em $1$-planar} if it can be drawn on the plane such that every edge is crossed by at most one other edge. In this paper, we prove that every triangle-free $1$-planar graph $G$ has an acyclic edge coloring with $\Delta(G) + 16$ colors. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C15 ; ,"Chen, Jijuan ; Wang, Tao ; Zhang, Huiqin ; "
http://arxiv.org/abs/1504.06320,The Fallacy of Favoring Gradual Replacement Mind Uploading Over   Scan-and-Copy,  Mind uploading speculation and debate often concludes that a procedure described as gradual in-place replacement preserves personal identity while a procedure described as destructive scan-and-copy produces some other identity in the target substrate such that personal identity is lost along with the biological brain. This paper demonstrates a chain of reasoning that establishes metaphysical equivalence between these two methods in terms of preserving personal identity. ,Computer Science - Other Computer Science ; I.2.0 ; ,"Wiley, Keith B. ; Koene, Randal A. ; "
http://arxiv.org/abs/1504.06544,Sampling Correctors,"  In many situations, sample data is obtained from a noisy or imperfect source. In order to address such corruptions, this paper introduces the concept of a sampling corrector. Such algorithms use structure that the distribution is purported to have, in order to allow one to make ""on-the-fly"" corrections to samples drawn from probability distributions. These algorithms then act as filters between the noisy data and the end user.   We show connections between sampling correctors, distribution learning algorithms, and distribution property testing algorithms. We show that these connections can be utilized to expand the applicability of known distribution learning and property testing algorithms as well as to achieve improved algorithms for those tasks.   As a first step, we show how to design sampling correctors using proper learning algorithms. We then focus on the question of whether algorithms for sampling correctors can be more efficient in terms of sample complexity than learning algorithms for the analogous families of distributions. When correcting monotonicity, we show that this is indeed the case when also granted query access to the cumulative distribution function. We also obtain sampling correctors for monotonicity without this stronger type of access, provided that the distribution be originally very close to monotone (namely, at a distance $O(1/\log^2 n)$). In addition to that, we consider a restricted error model that aims at capturing ""missing data"" corruptions. In this model, we show that distributions that are close to monotone have sampling correctors that are significantly more efficient than achievable by the learning approach.   We also consider the question of whether an additional source of independent random bits is required by sampling correctors to implement the correction process. ",Computer Science - Data Structures and Algorithms ; Computer Science - Machine Learning ; Mathematics - Probability ; ,"Canonne, Clément ; Gouleakis, Themis ; Rubinfeld, Ronitt ; "
http://arxiv.org/abs/1504.06582,Approximate Fitting of a Circular Arc When Two Points Are Known,"  The task of approximating points with circular arcs is performed in many applications, such as polyline compression, noise filtering, and feature recognition. However, the development of algorithms that perform a significant amount of circular arcs fitting requires an efficient way of fitting circular arcs with complexity O(1). The elegant solution to this task based on an eigenvector problem for a square nonsymmetrical matrix is described in [1]. For the compression algorithm described in [2], it is necessary to solve this task when two points on the arc are known. This paper describes a different approach to efficiently fitting the arcs and solves the task when one or two points are known. ",Computer Science - Computational Geometry ; ,"Gribov, Alexander ; "
http://arxiv.org/abs/1504.06584,Searching for a Compressed Polyline with a Minimum Number of Vertices,"  There are many practical applications that require simplification of polylines. Some of the goals are to reduce the amount of information necessary to store, improve processing time, or simplify editing. The simplification is usually done by removing some of the vertices, making the resultant polyline go through a subset of the source polyline vertices. However, such approaches do not necessarily produce a new polyline with the minimum number of vertices. The approximate solution to find a polyline, within a specified tolerance, with the minimum number of vertices is described in this paper. ",Computer Science - Computational Geometry ; ,"Gribov, Alexander ; "
http://arxiv.org/abs/1504.06804,High Speed Hashing for Integers and Strings,"  These notes describe the most efficient hash functions currently known for hashing integers and strings. These modern hash functions are often an order of magnitude faster than those presented in standard text books. They are also simpler to implement, and hence a clear win in practice, but their analysis is harder. Some of the most practical hash functions have only appeared in theory papers, and some of them requires combining results from different theory papers. The goal here is to combine the information in lecture-style notes that can be used by theoreticians and practitioners alike, thus making these practical fruits of theory more widely accessible. ",Computer Science - Data Structures and Algorithms ; ,"Thorup, Mikkel ; "
http://arxiv.org/abs/1504.06979,Obstructions for three-coloring graphs without induced paths on six   vertices,"  We prove that there are 24 4-critical $P_6$-free graphs, and give the complete list. We remark that, if $H$ is connected and not a subgraph of $P_6$, there are infinitely many 4-critical $H$-free graphs. Our result answers questions of Golovach et al. and Seymour. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Chudnovsky, Maria ; Goedgebeur, Jan ; Schaudt, Oliver ; Zhong, Mingxian ; "
http://arxiv.org/abs/1504.07056,A Deterministic Almost-Tight Distributed Algorithm for Approximating   Single-Source Shortest Paths,"  We present a deterministic $(1+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source shortest paths problem on distributed weighted networks (the CONGEST model); here $n$ is the number of nodes in the network and $D$ is its (hop) diameter. This is the first non-trivial deterministic algorithm for this problem. It also improves (i) the running time of the randomized $(1+o(1))$-approximation $\tilde O(n^{1/2}D^{1/4}+D)$-time algorithm of Nanongkai [STOC 2014] by a factor of as large as $n^{1/8}$, and (ii) the $O(\epsilon^{-1}\log \epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir's $\tilde O(n^{1/2+\epsilon}+D)$-time algorithm [STOC 2013] within the same running time. Our running time matches the known time lower bound of $\Omega(n^{1/2}/\log n + D)$ [Elkin STOC 2004] up to subpolynomial factors, thus essentially settling the status of this problem which was raised at least a decade ago [Elkin SIGACT News 2004]. It also implies a $(2+o(1))$-approximation $(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for approximating a network's weighted diameter which almost matches the lower bound by Holzer and Pinsker [OPODIS 2015]. In achieving this result, we develop two techniques which might be of independent interest and useful in other settings: (i) a deterministic process that replaces the ""hitting set argument"" commonly used for shortest paths computation in various settings, and (ii) a simple, deterministic, construction of an $(n^{o(1)}, o(1))$-hop set of size $n^{1+o(1)}$. We combine these techniques with many distributed algorithmic techniques, some of which from problems that are not directly related to shortest paths, e.g., ruling sets [Goldberg et al. STOC 1987], source detection [Lenzen and Peleg PODC 2013], and partial distance estimation [Lenzen and Patt-Shamir PODC 2015]. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Data Structures and Algorithms ; C.2.4 ; F.2.0 ; G.2.2 ; ","Henzinger, Monika ; Krinninger, Sebastian ; Nanongkai, Danupon ; "
http://arxiv.org/abs/1504.07766,A multi-class approach for ranking graph nodes: models and experiments   with incomplete data,"  After the phenomenal success of the PageRank algorithm, many researchers have extended the PageRank approach to ranking graphs with richer structures beside the simple linkage structure. In some scenarios we have to deal with multi-parameters data where each node has additional features and there are relationships between such features.   This paper stems from the need of a systematic approach when dealing with multi-parameter data. We propose models and ranking algorithms which can be used with little adjustments for a large variety of networks (bibliographic data, patent data, twitter and social data, healthcare data). In this paper we focus on several aspects which have not been addressed in the literature: (1) we propose different models for ranking multi-parameters data and a class of numerical algorithms for efficiently computing the ranking score of such models, (2) by analyzing the stability and convergence properties of the numerical schemes we tune a fast and stable technique for the ranking problem, (3) we consider the issue of the robustness of our models when data are incomplete. The comparison of the rank on the incomplete data with the rank on the full structure shows that our models compute consistent rankings whose correlation is up to 60% when just 10% of the links of the attributes are maintained suggesting the suitability of our model also when the data are incomplete. ",Mathematics - Numerical Analysis ; Computer Science - Information Retrieval ; Physics - Physics and Society ; 65F15 ; G.2.2 ; F.2.1 ; ,"Del Corso, Gianna M. ; Romani, Francesco ; "
http://arxiv.org/abs/1504.07959,Sublinear-Time Decremental Algorithms for Single-Source Reachability and   Shortest Paths on Directed Graphs,"  We consider dynamic algorithms for maintaining Single-Source Reachability (SSR) and approximate Single-Source Shortest Paths (SSSP) on $n$-node $m$-edge directed graphs under edge deletions (decremental algorithms). The previous fastest algorithm for SSR and SSSP goes back three decades to Even and Shiloach [JACM 1981]; it has $ O(1) $ query time and $ O (mn) $ total update time (i.e., linear amortized update time if all edges are deleted). This algorithm serves as a building block for several other dynamic algorithms. The question whether its total update time can be improved is a major, long standing, open problem.   In this paper, we answer this question affirmatively. We obtain a randomized algorithm with an expected total update time of $ O(\min (m^{7/6} n^{2/3 + o(1)}, m^{3/4} n^{5/4 + o(1)}) ) = O (m n^{9/10 + o(1)}) $ for SSR and $(1+\epsilon)$-approximate SSSP if the edge weights are integers from $ 1 $ to $ W \leq 2^{\log^c{n}} $ and $ \epsilon \geq 1 / \log^c{n} $ for some constant $ c $. We also extend our algorithm to achieve roughly the same running time for Strongly Connected Components (SCC), improving the algorithm of Roditty and Zwick [FOCS 2002]. Our algorithm is most efficient for sparse and dense graphs. When $ m = \Theta(n) $ its running time is $ O (n^{1 + 5/6 + o(1)}) $ and when $ m = \Theta(n^2) $ its running time is $ O (n^{2 + 3/4 + o(1)}) $. For SSR we also obtain an algorithm that is faster for dense graphs and has a total update time of $ O ( m^{2/3} n^{4/3 + o(1)} + m^{3/7} n^{12/7 + o(1)}) $ which is $ O (n^{2 + 2/3}) $ when $ m = \Theta(n^2) $. All our algorithms have constant query time in the worst case and are correct with high probability against an oblivious adversary. ",Computer Science - Data Structures and Algorithms ; ,"Henzinger, Monika ; Krinninger, Sebastian ; Nanongkai, Danupon ; "
http://arxiv.org/abs/1504.08117,Average Convergence Rate of Evolutionary Algorithms,"  In evolutionary optimization, it is important to understand how fast evolutionary algorithms converge to the optimum per generation, or their convergence rate. This paper proposes a new measure of the convergence rate, called average convergence rate. It is a normalised geometric mean of the reduction ratio of the fitness difference per generation. The calculation of the average convergence rate is very simple and it is applicable for most evolutionary algorithms on both continuous and discrete optimization. A theoretical study of the average convergence rate is conducted for discrete optimization. Lower bounds on the average convergence rate are derived. The limit of the average convergence rate is analysed and then the asymptotic average convergence rate is proposed. ",Computer Science - Neural and Evolutionary Computing ; ,"He, Jun ; Lin, Guangming ; "
http://arxiv.org/abs/1505.00199,Theory of Optimizing Pseudolinear Performance Measures: Application to   F-measure,"  Non-linear performance measures are widely used for the evaluation of learning algorithms. For example, $F$-measure is a commonly used performance measure for classification problems in machine learning and information retrieval community. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes $F$-measure, \emph{Jaccard Index}, among many others. We establish that many notions of $F$-measures and \emph{Jaccard Index} are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the $F$-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on pseudo-linear measures, which are asymptotic in nature. We also establish the multi-objective nature of the $F$-score maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various $F$-measure optimization tasks. ",Computer Science - Machine Learning ; ,"Parambath, Shameem A Puthiya ; Usunier, Nicolas ; Grandvalet, Yves ; "
http://arxiv.org/abs/1505.00398,Block Basis Factorization for Scalable Kernel Matrix Evaluation,"  Kernel methods are widespread in machine learning; however, they are limited by the quadratic complexity of the construction, application, and storage of kernel matrices. Low-rank matrix approximation algorithms are widely used to address this problem and reduce the arithmetic and storage cost. However, we observed that for some datasets with wide intra-class variability, the optimal kernel parameter for smaller classes yields a matrix that is less well approximated by low-rank methods. In this paper, we propose an efficient structured low-rank approximation method---the Block Basis Factorization (BBF)---and its fast construction algorithm to approximate radial basis function (RBF) kernel matrices. Our approach has linear memory cost and floating point operations. BBF works for a wide range of kernel bandwidth parameters and extends the domain of applicability of low-rank approximation methods significantly. Our empirical results demonstrate the stability and superiority over the state-of-art kernel approximation algorithms. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Computer Science - Numerical Analysis ; ,"Wang, Ruoxi ; Li, Yingzhou ; Mahoney, Michael W. ; Darve, Eric ; "
http://arxiv.org/abs/1505.00947,Colocated MIMO Radar Waveform Design for Transmit Beampattern Formation,"  In this paper, colocated MIMO radar waveform design is considered by minimizing the integrated side-lobe level to obtain beam patterns with lower side-lobe levels than competing methods. First, a quadratic programming problem is formulated to design beam patterns by using the criteria for a minimal integrated side-lobe level. A theorem is derived that provides a closed-form analytical optimal solution that appears to be an extension of the Rayleigh quotient minimization for a possibly singular matrix in quadratic form. Such singularities are shown to occur in the problem of interest, but proofs for the optimum solution in these singular matrix cases could not be found in the literature. Next, an additional constraint is added to obtain beam patterns with desired 3 dB beamwidths, resulting in a nonconvex quadratically constrained quadratic program which is NP-hard. A semidefinite program and a Gaussian randomized semidefinite relaxation are used to determine feasible solutions arbitrarily close to the solution to the original problem. Theoretical and numerical analyses illustrate the impacts of changing the number of transmitters and orthogonal waveforms employed in the designs. Numerical comparisons are conducted to evaluate the proposed design approaches. ",Computer Science - Information Theory ; ,"Xu, Haisheng ; Blum, Rick S. ; Wang, Jian ; Yuan, Jian ; "
http://arxiv.org/abs/1505.01189,On the Rigidity of Sparse Random Graphs,"  A graph with a trivial automorphism group is said to be rigid. Wright proved that for $\frac{\log n}{n}+\omega(\frac 1n)\leq p\leq \frac 12$ a random graph $G\in G(n,p)$ is rigid whp. It is not hard to see that this lower bound is sharp and for $p<\frac{(1-\epsilon)\log n}{n}$ with positive probability $\text{aut}(G)$ is nontrivial. We show that in the sparser case $\omega(\frac 1 n)\leq p\leq \frac{\log n}{n}+\omega(\frac 1n)$, it holds whp that $G$'s $2$-core is rigid. We conclude that for all $p$, a graph in $G(n,p)$ is reconstrutible whp. In addition this yields for $\omega(\frac 1n)\leq p\leq \frac 12$ a canonical labeling algorithm that almost surely runs in polynomial time with $o(1)$ error rate. This extends the range for which such an algorithm is currently known. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Mathematics - Probability ; ,"Linial, Nati ; Mosheiff, Jonathan ; "
http://arxiv.org/abs/1505.01668,Multi-Target Tracking in Distributed Sensor Networks using Particle PHD   Filters,"  Multi-target tracking is an important problem in civilian and military applications. This paper investigates multi-target tracking in distributed sensor networks. Data association, which arises particularly in multi-object scenarios, can be tackled by various solutions. We consider sequential Monte Carlo implementations of the Probability Hypothesis Density (PHD) filter based on random finite sets. This approach circumvents the data association issue by jointly estimating all targets in the region of interest. To this end, we develop the Diffusion Particle PHD Filter (D-PPHDF) as well as a centralized version, called the Multi-Sensor Particle PHD Filter (MS-PPHDF). Their performance is evaluated in terms of the Optimal Subpattern Assignment (OSPA) metric, benchmarked against a distributed extension of the Posterior Cram\'er-Rao Lower Bound (PCRLB), and compared to the performance of an existing distributed PHD Particle Filter. Furthermore, the robustness of the proposed tracking algorithms against outliers and their performance with respect to different amounts of clutter is investigated. ",Computer Science - Multiagent Systems ; Computer Science - Systems and Control ; Statistics - Applications ; 68 ; ,"Leonard, Mark R. ; Zoubir, Abdelhak M. ; "
http://arxiv.org/abs/1505.02091,Weihrauch-completeness for layerwise computability,"  We introduce the notion of being Weihrauch-complete for layerwise computability and provide several natural examples related to complex oscillations, the law of the iterated logarithm and Birkhoff's theorem. We also consider hitting time operators, which share the Weihrauch degree of the former examples but fail to be layerwise computable. ",Computer Science - Logic in Computer Science ; ,"Pauly, Arno ; Fouché, Willem ; Davie, George ; "
http://arxiv.org/abs/1505.02213,Measuring dependence powerfully and equitably,"  Given a high-dimensional data set we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used is equitable [Reshef et al. 2015a], i.e., if, for some measure of noise, it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic).   In this paper, we introduce and characterize a population measure of dependence called MIC*. We show three ways that MIC* can be viewed: as the population value of MIC, a highly equitable statistic from [Reshef et al. 2011], as a canonical ""smoothing"" of mutual information, and as the supremum of an infinite sequence defined in terms of optimal one-dimensional partitions of the marginals of the joint distribution. Based on this theory, we introduce an efficient approach for computing MIC* from the density of a pair of random variables, and we define a new consistent estimator MICe for MIC* that is efficiently computable. In contrast, there is no known polynomial-time algorithm for computing the original equitable statistic MIC. We show through simulations that MICe has better bias-variance properties than MIC. We then introduce and prove the consistency of a second statistic, TICe, that is a trivial side-product of the computation of MICe and whose goal is powerful independence testing rather than equitability.   We show in simulations that MICe and TICe have good equitability and power against independence respectively. The analyses here complement a more in-depth empirical evaluation of several leading measures of dependence [Reshef et al. 2015b] that shows state-of-the-art performance for MICe and TICe. ",Statistics - Methodology ; Computer Science - Information Theory ; Computer Science - Machine Learning ; Quantitative Biology - Quantitative Methods ; Statistics - Machine Learning ; ,"Reshef, Yakir A. ; Reshef, David N. ; Finucane, Hilary K. ; Sabeti, Pardis C. ; Mitzenmacher, Michael M. ; "
http://arxiv.org/abs/1505.02214,An Empirical Study of Leading Measures of Dependence,"  In exploratory data analysis, we are often interested in identifying promising pairwise associations for further analysis while filtering out weaker, less interesting ones. This can be accomplished by computing a measure of dependence on all variable pairs and examining the highest-scoring pairs, provided the measure of dependence used assigns similar scores to equally noisy relationships of different types. This property, called equitability, is formalized in Reshef et al. [2015b]. In addition to equitability, measures of dependence can also be assessed by the power of their corresponding independence tests as well as their runtime.   Here we present extensive empirical evaluation of the equitability, power against independence, and runtime of several leading measures of dependence. These include two statistics introduced in Reshef et al. [2015a]: MICe, which has equitability as its primary goal, and TICe, which has power against independence as its goal. Regarding equitability, our analysis finds that MICe is the most equitable method on functional relationships in most of the settings we considered, although mutual information estimation proves the most equitable at large sample sizes in some specific settings. Regarding power against independence, we find that TICe, along with Heller and Gorfine's S^DDP, is the state of the art on the relationships we tested. Our analyses also show a trade-off between power against independence and equitability consistent with the theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are significantly faster than many other measures of dependence tested, and computing either one makes computing the other trivial. This suggests that a fast and useful strategy for achieving a combination of power against independence and equitability may be to filter relationships by TICe and then to examine the MICe of only the significant ones. ",Statistics - Methodology ; Computer Science - Information Theory ; Computer Science - Machine Learning ; Quantitative Biology - Quantitative Methods ; Statistics - Machine Learning ; ,"Reshef, David N. ; Reshef, Yakir A. ; Sabeti, Pardis C. ; Mitzenmacher, Michael M. ; "
http://arxiv.org/abs/1505.02348,The Topology of Biological Networks from a Complexity Perspective,"  A complexity-theoretic approach to studying biological networks is proposed. A simple graph representation is used where molecules (DNA, RNA, proteins and chemicals) are vertices and relations between them are directed and signed (promotional (+) or inhibitory (-)) edges. Based on this model, the problem of network evolution (NE) is defined formally as an optimization problem and subsequently proven to be fundamentally hard (NP-hard) by means of reduction from the Knapsack problem (KP). Second, for empirical validation, various biological networks of experimentally-validated interactions are compared against randomly generated networks with varying degree distributions. An NE instance is created using a given real or synthetic (random) network. After being reverse-reduced to a KP instance, each NE instance is fed to a KP solver and the average achieved knapsack value-to-weight ratio is recorded from multiple rounds of simulated evolutionary pressure. The results show that biological networks (and synthetic networks of similar degree distribution) achieve the highest ratios at maximal evolutionary pressure and minimal error tolerance conditions. The more distant (in degree distribution) a synthetic network is from biological networks the lower its achieved ratio. The results shed light on how computational intractability has shaped the evolution of biological networks into their current topology. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; Quantitative Biology - Molecular Networks ; ,"Atiia, Ali ; Major, François ; Waldispühl, Jérôme ; "
http://arxiv.org/abs/1505.02921,How Far Can You Get By Combining Change Detection Algorithms?,"  Given the existence of many change detection algorithms, each with its own peculiarities and strengths, we propose a combination strategy, that we termed IUTIS (In Unity There Is Strength), based on a genetic Programming framework. This combination strategy is aimed at leveraging the strengths of the algorithms and compensate for their weakness. In this paper we show our findings in applying the proposed strategy in two different scenarios. The first scenario is purely performance-based. The second scenario performance and efficiency must be balanced. Results demonstrate that starting from simple algorithms we can achieve comparable results with respect to more complex state-of-the-art change detection algorithms, while keeping the computational complexity affordable for real-time applications. ",Computer Science - Computer Vision and Pattern Recognition ; I.4.8 ; G.1.6 ; ,"Bianco, Simone ; Ciocca, Gianluigi ; Schettini, Raimondo ; "
http://arxiv.org/abs/1505.03001,Detecting the large entries of a sparse covariance matrix in   sub-quadratic time,"  The covariance matrix of a $p$-dimensional random variable is a fundamental quantity in data analysis. Given $n$ i.i.d. observations, it is typically estimated by the sample covariance matrix, at a computational cost of $O(np^{2})$ operations. When $n,p$ are large, this computation may be prohibitively slow. Moreover, in several contemporary applications, the population matrix is approximately sparse, and only its few large entries are of interest. This raises the following question, at the focus of our work: Assuming approximate sparsity of the covariance matrix, can its large entries be detected much faster, say in sub-quadratic time, without explicitly computing all its $p^{2}$ entries? In this paper, we present and theoretically analyze two randomized algorithms that detect the large entries of an approximately sparse sample covariance matrix using only $O(np\text{ poly log } p)$ operations. Furthermore, assuming sparsity of the population matrix, we derive sufficient conditions on the underlying random variable and on the number of samples $n$, for the sample covariance matrix to satisfy our approximate sparsity requirements. Finally, we illustrate the performance of our algorithms via several simulations. ",Statistics - Computation ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Shwartz, Ofer ; Nadler, Boaz ; "
http://arxiv.org/abs/1505.03653,Timed Consistent Network Updates,"  Network updates such as policy and routing changes occur frequently in Software Defined Networks (SDN). Updates should be performed consistently, preventing temporary disruptions, and should require as little overhead as possible. Scalability is increasingly becoming an essential requirement in SDN. In this paper we propose to use time-triggered network updates to achieve consistent updates. Our proposed solution requires lower overhead than existing update approaches, without compromising the consistency during the update. We demonstrate that accurate time enables far more scalable consistent updates in SDN than previously available. In addition, it provides the SDN programmer with fine-grained control over the tradeoff between consistency and scalability. ",Computer Science - Networking and Internet Architecture ; ,"Mizrahi, Tal ; Saat, Efi ; Moses, Yoram ; "
http://arxiv.org/abs/1505.03898,Pinball Loss Minimization for One-bit Compressive Sensing: Convex Models   and Algorithms,"  The one-bit quantization is implemented by one single comparator that operates at low power and a high rate. Hence one-bit compressive sensing (1bit-CS) becomes attractive in signal processing. When measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The one-sided $\ell_1$ loss and the linear loss are two popular loss functions for 1bit-CS. To improve the decoding performance on noisy data, we consider the pinball loss, which provides a bridge between the one-sided $\ell_1$ loss and the linear loss. Using the pinball loss, two convex models, an elastic-net pinball model and its modification with the $\ell_1$-norm constraint, are proposed. To efficiently solve them, the corresponding dual coordinate ascent algorithms are designed and their convergence is proved. The numerical experiments confirm the effectiveness of the proposed algorithms and the performance of the pinball loss minimization for 1bit-CS. ",Computer Science - Information Theory ; Mathematics - Numerical Analysis ; Mathematics - Optimization and Control ; Statistics - Machine Learning ; ,"Huang, Xiaolin ; Shi, Lei ; Yan, Ming ; Suykens, Johan A. K. ; "
http://arxiv.org/abs/1505.03931,Robust Biomolecular Finite Automata,"  We present a uniform method for translating an arbitrary nondeterministic finite automaton (NFA) into a deterministic mass action input/output chemical reaction network (I/O CRN) that simulates it. The I/O CRN receives its input as a continuous time signal consisting of concentrations of chemical species that vary to represent the NFA's input string in a natural way. The I/O CRN exploits the inherent parallelism of chemical kinetics to simulate the NFA in real time with a number of chemical species that is linear in the size of the NFA. We prove that the simulation is correct and that it is robust with respect to perturbations of the input signal, the initial concentrations of species, the output (decision), and the rate constants of the reactions of the I/O CRN. ",Computer Science - Computational Complexity ; Computer Science - Emerging Technologies ; Computer Science - Formal Languages and Automata Theory ; ,"Klinge, Titus H. ; Lathrop, James I. ; Lutz, Jack H. ; "
http://arxiv.org/abs/1505.04026,Automatic Facial Expression Recognition Using Features of Salient Facial   Patches,"  Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Happy, S L ; Routray, Aurobinda ; "
http://arxiv.org/abs/1505.04036,Unified way for computing dynamics of Bose-Einstein condensates and   degenerate Fermi gases,"  In this work we present a very simple and efficient numerical scheme which can be applied to study the dynamics of bosonic systems like, for instance, spinor Bose-Einstein condensates with nonlocal interactions but equally well works for Fermi gases. The method we use is a modification of well known Split Operator Method (SOM). We carefully examine this algorithm in the case of $F=1$ spinor Bose-Einstein condensate without and with dipolar interactions and for strongly interacting two-component Fermi gas. Our extension of the SOM method has many advantages: it is fast, stable, and keeps constant all the physical constraints (constants of motion) at high level. ","Computer Science - Computational Engineering, Finance, and Science ; Condensed Matter - Quantum Gases ; ","Gawryluk, Krzysztof ; Karpiuk, Tomasz ; Gajda, Mariusz ; Rzazewski, Kazimierz ; Brewczyk, Miroslaw ; "
http://arxiv.org/abs/1505.04252,Global Convergence of Unmodified 3-Block ADMM for a Class of Convex   Minimization Problems,"  The alternating direction method of multipliers (ADMM) has been successfully applied to solve structured convex optimization problems due to its superior practical performance. The convergence properties of the 2-block ADMM have been studied extensively in the literature. Specifically, it has been proven that the 2-block ADMM globally converges for any penalty parameter $\gamma>0$. In this sense, the 2-block ADMM allows the parameter to be free, i.e., there is no need to restrict the value for the parameter when implementing this algorithm in order to ensure convergence. However, for the 3-block ADMM, Chen \etal \cite{Chen-admm-failure-2013} recently constructed a counter-example showing that it can diverge if no further condition is imposed. The existing results on studying further sufficient conditions on guaranteeing the convergence of the 3-block ADMM usually require $\gamma$ to be smaller than a certain bound, which is usually either difficult to compute or too small to make it a practical algorithm. In this paper, we show that the 3-block ADMM still globally converges with any penalty parameter $\gamma>0$ if the third function $f_3$ in the objective is smooth and strongly convex, and its condition number is in $[1,1.0798)$, besides some other mild conditions. This requirement covers an important class of problems to be called regularized least squares decomposition (RLSD) in this paper. ",Mathematics - Optimization and Control ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Lin, Tianyi ; Ma, Shiqian ; Zhang, Shuzhong ; "
http://arxiv.org/abs/1505.04343,Provably Correct Algorithms for Matrix Column Subset Selection with   Selectively Sampled Data,"  We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks (Drineas et al., 2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and Singh, 2014). Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Wang, Yining ; Singh, Aarti ; "
http://arxiv.org/abs/1505.04911,Read Mapping on de Bruijn graph,"  Background Next Generation Sequencing (NGS) has dramatically enhanced our ability to sequence genomes, but not to assemble them. In practice, many published genome sequences remain in the state of a large set of contigs. Each contig describes the sequence found along some path of the assembly graph, however, the set of contigs does not record all the sequence information contained in that graph. Although many subsequent analyses can be performed with the set of contigs, one may ask whether mapping reads on the contigs is as informative as mapping them on the paths of the assembly graph. Currently, one lacks practical tools to perform mapping on such graphs. Results Here, we propose a formal definition of mapping on a de Bruijn graph, analyse the problem complexity which turns out to be NP-complete, and provide a practical solution.We propose a pipeline called GGMAP (Greedy Graph MAPping). Its novelty is a procedure to map reads on branching paths of the graph, for which we designed a heuristic algorithm called BGREAT (de Bruijn Graph REAd mapping Tool). For the sake of efficiency, BGREAT rewrites a read sequence as a succession of unitigs sequences. GGMAP can map millions of reads per CPU hour on a de Bruijn graph built from a large set of human genomic reads. Surprisingly, results show that up to 22% more reads can be mapped on the graph but not on the contig set. Conclusions Although mapping reads on a de Bruijn graph is complex task, our proposal offers a practical solution combining efficiency with an improved mapping capacity compared to assembly-based mapping even for complex eukaryotic data. Availability: github.com/Malfoy/BGREAT Keywords: Read mapping; De bruijn graphs; NGS; NP-completeness ",Computer Science - Data Structures and Algorithms ; Quantitative Biology - Genomics ; ,"Limasset, Antoine ; Cazaux, Bastien ; Rivals, Eric ; Peterlongo, Pierre ; "
http://arxiv.org/abs/1505.04938,Convective regularization for optical flow,"  We argue that the time derivative in a fixed coordinate frame may not be the most appropriate measure of time regularity of an optical flow field. Instead, for a given velocity field $v$ we consider the convective acceleration $v_t + \nabla v v$ which describes the acceleration of objects moving according to $v$. Consequently we investigate the suitability of the nonconvex functional $\|v_t + \nabla v v\|^2_{L^2}$ as a regularization term for optical flow. We demonstrate that this term acts as both a spatial and a temporal regularizer and has an intrinsic edge-preserving property. We incorporate it into a contrast invariant and time-regularized variant of the Horn-Schunck functional, prove existence of minimizers and verify experimentally that it addresses some of the problems of basic quadratic models. For the minimization we use an iterative scheme that approximates the original nonlinear problem with a sequence of linear ones. We believe that the convective acceleration may be gainfully introduced in a variety of optical flow models. ","Mathematics - Optimization and Control ; Computer Science - Computer Vision and Pattern Recognition ; 49N45, 68T45, 68U10 ; ","Iglesias, José A. ; Kirisits, Clemens ; "
http://arxiv.org/abs/1505.05193,Synthesising Executable Gene Regulatory Networks from Single-cell Gene   Expression Data,"  Recent experimental advances in biology allow researchers to obtain gene expression profiles at single-cell resolution over hundreds, or even thousands of cells at once. These single-cell measurements provide snapshots of the states of the cells that make up a tissue, instead of the population-level averages provided by conventional high-throughput experiments. This new data therefore provides an exciting opportunity for computational modelling. In this paper we introduce the idea of viewing single-cell gene expression profiles as states of an asynchronous Boolean network, and frame model inference as the problem of reconstructing a Boolean network from its state space. We then give a scalable algorithm to solve this synthesis problem. We apply our technique to both simulated and real data. We first apply our technique to data simulated from a well established model of common myeloid progenitor differentiation. We show that our technique is able to recover the original Boolean network rules. We then apply our technique to a large dataset taken during embryonic development containing thousands of cell measurements. Our technique synthesises matching Boolean networks, and analysis of these models yields new predictions about blood development which our experimental collaborators were able to verify. ","Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Logic in Computer Science ; Quantitative Biology - Molecular Networks ; ","Fisher, Jasmin ; Köksal, Ali Sinan ; Piterman, Nir ; Woodhouse, Steven ; "
http://arxiv.org/abs/1505.05312,A New Oscillating-Error Technique for Classifiers,"  This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made. ",Computer Science - Artificial Intelligence ; ,"Greer, Kieran ; "
http://arxiv.org/abs/1505.05451,Fuzzy Least Squares Twin Support Vector Machines,"  Least Squares Twin Support Vector Machine (LST-SVM) has been shown to be an efficient and fast algorithm for binary classification. It combines the operating principles of Least Squares SVM (LS-SVM) and Twin SVM (T-SVM); it constructs two non-parallel hyperplanes (as in T-SVM) by solving two systems of linear equations (as in LS-SVM). Despite its efficiency, LST-SVM is still unable to cope with two features of real-world problems. First, in many real-world applications, labels of samples are not deterministic; they come naturally with their associated membership degrees. Second, samples in real-world applications may not be equally important and their importance degrees affect the classification. In this paper, we propose Fuzzy LST-SVM (FLST-SVM) to deal with these two characteristics of real-world data. Two models are introduced for FLST-SVM: the first model builds up crisp hyperplanes using training samples and their corresponding membership degrees. The second model, on the other hand, constructs fuzzy hyperplanes using training samples and their membership degrees. Numerical evaluation of the proposed method with synthetic and real datasets demonstrate significant improvement in the classification accuracy of FLST-SVM when compared to well-known existing versions of SVM. ",Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Sartakhti, Javad Salimi ; Afrabandpey, Homayun ; Ghadiri, Nasser ; "
http://arxiv.org/abs/1505.05917,Decentralized Sequential Composite Hypothesis Test Based on One-Bit   Communication,"  This paper considers the sequential composite hypothesis test with multiple sensors. The sensors observe random samples in parallel and communicate with a fusion center, who makes the global decision based on the sensor inputs. On one hand, in the centralized scenario, where local samples are precisely transmitted to the fusion center, the generalized sequential likelihood ratio test (GSPRT) is shown to be asymptotically optimal in terms of the expected sample size as error rates tend to zero. On the other hand, for systems with limited power and bandwidth resources, decentralized solutions that only send a summary of local samples (we particularly focus on a one-bit communication protocol) to the fusion center is of great importance. To this end, we first consider a decentralized scheme where sensors send their one-bit quantized statistics every fixed period of time to the fusion center. We show that such a uniform sampling and quantization scheme is strictly suboptimal and its suboptimality can be quantified by the KL divergence of the distributions of the quantized statistics under both hypotheses. We then propose a decentralized GSPRT based on level-triggered sampling. That is, each sensor runs its own GSPRT repeatedly and reports its local decision to the fusion center asynchronously. We show that this scheme is asymptotically optimal as the local thresholds and global thresholds grow large at different rates. Lastly, two particular models and their associated applications are studied to compare the centralized and decentralized approaches. Numerical results are provided to demonstrate that the proposed level-triggered sampling based decentralized scheme aligns closely with the centralized scheme with substantially lower communication overhead, and significantly outperforms the uniform sampling and quantization based decentralized scheme. ",Statistics - Applications ; Computer Science - Information Theory ; ,"Li, Shang ; Li, Xiaoou ; Wang, Xiaodong ; Liu, Jingchen ; "
http://arxiv.org/abs/1505.06036,VPG and EPG bend-numbers of Halin Graphs,"  A piecewise linear curve in the plane made up of $k+1$ line segments, each of which is either horizontal or vertical, with consecutive segments being of different orientation is called a $k$-bend path. Given a graph $G$, a collection of $k$-bend paths in which each path corresponds to a vertex in $G$ and two paths have a common point if and only if the vertices corresponding to them are adjacent in $G$ is called a $B_k$-VPG representation of $G$. Similarly, a collection of $k$-bend paths each of which corresponds to a vertex in $G$ is called an $B_k$-EPG representation of $G$ if any two paths have a line segment of non-zero length in common if and only if their corresponding vertices are adjacent in $G$. The VPG bend-number $b_v(G)$ of a graph $G$ is the minimum $k$ such that $G$ has a $B_k$-VPG representation. Similarly, the EPG bend-number $b_e(G)$ of a graph $G$ is the minimum $k$ such that $G$ has a $B_k$-EPG representation. Halin graphs are the graphs formed by taking a tree with no degree $2$ vertex and then connecting its leaves to form a cycle in such a way that the graph has a planar embedding. We prove that if $G$ is a Halin graph then $b_v(G) \leq 1$ and $b_e(G) \leq 2$. These bounds are tight. In fact, we prove the stronger result that if $G$ is a planar graph formed by connecting the leaves of any tree to form a simple cycle, then it has a VPG-representation using only one type of 1-bend paths and an EPG-representation using only one type of 2-bend paths. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C62 ; ,"Francis, Mathew C. ; Lahiri, Abhiruk ; "
http://arxiv.org/abs/1505.06362,Polynomially Low Error PCPs with polyloglog n Queries via Modular   Composition,"  We show that every language in NP has a PCP verifier that tosses $O(\log n)$ random coins, has perfect completeness, and a soundness error of at most $1/\text{poly}(n)$, while making at most $O(\text{poly}\log\log n)$ queries into a proof over an alphabet of size at most $n^{1/\text{poly}\log\log n}$. Previous constructions that obtain $1/\text{poly}(n)$ soundness error used either $\text{poly}\log n $ queries or an exponential sized alphabet, i.e. of size $2^{n^c}$ for some $c>0$. Our result is an exponential improvement in both parameters simultaneously.   Our result can be phrased as a polynomial-gap hardness for approximate CSPs with arity $\text{poly}\log\log n$ and alphabet size $n^{1/\text{poly}\log n}$. The ultimate goal, in this direction, would be to prove polynomial hardness for CSPs with constant arity and polynomial alphabet size (aka the sliding scale conjecture for inverse polynomial soundness error).   Our construction is based on a modular generalization of previous PCP constructions in this parameter regime, which involves a composition theorem that uses an extra `consistency' query but maintains the inverse polynomial relation between the soundness error and the alphabet size.   Our main technical/conceptual contribution is a new notion of soundness, which we refer to as {\em distributional soundness}, that replaces the previous notion of ""list decoding soundness"", and that allows us to prove a modular composition theorem with tighter parameters. This new notion of soundness allows us to invoke composition a super-constant number of times without incurring a blow-up in the soundness error. ",Computer Science - Computational Complexity ; ,"Dinur, Irit ; Harsha, Prahladh ; Kindler, Guy ; "
http://arxiv.org/abs/1505.06770,Sketching for Sequential Change-Point Detection,"  We study sequential change-point detection procedures based on linear sketches of high-dimensional signal vectors using generalized likelihood ratio (GLR) statistics. The GLR statistics allow for an unknown post-change mean that represents an anomaly or novelty. We consider both fixed and time-varying projections, derive theoretical approximations to two fundamental performance metrics: the average run length (ARL) and the expected detection delay (EDD); these approximations are shown to be highly accurate by numerical simulations. We further characterize the relative performance measure of the sketching procedure compared to that without sketching and show that there can be little performance loss when the signal strength is sufficiently large, and enough number of sketches are used. Finally, we demonstrate the good performance of sketching procedures using simulation and real-data examples on solar flare detection and failure detection in power networks. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Cao, Yang ; Thompson, Andrew ; Wang, Meng ; Xie, Yao ; "
http://arxiv.org/abs/1505.06819,Coalgebraic Infinite Traces and Kleisli Simulations,"  Kleisli simulation is a categorical notion introduced by Hasuo to verify finite trace inclusion. They allow us to give definitions of forward and backward simulation for various types of systems. A generic categorical theory behind Kleisli simulation has been developed and it guarantees the soundness of those simulations with respect to finite trace semantics. Moreover, those simulations can be aided by forward partial execution (FPE)---a categorical transformation of systems previously introduced by the authors.   In this paper, we give Kleisli simulation a theoretical foundation that assures its soundness also with respect to infinitary traces. There, following Jacobs' work, infinitary trace semantics is characterized as the ""largest homomorphism."" It turns out that soundness of forward simulations is rather straightforward; that of backward simulation holds too, although it requires certain additional conditions and its proof is more involved. We also show that FPE can be successfully employed in the infinitary trace setting to enhance the applicability of Kleisli simulations as witnesses of trace inclusion. Our framework is parameterized in the monad for branching as well as in the functor for linear-time behaviors; for the former we mainly use the powerset monad (for nondeterminism), the sub-Giry monad (for probability), and the lift monad (for exception). ",Computer Science - Logic in Computer Science ; ,"Urabe, Natsuki ; Hasuo, Ichiro ; "
http://arxiv.org/abs/1505.06897,Times series averaging from a probabilistic interpretation of   time-elastic kernel,"  At the light of regularized dynamic time warping kernels, this paper reconsider the concept of time elastic centroid (TEC) for a set of time series. From this perspective, we show first how TEC can easily be addressed as a preimage problem. Unfortunately this preimage problem is ill-posed, may suffer from over-fitting especially for long time series and getting a sub-optimal solution involves heavy computational costs. We then derive two new algorithms based on a probabilistic interpretation of kernel alignment matrices that expresses in terms of probabilistic distributions over sets of alignment paths. The first algorithm is an iterative agglomerative heuristics inspired from the state of the art DTW barycenter averaging (DBA) algorithm proposed specifically for the Dynamic Time Warping measure. The second proposed algorithm achieves a classical averaging of the aligned samples but also implements an averaging of the time of occurrences of the aligned samples. It exploits a straightforward progressive agglomerative heuristics. An experimentation that compares for 45 time series datasets classification error rates obtained by first near neighbors classifiers exploiting a single medoid or centroid estimate to represent each categories show that: i) centroids based approaches significantly outperform medoids based approaches, ii) on the considered experience, the two proposed algorithms outperform the state of the art DBA algorithm, and iii) the second proposed algorithm that implements an averaging jointly in the sample space and along the time axes emerges as the most significantly robust time elastic averaging heuristic with an interesting noise reduction capability. Index Terms-Time series averaging Time elastic kernel Dynamic Time Warping Time series clustering and classification. ",Computer Science - Machine Learning ; Computer Science - Data Structures and Algorithms ; ,"Marteau, Pierre-François ; "
http://arxiv.org/abs/1505.07368,Revisiting Actor Programming in C++,"  The actor model of computation has gained significant popularity over the last decade. Its high level of abstraction makes it appealing for concurrent applications in parallel and distributed systems. However, designing a real-world actor framework that subsumes full scalability, strong reliability, and high resource efficiency requires many conceptual and algorithmic additives to the original model.   In this paper, we report on designing and building CAF, the ""C++ Actor Framework"". CAF targets at providing a concurrent and distributed native environment for scaling up to very large, high-performance applications, and equally well down to small constrained systems. We present the key specifications and design concepts---in particular a message-transparent architecture, type-safe message interfaces, and pattern matching facilities---that make native actors a viable approach for many robust, elastic, and highly distributed developments. We demonstrate the feasibility of CAF in three scenarios: first for elastic, upscaling environments, second for including heterogeneous hardware like GPGPUs, and third for distributed runtime systems. Extensive performance evaluations indicate ideal runtime behaviour for up to 64 cores at very low memory footprint, or in the presence of GPUs. In these tests, CAF continuously outperforms the competing actor environments Erlang, Charm++, SalsaLite, Scala, ActorFoundry, and even the OpenMPI. ",Computer Science - Programming Languages ; ,"Charousset, Dominik ; Hiesgen, Raphael ; Schmidt, Thomas C. ; "
http://arxiv.org/abs/1505.07429,Semi-algebraic colorings of complete graphs,"  We consider $m$-colorings of the edges of a complete graph, where each color class is defined semi-algebraically with bounded complexity. The case $m = 2$ was first studied by Alon et al., who applied this framework to obtain surprisingly strong Ramsey-type results for intersection graphs of geometric objects and for other graphs arising in computational geometry. Considering larger values of $m$ is relevant, e.g., to problems concerning the number of distinct distances determined by a point set.   For $p\ge 3$ and $m\ge 2$, the classical Ramsey number $R(p;m)$ is the smallest positive integer $n$ such that any $m$-coloring of the edges of $K_n$, the complete graph on $n$ vertices, contains a monochromatic $K_p$. It is a longstanding open problem that goes back to Schur (1916) to decide whether $R(p;m)=2^{O(m)}$, for a fixed $p$. We prove that this is true if each color class is defined semi-algebraically with bounded complexity. The order of magnitude of this bound is tight. Our proof is based on the Cutting Lemma of Chazelle {\em et al.}, and on a Szemer\'edi-type regularity lemma for multicolored semi-algebraic graphs, which is of independent interest. The same technique is used to address the semi-algebraic variant of a more general Ramsey-type problem of Erd\H{o}s and Shelah. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Fox, Jacob ; Pach, Janos ; Suk, Andrew ; "
http://arxiv.org/abs/1505.08162,Dimension and cut vertices: an application of Ramsey theory,"  Motivated by quite recent research involving the relationship between the dimension of a poset and graph-theoretic properties of its cover graph, we show that for every $d\geq 1$, if $P$ is a poset and the dimension of a subposet $B$ of $P$ is at most $d$ whenever the cover graph of $B$ is a block of the cover graph of $P$, then the dimension of $P$ is at most $d+2$. We also construct examples which show that this inequality is best possible. We consider the proof of the upper bound to be fairly elegant and relatively compact. However, we know of no simple proof for the lower bound, and our argument requires a powerful tool known as the Product Ramsey Theorem. As a consequence, our constructions involve posets of enormous size. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 06A07, 05C35 ; ","Trotter, William T. ; Walczak, Bartosz ; Wang, Ruidong ; "
http://arxiv.org/abs/1506.00147,Team Performance with Test Scores,"  Team performance is a ubiquitous area of inquiry in the social sciences, and it motivates the problem of team selection -- choosing the members of a team for maximum performance. Influential work of Hong and Page has argued that testing individuals in isolation and then assembling the highest-scoring ones into a team is not an effective method for team selection. For a broad class of performance measures, based on the expected maximum of random variables representing individual candidates, we show that tests directly measuring individual performance are indeed ineffective, but that a more subtle family of tests used in isolation can provide a constant-factor approximation for team performance. These new tests measure the ""potential"" of individuals, in a precise sense, rather than performance, to our knowledge they represent the first time that individual tests have been shown to produce near-optimal teams for a non-trivial team performance measure. We also show families of subdmodular and supermodular team performance functions for which no test applied to individuals can produce near-optimal teams, and discuss implications for submodular maximization via hill-climbing. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; ,"Kleinberg, Jon ; Raghu, Maithra ; "
http://arxiv.org/abs/1506.00272,Synapse: Synthetic Application Profiler and Emulator,"  We introduce Synapse motivated by the needs to estimate and emulate workload execution characteristics on high-performance and distributed heterogeneous resources. Synapse has a platform independent application profiler, and the ability to emulate profiled workloads on a variety of heterogeneous resources. Synapse is used as a proxy application (or ""representative application"") for real workloads, with the added advantage that it can be tuned at arbitrary levels of granularity in ways that are simply not possible using real applications. Experiments show that automated profiling using Synapse represents application characteristics with high fidelity. Emulation using Synapse can reproduce the application behavior in the original runtime environment, as well as reproducing properties when used in a different run-time environments. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Merzky, Andre ; Jha, Shantenu ; "
http://arxiv.org/abs/1506.00290,Compressing Communication in Distributed Protocols,"  We show how to compress communication in selection protocols, where the goal is to agree on a sequence of random bits using only a broadcast channel. More specifically, we present a generic method for converting any selection protocol, into another selection protocol where each message is ``short'' while preserving the same number of rounds, the same output distribution, and the same resilience to error. Assuming that the output of the protocol lies in some universe of size $M$, in our resulting protocol each message consists of only $\mathsf{polylog}(M,n,d)$ many bits, where $n$ is the number of parties and $d$ is the number of rounds. Our transformation works in the presence of either static or adaptive Byzantine faults.   As a corollary, we conclude that for any $\mathsf{poly}(n)$-round collective coin-flipping protocol, leader election protocol, or general selection protocols, messages of length $\mathsf{polylog}(n)$ suffice (in the presence of either static or adaptive Byzantine faults). ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Kalai, Yael Tauman ; Komargodski, Ilan ; "
http://arxiv.org/abs/1506.00366,Formal Concept Analysis for Knowledge Discovery from Biological Data,"  Due to rapid advancement in high-throughput techniques, such as microarrays and next generation sequencing technologies, biological data are increasing exponentially. The current challenge in computational biology and bioinformatics research is how to analyze these huge raw biological data to extract biologically meaningful knowledge. This review paper presents the applications of formal concept analysis for the analysis and knowledge discovery from biological data, including gene expression discretization, gene co-expression mining, gene expression clustering, finding genes in gene regulatory networks, enzyme/protein classifications, binding site classifications, and so on. It also presents a list of FCA-based software tools applied in biological domain and covers the challenges faced so far. ","Computer Science - Artificial Intelligence ; Computer Science - Computational Engineering, Finance, and Science ; Quantitative Biology - Genomics ; ","Raza, Khalid ; "
http://arxiv.org/abs/1506.00529,Desirability and the birth of incomplete preferences,"  We establish an equivalence between two seemingly different theories: one is the traditional axiomatisation of incomplete preferences on horse lotteries based on the mixture independence axiom; the other is the theory of desirable gambles developed in the context of imprecise probability. The equivalence allows us to revisit incomplete preferences from the viewpoint of desirability and through the derived notion of coherent lower previsions. On this basis, we obtain new results and insights: in particular, we show that the theory of incomplete preferences can be developed assuming only the existence of a worst act---no best act is needed---, and that a weakened Archimedean axiom suffices too; this axiom allows us also to address some controversy about the regularity assumption (that probabilities should be positive---they need not), which enables us also to deal with uncountable possibility spaces; we show that it is always possible to extend in a minimal way a preference relation to one with a worst act, and yet the resulting relation is never Archimedean, except in a trivial case; we show that the traditional notion of state independence coincides with the notion called strong independence in imprecise probability---this leads us to give much a weaker definition of state independence than the traditional one; we rework and uniform the notions of complete preferences, beliefs, values; we argue that Archimedeanity does not capture all the problems that can be modelled with sets of expected utilities and we provide a new notion that does precisely that. Perhaps most importantly, we argue throughout that desirability is a powerful and natural setting to model, and work with, incomplete preferences, even in case of non-Archimedean problems. This leads us to suggest that desirability, rather than preference, should be the primitive notion at the basis of decision-theoretic axiomatisations. ",Computer Science - Artificial Intelligence ; ,"Zaffalon, Marco ; Miranda, Enrique ; "
http://arxiv.org/abs/1506.00552,Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than   Random Selection,"  There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that---except in extreme cases---its convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule. ",Mathematics - Optimization and Control ; Computer Science - Machine Learning ; Statistics - Computation ; Statistics - Machine Learning ; ,"Nutini, Julie ; Schmidt, Mark ; Laradji, Issam H. ; Friedlander, Michael ; Koepke, Hoyt ; "
http://arxiv.org/abs/1506.00571,Calculation of the confidence bounds for the fraction nonconforming of   normal populations of measurements in clinical laboratory medicine,"  The fraction nonconforming is a key quality measure used in statistical quality control design in clinical laboratory medicine. The confidence bounds of normal populations of measurements for the fraction nonconforming each of the lower and upper quality specification limits when both the random and the systematic error are unknown can be calculated using the noncentral t-distribution, as it is described in detail and illustrated with examples. ","Computer Science - Computational Engineering, Finance, and Science ; 6804 ; J.2 ; ","Hatjimihail, Aristides T. ; "
http://arxiv.org/abs/1506.00573,Two-dimensional Decoding Algorithms and Recording Techniques for Bit   Patterned Media Feasibility Demonstrations,"  Recording experiments and decoding algorithms are presented for evaluating the bit-error-rate of state-of-the-art magnetic bitpatterned media. The recording experiments are performed with a static tester and conventional hard-disk-drive heads. As the reader dimensions are larger than the bit dimensions in both the down-track and the cross-track directions, a two-dimensional bit decoding algorithm is required. Two such algorithms are presented in details together with the methodology implemented to accurately retrieve island positions during recording. Using these techniques, a 1.6 Td/in$^2$ magnetic bit pattern media is demonstrated to support 2D bit error rates below 1e-2 under shingled magnetic recording conditions. ","Computer Science - Information Theory ; 94A40 (Primary), 94B10, 82D40, 68P30 (Secondary) ; B.4.2 ; B.3.2 ; ","Obukhov, Yuri ; Jubert, Pierre-Olivier ; Bedau, Daniel ; Grobis, Michael ; "
http://arxiv.org/abs/1506.00768,Soft Computing Techniques for Change Detection in remotely sensed images   : A Review,"  With the advent of remote sensing satellites, a huge repository of remotely sensed images is available. Change detection in remotely sensed images has been an active research area as it helps us understand the transitions that are taking place on the Earths surface. This paper discusses the methods and their classifications proposed by various researchers for change detection. Since use of soft computing based techniques are now very popular among research community, this paper also presents a classification based on learning techniques used in soft-computing methods for change detection. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Computer Vision and Pattern Recognition ; ,"Khurana, Madhu ; Saxena, Vikas ; "
http://arxiv.org/abs/1506.01110,Multi-View Factorization Machines,"  For a learning task, data can usually be collected from different sources or be represented from multiple views. For example, laboratory results from different medical examinations are available for disease diagnosis, and each of them can only reflect the health state of a person from a particular aspect/view. Therefore, different views provide complementary information for learning tasks. An effective integration of the multi-view information is expected to facilitate the learning performance. In this paper, we propose a general predictor, named multi-view machines (MVMs), that can effectively include all the possible interactions between features from multiple views. A joint factorization is embedded for the full-order interaction parameters which allows parameter estimation under sparsity. Moreover, MVMs can work in conjunction with different loss functions for a variety of machine learning tasks. A stochastic gradient descent method is presented to learn the MVM model. We further illustrate the advantages of MVMs through comparison with other methods for multi-view classification, including support vector machines (SVMs), support tensor machines (STMs) and factorization machines (FMs). ",Computer Science - Machine Learning ; Statistics - Machine Learning ; H.2.8 ; ,"Cao, Bokai ; Zhou, Hucheng ; Li, Guoqiang ; Yu, Philip S. ; "
http://arxiv.org/abs/1506.01634,Signs of universality in the structure of culture,"  Understanding the dynamics of opinions, preferences and of culture as whole requires more use of empirical data than has been done so far. It is clear that an important role in driving this dynamics is played by social influence, which is the essential ingredient of many quantitative models. Such models require that all traits are fixed when specifying the ""initial cultural state"". Typically, this initial state is randomly generated, from a uniform distribution over the set of possible combinations of traits. However, recent work has shown that the outcome of social influence dynamics strongly depends on the nature of the initial state. If the latter is sampled from empirical data instead of being generated in a uniformly random way, a higher level of cultural diversity is found after long-term dynamics, for the same level of propensity towards collective behavior in the short-term. Moreover, if the initial state is randomized by shuffling the empirical traits among people, the level of long-term cultural diversity is in-between those obtained for the empirical and uniformly random counterparts. The current study repeats the analysis for multiple empirical data sets, showing that the results are remarkably similar, although the matrix of correlations between cultural variables clearly differs across data sets. This points towards robust structural properties inherent in empirical cultural states, possibly due to universal laws governing the dynamics of culture in the real world. The results also suggest that this dynamics might be characterized by criticality and involve mechanisms beyond social influence. ","Physics - Physics and Society ; Computer Science - Computers and Society ; Physics - Data Analysis, Statistics and Probability ; ","Băbeanu, Alexandru-Ionuţ ; Talman, Leandros ; Garlaschelli, Diego ; "
http://arxiv.org/abs/1506.01978,Information measures and cognitive limits in multilayer navigation,"  Cities and their transportation systems become increasingly complex and multimodal as they grow, and it is natural to wonder if it is possible to quantitatively characterize our difficulty to navigate in them and whether such navigation exceeds our cognitive limits. A transition between different searching strategies for navigating in metropolitan maps has been observed for large, complex metropolitan networks. This evidence suggests the existence of another limit associated to the cognitive overload and caused by large amounts of information to process. In this light, we analyzed the world's 15 largest metropolitan networks and estimated the information limit for determining a trip in a transportation system to be on the order of 8 bits. Similar to the ""Dunbar number,"" which represents a limit to the size of an individual's friendship circle, our cognitive limit suggests that maps should not consist of more than about $250$ connections points to be easily readable. We also show that including connections with other transportation modes dramatically increases the information needed to navigate in multilayer transportation networks: in large cities such as New York, Paris, and Tokyo, more than $80\%$ of trips are above the 8-bit limit. Multimodal transportation systems in large cities have thus already exceeded human cognitive limits and consequently the traditional view of navigation in cities has to be revised substantially. ",Physics - Physics and Society ; Condensed Matter - Disordered Systems and Neural Networks ; Computer Science - Social and Information Networks ; ,"Gallotti, Riccardo ; Porter, Mason A. ; Barthelemy, Marc ; "
http://arxiv.org/abs/1506.02228,Strong converse exponents for the feedback-assisted classical capacity   of entanglement-breaking channels,"  Quantum entanglement can be used in a communication scheme to establish a correlation between successive channel inputs that is impossible by classical means. It is known that the classical capacity of quantum channels can be enhanced by such entangled encoding schemes, but this is not always the case. In this paper, we prove that a strong converse theorem holds for the classical capacity of an entanglement-breaking channel even when it is assisted by a classical feedback link from the receiver to the transmitter. In doing so, we identify a bound on the strong converse exponent, which determines the exponentially decaying rate at which the success probability tends to zero, for a sequence of codes with communication rate exceeding capacity. Proving a strong converse, along with an achievability theorem, shows that the classical capacity is a sharp boundary between reliable and unreliable communication regimes. One of the main tools in our proof is the sandwiched Renyi relative entropy. The same method of proof is used to derive an exponential bound on the success probability when communicating over an arbitrary quantum channel assisted by classical feedback, provided that the transmitter does not use entangled encoding schemes. ",Quantum Physics ; Computer Science - Information Theory ; ,"Ding, Dawei ; Wilde, Mark M. ; "
http://arxiv.org/abs/1506.02438,High-Dimensional Continuous Control Using Generalized Advantage   Estimation,"  Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time. ",Computer Science - Machine Learning ; Computer Science - Robotics ; Computer Science - Systems and Control ; ,"Schulman, John ; Moritz, Philipp ; Levine, Sergey ; Jordan, Michael ; Abbeel, Pieter ; "
http://arxiv.org/abs/1506.02930,Arguments for the Effectiveness of Human Problem Solving,"  The question of how humans solve problem has been addressed extensively. However, the direct study of the effectiveness of this process seems to be overlooked. In this paper, we address the issue of the effectiveness of human problem solving: we analyze where this effectiveness comes from and what cognitive mechanisms or heuristics are involved. Our results are based on the optimal probabilistic problem solving strategy that appeared in Solomonoff paper on general problem solving system. We provide arguments that a certain set of cognitive mechanisms or heuristics drive human problem solving in the similar manner as the optimal Solomonoff strategy. The results presented in this paper can serve both cognitive psychology in better understanding of human problem solving processes as well as artificial intelligence in designing more human-like agents. ",Computer Science - Artificial Intelligence ; 68T20 ; I.2.0 ; I.2.8 ; ,"Duris, Frantisek ; "
http://arxiv.org/abs/1506.03171,Error Correction by Structural Simplicity: Correcting Samplable Additive   Errors,"  This paper explores the possibilities and limitations of error correction by the structural simplicity of error mechanisms. Specifically, we consider channel models, called \emph{samplable additive channels}, in which (a) errors are efficiently sampled without the knowledge of the coding scheme or the transmitted codeword; (b) the entropy of the error distribution is bounded; and (c) the number of errors introduced by the channel is unbounded. For the channels, several negative and positive results are provided. Assuming the existence of one-way functions, there are samplable additive errors of entropy $n^{\epsilon}$ for $\epsilon \in (0,1)$ that are pseudorandom, and thus not correctable by efficient coding schemes. It is shown that there is an oracle algorithm that induces a samplable distribution over $\{0,1\}^n$ of entropy $m = \omega( \log n)$ that is not pseudorandom, but is uncorrectable by efficient schemes of rate less than $1 - m/n - o(1)$. The results indicate that restricting error mechanisms to be efficiently samplable and not pseudorandom is insufficient for error correction. As positive results, some conditions are provided under which efficient error correction is possible. ",Computer Science - Information Theory ; ,"Yasunaga, Kenji ; "
http://arxiv.org/abs/1506.03410,Random Projection Forests,"  Ensemble methods---particularly those based on decision trees---have recently demonstrated superior performance in a variety of machine learning settings. We introduce a generalization of many existing decision tree methods called ""Random Projection Forests"" (RPF), which is any decision forest that uses (possibly data dependent and random) linear projections. Using this framework, we introduce a special case, called ""Lumberjack"", using very sparse random projections, that is, linear combinations of a small subset of features. Lumberjack obtains statistically significantly improved accuracy over Random Forests, Gradient Boosted Trees, and other approaches on a standard benchmark suites for classification with varying dimension, sample size, and number of classes. To illustrate how, why, and when Lumberjack outperforms other methods, we conduct extensive simulated experiments, in vectors, images, and nonlinear manifolds. Lumberjack typically yields improved performance over existing decision trees ensembles, while mitigating computational efficiency and scalability, and maintaining interpretability. Lumberjack can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; 68T10 ; I.5.2 ; ,"Tomita, Tyler M. ; Browne, James ; Shen, Cencheng ; Patsolic, Jesse L. ; Yim, Jason ; Priebe, Carey E. ; Burns, Randal ; Maggioni, Mauro ; Vogelstein, Joshua T. ; "
http://arxiv.org/abs/1506.03437,Topology design for stochastically-forced consensus networks,"  We study an optimal control problem aimed at achieving a desired tradeoff between the network coherence and communication requirements in the distributed controller. Our objective is to add a certain number of edges to an undirected network, with a known graph Laplacian, in order to optimally enhance closed-loop performance. To promote controller sparsity, we introduce $\ell_1$-regularization into the optimal ${\cal H}_2$ formulation and cast the design problem as a semidefinite program. We derive a Lagrange dual, provide interpretation of dual variables, and exploit structure of the optimality conditions for undirected networks to develop customized proximal gradient and Newton algorithms that are well-suited for large problems. We illustrate that our algorithms can solve the problems with more than million edges in the controller graph in a few minutes, on a PC. We also exploit structure of connected resistive networks to demonstrate how additional edges can be systematically added in order to minimize the ${\cal H}_2$ norm of the closed-loop system. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Hassan-Moghaddam, Sepideh ; Jovanović, Mihailo R. ; "
http://arxiv.org/abs/1506.03523,Sparsification of Matrices and Compressed Sensing,"  Compressed sensing is a signal processing technique whereby the limits imposed by the Shannon--Nyquist theorem can be exceeded provided certain conditions are imposed on the signal. Such conditions occur in many real-world scenarios, and compressed sensing has emerging applications in medical imaging, big data, and statistics. Finding practical matrix constructions and computationally efficient recovery algorithms for compressed sensing is an area of intense research interest. Many probabilistic matrix constructions have been proposed, and it is now well known that matrices with entries drawn from a suitable probability distribution are essentially optimal for compressed sensing.   Potential applications have motivated the search for constructions of sparse compressed sensing matrices (i.e., matrices containing few non-zero entries). Various constructions have been proposed, and simulations suggest that their performance is comparable to that of dense matrices. In this paper, extensive simulations are presented which suggest that sparsification leads to a marked improvement in compressed sensing performance for a large class of matrix constructions and for many different recovery algorithms. ","Computer Science - Information Theory ; 94A12, 94A15 ; ","Hegarty, Fintan ; Catháin, Padraig Ó ; Zhao, Yunbin ; "
http://arxiv.org/abs/1506.03872,Diamond Sampling for Approximate Maximum All-pairs Dot-product (MAD)   Search,"  Given two sets of vectors, $A = \{{a_1}, \dots, {a_m}\}$ and $B=\{{b_1},\dots,{b_n}\}$, our problem is to find the top-$t$ dot products, i.e., the largest $|{a_i}\cdot{b_j}|$ among all possible pairs. This is a fundamental mathematical problem that appears in numerous data applications involving similarity search, link prediction, and collaborative filtering. We propose a sampling-based approach that avoids direct computation of all $mn$ dot products. We select diamonds (i.e., four-cycles) from the weighted tripartite representation of $A$ and $B$. The probability of selecting a diamond corresponding to pair $(i,j)$ is proportional to $({a_i}\cdot{b_j})^2$, amplifying the focus on the largest-magnitude entries. Experimental results indicate that diamond sampling is orders of magnitude faster than direct computation and requires far fewer samples than any competing approach. We also apply diamond sampling to the special case of maximum inner product search, and get significantly better results than the state-of-the-art hashing methods. ",Computer Science - Social and Information Networks ; Computer Science - Data Structures and Algorithms ; ,"Ballard, Grey ; Pinar, Ali ; Kolda, Tamara G. ; Seshadhri, C. ; "
http://arxiv.org/abs/1506.04391,CamFlow: Managed Data-sharing for Cloud Services,"  A model of cloud services is emerging whereby a few trusted providers manage the underlying hardware and communications whereas many companies build on this infrastructure to offer higher level, cloud-hosted PaaS services and/or SaaS applications. From the start, strong isolation between cloud tenants was seen to be of paramount importance, provided first by virtual machines (VM) and later by containers, which share the operating system (OS) kernel. Increasingly it is the case that applications also require facilities to effect isolation and protection of data managed by those applications. They also require flexible data sharing with other applications, often across the traditional cloud-isolation boundaries; for example, when government provides many related services for its citizens on a common platform. Similar considerations apply to the end-users of applications. But in particular, the incorporation of cloud services within `Internet of Things' architectures is driving the requirements for both protection and cross-application data sharing.   These concerns relate to the management of data. Traditional access control is application and principal/role specific, applied at policy enforcement points, after which there is no subsequent control over where data flows; a crucial issue once data has left its owner's control by cloud-hosted applications and within cloud-services. Information Flow Control (IFC), in addition, offers system-wide, end-to-end, flow control based on the properties of the data. We discuss the potential of cloud-deployed IFC for enforcing owners' dataflow policy with regard to protection and sharing, as well as safeguarding against malicious or buggy software. In addition, the audit log associated with IFC provides transparency, giving configurable system-wide visibility over data flows. [...] ","Computer Science - Cryptography and Security ; Computer Science - Distributed, Parallel, and Cluster Computing ; D.4.6 ; ","Pasquier, Thomas F. J. -M. ; Singh, Jatinder ; Eyers, David ; Bacon, Jean ; "
http://arxiv.org/abs/1506.04440,Traces of Hecke Operators and Refined Weight Enumerators of Reed-Solomon   Codes,"  We study the quadratic residue weight enumerators of the dual projective Reed-Solomon codes of dimensions $5$ and $q-4$ over the finite field $\mathbb{F}_q$. Our main results are formulas for the coefficients of the the quadratic residue weight enumerators for such codes. If $q=p^v$ and we fix $v$ and vary $p$ then our formulas for the coefficients of the dimension $q-4$ code involve only polynomials in $p$ and the trace of the $q$th and $(q/p^2)$th Hecke operators acting on spaces of cusp forms for the congruence groups $\operatorname{SL}_2 (\mathbb{Z}), \Gamma_0(2)$, and $\Gamma_0(4)$. The main tool we use is the Eichler-Selberg trace formula, which gives along the way a variation of a theorem of Birch on the distribution of rational point counts for elliptic curves with prescribed $2$-torsion over a fixed finite field. ","Mathematics - Number Theory ; Computer Science - Information Theory ; Primary 11T71, Secondary 11F25, 11G20, 94B27 ; ","Kaplan, Nathan ; Petrow, Ian ; "
http://arxiv.org/abs/1506.04496,"The Peano software - parallel, automaton-based, dynamically adaptive   grid traversals","  We discuss the design decisions, design alternatives and rationale behind the third generation of Peano, a framework for dynamically adaptive Cartesian meshes derived from spacetrees. Peano ties the mesh traversal to the mesh storage and supports only one element-wise traversal order resulting from space-filling curves. The user is not free to choose a traversal order herself. The traversal can exploit regular grid subregions and shared memory as well as distributed memory systems with almost no modifications to a serial application code. We formalize the software design by means of two interacting automata---one automaton for the multiscale grid traversal and one for the application-specific algorithmic steps. This yields a callback-based programming paradigm. We further sketch the supported application types and the two data storage schemes realized, before we detail high-performance computing aspects and lessons learned. Special emphasis is put on observations regarding the used programming idioms and algorithmic concepts. This transforms our report from a ""one way to implement things"" code description into a generic discussion and summary of some alternatives, rationale and design decisions to be made for any tree-based adaptive mesh refinement software. ",Computer Science - Mathematical Software ; ,"Weinzierl, Tobias ; "
http://arxiv.org/abs/1506.04497,Lower bounds for the dynamically defined measures,"  The dynamically defined measure (DDM) $\Phi$ arising from a finite measure $\phi_0$ on an initial $\sigma$-algebra on a set and an invertible map acting on the latter is considered. Several lower bounds for it are obtained and sufficient conditions for its positivity are deduced under the general assumption that there exists an invariant measure $\Lambda$ such that $\Lambda\ll\phi_0$.   In particular, DDMs arising from the Hellinger integral $\mathcal{J}_\alpha(\Lambda,\phi_0)\geq\mathcal{H}^{\alpha,0}(\Lambda,\phi_0)\geq\mathcal{H}_\alpha(\Lambda,\phi_0)$ are constructed with $\mathcal{H}_{0}\left(\Lambda,\phi_0\right)(Q) = \Phi(Q)$, $\mathcal{H}_{1}\left(\Lambda,\phi_0\right)(Q) = \Lambda(Q)$, and \[\Phi(Q)^{1-\alpha}\Lambda(Q)^{\alpha}\geq\mathcal{J}_{\alpha}\left(\Lambda,\phi_0\right)(Q)\] for all measurable $Q$ and $\alpha\in[0,1]$, and further computable lower bounds for them are obtained and analyzed. It is shown, in particular, that $(0,1)\owns\alpha\longmapsto\mathcal{H}_{\alpha}(\Lambda,\phi_0)(Q)$ is completely determined by the $\Lambda$-essential supremum of $d\Lambda/d\phi_0$ for all $0<\alpha<1$ if $\Lambda$ is ergodic, and if also a condition for the continuity at $0$ is satisfied, the above inequalities become equalities. In general, for every measurable $Q$, it is shown that $[0,1]\owns\alpha\longmapsto\mathcal{J}_{\alpha}(\Lambda,\phi_0)(Q)$ is log-convex, all one-sided derivatives of $(0,1)\owns\alpha\longmapsto\mathcal{H}^{\alpha,0}(\Lambda,\phi_0)(Q)$ and $(0,1)\owns\alpha\longmapsto\mathcal{J}_{\alpha}(\Lambda,\phi_0)(Q)$ are obtained, and some lower bounds for the functions by means of the derivatives are given. Some sufficient conditions for the continuity and a one-sided differentiability of $(0,1)\owns\alpha\longmapsto\mathcal{H}_{\alpha}(\Lambda,\phi_0)(Q)$ are provided. ","Mathematics - Dynamical Systems ; Computer Science - Information Theory ; Mathematical Physics ; 28A99, 37A60, 37A05, 82C05 ; ","Werner, Ivan ; "
http://arxiv.org/abs/1506.04651,Task-based adaptive multiresolution for time-space multi-scale   reaction-diffusion systems on multi-core architectures,"  A new solver featuring time-space adaptation and error control has been recently introduced to tackle the numerical solution of stiff reaction-diffusion systems. Based on operator splitting, finite volume adaptive multiresolution and high order time integrators with specific stability properties for each operator, this strategy yields high computational efficiency for large multidimensional computations on standard architectures such as powerful workstations. However, the data structure of the original implementation, based on trees of pointers, provides limited opportunities for efficiency enhancements, while posing serious challenges in terms of parallel programming and load balancing. The present contribution proposes a new implementation of the whole set of numerical methods including Radau5 and ROCK4, relying on a fully different data structure together with the use of a specific library, TBB, for shared-memory, task-based parallelism with work-stealing. The performance of our implementation is assessed in a series of test-cases of increasing difficulty in two and three dimensions on multi-core and many-core architectures, demonstrating high scalability. ","Computer Science - Numerical Analysis ; Computer Science - Distributed, Parallel, and Cluster Computing ; Mathematics - Analysis of PDEs ; Mathematics - Numerical Analysis ; ","Descombes, Stéphane ; Duarte, Max ; Dumont, Thierry ; Guillet, Thomas ; Louvet, Violaine ; Massot, Marc ; "
http://arxiv.org/abs/1506.04773,DistFlow Extensions for AC Transmission Systems,"  Convex relaxations of the power flow equations and, in particular, the Semi-Definite Programming (SDP), Second-Order Cone (SOC), and Convex DistFlow (CDF) relaxations, have attracted significant interest in recent years. Thus far, studies of the CDF model and its connection to the other relaxations have been limited to power distribution systems, which omit several parameters necessary for modeling transmission systems. To increase the applicability of the CDF relaxation, this paper develops an extended CDF model that is suitable for transmission systems by incorporating bus shunts, line charging, and transformers. Additionally, a theoretical result shows that the established equivalence of the SOC and CDF models for distribution systems also holds in this transmission system extension. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; ,"Coffrin, Carleton ; Hijazi, Hassan L. ; Van Hentenryck, Pascal ; "
http://arxiv.org/abs/1506.04972,A Unified Successive Pseudo-Convex Approximation Framework,"  In this paper, we propose a successive pseudo-convex approximation algorithm to efficiently compute stationary points for a large class of possibly nonconvex optimization problems. The stationary points are obtained by solving a sequence of successively refined approximate problems, each of which is much easier to solve than the original problem. To achieve convergence, the approximate problem only needs to exhibit a weak form of convexity, namely, pseudo-convexity. We show that the proposed framework not only includes as special cases a number of existing methods, for example, the gradient method and the Jacobi algorithm, but also leads to new algorithms which enjoy easier implementation and faster convergence speed. We also propose a novel line search method for nondifferentiable optimization problems, which is carried out over a properly constructed differentiable function with the benefit of a simplified implementation as compared to state-of-the-art line search techniques that directly operate on the original nondifferentiable objective function. The advantages of the proposed algorithm are shown, both theoretically and numerically, by several example applications, namely, MIMO broadcast channel capacity computation, energy efficiency maximization in massive MIMO systems and LASSO in sparse signal recovery. ",Mathematics - Optimization and Control ; Computer Science - Numerical Analysis ; ,"Yang, Yang ; Pesavento, Marius ; "
http://arxiv.org/abs/1506.05193,"Anxiety, Alcohol, and Academics: A Textual Analysis of Student Facebook   Confessions Pages","  What do college students reveal to their peers on social media under complete anonymity? Do their campus environments relate to the topics of their disclosure? To answer these questions, I analyze Facebook confessions pages. Popular on hundreds of college campuses, these pages allow students to anonymously post personal confessions on a public community forum. In this preliminary research note, I analyze several explanatory factors of online student confessional behavior. Aggregating nearly 200,000 confessions posts spanning a period of 3 years, I combine Latent Dirichlet Allocation (LDA) with human verification through Mechanical Turk to scalably identify topics in these online confessions. Where possible, I also link posts to real-world news events parsed from Twitter. I find that confessions mentioning socioeconomics as well as mental and physical health occur more often at top-ranking, expensive private colleges. While event-related confessions most often mention timely school-related events, many mention global and domestic events outside of the local campus sphere. Results suggest that undergraduates from different campuses disclose about topics such as race, socioeonomics, and politics differently, but in aggregate, post in similar patterns over time. Additionally, results confirm that anonymous Facebook confessors receive support for confessions on important, but taboo topics such as health and socioeconomic status. ",Computer Science - Social and Information Networks ; Computer Science - Computers and Society ; ,"Barari, Soubhik ; "
http://arxiv.org/abs/1506.05231,The Fractality of Polar and Reed-Muller Codes,"  The generator matrices of polar codes and Reed-Muller codes are obtained by selecting rows from the Kronecker product of a lower-triangular binary square matrix. For polar codes, the selection is based on the Bhattacharyya parameter of the row, which is closely related to the error probability of the corresponding input bit under sequential decoding. For Reed-Muller codes, the selection is based on the Hamming weight of the row. This work investigates the properties of the index sets pointing to those rows in the infinite blocklength limit. In particular, the Lebesgue measure, the Hausdorff dimension, and the self-similarity of these sets will be discussed. It is shown that these index sets have several properties that are common to fractals. ",Computer Science - Information Theory ; ,"Geiger, Bernhard C. ; "
http://arxiv.org/abs/1506.05855,Information-based inference for singular models and finite sample sizes:   A frequentist information criterion,"  In the information-based paradigm of inference, model selection is performed by selecting the candidate model with the best estimated predictive performance. The success of this approach depends on the accuracy of the estimate of the predictive complexity. In the large-sample-size limit of a regular model, the predictive performance is well estimated by the Akaike Information Criterion (AIC). However, this approximation can either significantly under or over-estimating the complexity in a wide range of important applications where models are either non-regular or finite-sample-size corrections are significant. We introduce an improved approximation for the complexity that is used to define a new information criterion: the Frequentist Information Criterion (QIC). QIC extends the applicability of information-based inference to the finite-sample-size regime of regular models and to singular models. We demonstrate the power and the comparative advantage of QIC in a number of example analyses. ","Statistics - Machine Learning ; Computer Science - Machine Learning ; Physics - Data Analysis, Statistics and Probability ; ","LaMont, Colin H. ; Wiggins, Paul A. ; "
http://arxiv.org/abs/1506.06011,A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with   Buffering,"  The purpose of this paper is to analyze the so-called back-off technique of the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to existing models, packets arriving when a station (or node) is in back-off state are not discarded, but are stored in a buffer of infinite capacity. As in previous studies, the key point of our analysis hinges on the assumption that the time on the channel is viewed as a random succession of transmission slots (whose duration corresponds to the length of a packet) and mini-slots during which the back-o? of the station is decremented. These events occur independently, with given probabilities. The state of a node is represented by a two-dimensional Markov chain in discrete-time, formed by the back-off counter and the number of packets at the station. Two models are proposed both of which are shown to cope reasonably well with the physical principles of the protocol. The stabillity (ergodicity) conditions are obtained and interpreted in terms of maximum throughput. Several approximations related to these models are also discussed. ","Computer Science - Performance ; Mathematics - Probability ; Primary 60J10, secondary 30D05, 30E99 ; ","Fayolle, Guy ; Muhlethaler, Paul ; "
http://arxiv.org/abs/1506.06055,Low PMEPR OFDM radar waveform design using the iterative least squares   algorithm,"  This letter considers waveform design of orthogonal frequency division multiplexing (OFDM) signal for radar applications, and aims at mitigating the envelope fluctuation in OFDM. A novel method is proposed to reduce the peak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate the fluctuation. The proposed method is based on the tone reservation approach, in which some bits or subcarriers of OFDM are allocated for decreasing PMEPR. We introduce the coefficient of variation of envelopes (CVE) as the cost function for waveform optimization, and develop an iterative least squares algorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is guaranteed that the cost function monotonically decreases by applying the iterative algorithm. Simulations demonstrate that the envelope is significantly smoothed by the proposed method. ",Electrical Engineering and Systems Science - Signal Processing ; Computer Science - Information Theory ; ,"Huang, Tianyao ; Zhao, Tong ; "
http://arxiv.org/abs/1506.06138,The evolution of lossy compression,"  In complex environments, there are costs to both ignorance and perception. An organism needs to track fitness-relevant information about its world, but the more information it tracks, the more resources it must devote to memory and processing. Rate-distortion theory shows that, when errors are allowed, remarkably efficient internal representations can be found by biologically-plausible hill-climbing mechanisms. We identify two regimes: a high-fidelity regime where perceptual costs scale logarithmically with environmental complexity, and a low-fidelity regime where perceptual costs are, remarkably, independent of the environment. When environmental complexity is rising, Darwinian evolution should drive organisms to the threshold between the high- and low-fidelity regimes. Organisms that code efficiently will find themselves able to make, just barely, the most subtle distinctions in their environment. ",Quantitative Biology - Neurons and Cognition ; Computer Science - Information Theory ; Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Physics - Physics and Society ; Quantitative Biology - Populations and Evolution ; ,"Marzen, Sarah E. ; DeDeo, Simon ; "
http://arxiv.org/abs/1506.06305,"Social media affects the timing, location, and severity of school   shootings","  Over the past two decades, school shootings within the United States have repeatedly devastated communities and shaken public opinion. Many of these attacks appear to be `lone wolf' ones driven by specific individual motivations, and the identification of precursor signals and hence actionable policy measures would thus seem highly unlikely. Here, we take a system-wide view and investigate the timing of school attacks and the dynamical feedback with social media. We identify a trend divergence in which college attacks have continued to accelerate over the last 25 years while those carried out on K-12 schools have slowed down. We establish the copycat effect in school shootings and uncover a statistical association between social media chatter and the probability of an attack in the following days. While hinting at causality, this relationship may also help mitigate the frequency and intensity of future attacks. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Garcia-Bernardo, J. ; Qi, H. ; Shultz, J. M. ; Cohen, A. M. ; Johnson, N. F. ; Dodds, P. S. ; "
http://arxiv.org/abs/1506.07094,pyMOR - Generic Algorithms and Interfaces for Model Order Reduction,"  Reduced basis methods are projection-based model order reduction techniques for reducing the computational complexity of solving parametrized partial differential equation problems. In this work we discuss the design of pyMOR, a freely available software library of model order reduction algorithms, in particular reduced basis methods, implemented with the Python programming language. As its main design feature, all reduction algorithms in pyMOR are implemented generically via operations on well-defined vector array, operator and discretization interface classes. This allows for an easy integration with existing open-source high-performance partial differential equation solvers without adding any model reduction specific code to these solvers. Besides an in-depth discussion of pyMOR's design philosophy and architecture, we present several benchmark results and numerical examples showing the feasibility of our approach. ","Computer Science - Mathematical Software ; Mathematics - Numerical Analysis ; 35-04, 35J20, 35L03, 65-04, 65N30, 65Y05, 68N01 ; ","Milk, René ; Rave, Stephan ; Schindler, Felix ; "
http://arxiv.org/abs/1506.07212,Elicitation Complexity of Statistical Properties,"  A property, or statistical functional, is said to be elicitable if it minimizes expected loss for some loss function. The study of which properties are elicitable sheds light on the capabilities and limits of empirical risk minimization. While several recent papers have asked which properties are elicitable, we instead advocate for a more nuanced question: how many dimensions are required to indirectly elicit a given property? This number is called the elicitation complexity of the property. We lay the foundation for a general theory of elicitation complexity, including several basic results about how elicitation complexity behaves, and the complexity of standard properties of interest. Building on this foundation, we establish several upper and lower bounds for the broad class of Bayes risks. We apply these results by proving tight complexity bounds, with respect to identifiable properties, for variance, financial risk measures, entropy, norms, and new properties of interest. We then show how some of these bounds can extend to other practical classes of properties, and conclude with a discussion of open directions. ",Computer Science - Machine Learning ; Mathematics - Optimization and Control ; Mathematics - Statistics Theory ; Quantitative Finance - Mathematical Finance ; ,"Frongillo, Rafael ; Kash, Ian A. ; "
http://arxiv.org/abs/1506.07437,The Truncated & Supplemented Pascal Matrix and Applications,"  In this paper, we introduce the $k\times n$ (with $k\leq n$) truncated, supplemented Pascal matrix which has the property that any $k$ columns form a linearly independent set. This property is also present in Reed-Solomon codes; however, Reed-Solomon codes are completely dense, whereas the truncated, supplemented Pascal matrix has multiple zeros. If the maximal-distance separable code conjecture is correct, then our matrix has the maximal number of columns (with the aformentioned property) that the conjecture allows. This matrix has applications in coding, network coding, and matroid theory. ","Mathematics - Combinatorics ; Computer Science - Information Theory ; 05B20, 05B35 ; ","Hua, M. ; Damelin, S. B. ; Sun, J. ; Yu, M. ; "
http://arxiv.org/abs/1506.07990,"Bisimulation and expressivity for conditional belief, degrees of belief,   and safe belief","  Plausibility models are Kripke models that agents use to reason about knowledge and belief, both of themselves and of each other. Such models are used to interpret the notions of conditional belief, degrees of belief, and safe belief. The logic of conditional belief contains that modality and also the knowledge modality, and similarly for the logic of degrees of belief and the logic of safe belief. With respect to these logics, plausibility models may contain too much information. A proper notion of bisimulation is required that characterises them. We define that notion of bisimulation and prove the required characterisations: on the class of image-finite and preimage-finite models (with respect to the plausibility relation), two pointed Kripke models are modally equivalent in either of the three logics, if and only if they are bisimilar. As a result, the information content of such a model can be similarly expressed in the logic of conditional belief, or the logic of degrees of belief, or that of safe belief. This, we found a surprising result. Still, that does not mean that the logics are equally expressive: the logics of conditional and degrees of belief are incomparable, the logics of degrees of belief and safe belief are incomparable, while the logic of safe belief is more expressive than the logic of conditional belief. In view of the result on bisimulation characterisation, this is an equally surprising result. We hope our insights may contribute to the growing community of formal epistemology and on the relation between qualitative and quantitative modelling. ",Computer Science - Artificial Intelligence ; Computer Science - Logic in Computer Science ; ,"Andersen, Mikkel Birkegaard ; Bolander, Thomas ; van Ditmarsch, Hans ; Jensen, Martin Holm ; "
http://arxiv.org/abs/1506.08009,Skopus: Mining top-k sequential patterns under leverage,"  This paper presents a framework for exact discovery of the top-k sequential patterns under Leverage. It combines (1) a novel definition of the expected support for a sequential pattern - a concept on which most interestingness measures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest. Our interestingness measure employs the partition approach. A pattern is interesting to the extent that it is more frequent than can be explained by assuming independence between any of the pairs of patterns from which it can be composed. The larger the support compared to the expectation under independence, the more interesting is the pattern. We build on these two elements to exactly extract the k sequential patterns with highest leverage, consistent with our definition of expected support. We conduct experiments on both synthetic data with known patterns and real-world datasets; both experiments confirm the consistency and relevance of our approach with regard to the state of the art. This article was published in Data Mining and Knowledge Discovery and is accessible at http://dx.doi.org/10.1007/s10618-016-0467-9. ",Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Petitjean, Francois ; Li, Tao ; Tatti, Nikolaj ; Webb, Geoffrey I. ; "
http://arxiv.org/abs/1506.08231,"A zero-sum monetary system, interest rates, and implications","  To the knowledge of the author, this is the first time it has been shown that interest rates that are extremely high by modern standards (100% and higher) are necessary within a zero-sum monetary system, and not just driven by greed. Extreme interest rates that appeared in various places and times reinforce the idea that hard money may have contributed to high rates of interest. Here a model is presented that examines the interest rate required to succeed as an investor in a zero-sum fixed quantity hard-money system. Even when the playing field is significantly tilted toward the investor, interest rates need to be much higher than expected. In a completely fair zero-sum system, an investor cannot break even without charging 100% interest. Even with a 5% advantage, an investor won't break even at 15% interest. From this it is concluded that what we consider usurious rates today are, within a hard-money system, driven by necessity.   Cryptocurrency is a novel form of hard-currency. The inability to virtualize the money creates a system close to zero-sum because of the limited supply design. Therefore, within the bounds of a cryptocurrency system that limits money creation, interest rates must rise to levels that the modern world considers usury. It is impossible, therefore, that a cryptocurrency that is not expandable could take over a modern economy and replace modern fiat currency. ","Computer Science - Computational Engineering, Finance, and Science ; J.4.1 ; ","Hanley, Brian P. ; "
http://arxiv.org/abs/1506.08235,Optimal Seed Solver: Optimizing Seed Selection in Read Mapping,"  Motivation: Optimizing seed selection is an important problem in read mapping. The number of non-overlapping seeds a mapper selects determines the sensitivity of the mapper while the total frequency of all selected seeds determines the speed of the mapper. Modern seed-and-extend mappers usually select seeds with either an equal and fixed-length scheme or with an inflexible placement scheme, both of which limit the potential of the mapper to select less frequent seeds to speed up the mapping process. Therefore, it is crucial to develop a new algorithm that can adjust both the individual seed length and the seed placement, as well as derive less frequent seeds.   Results: We present the Optimal Seed Solver (OSS), a dynamic programming algorithm that discovers the least frequently-occurring set of x seeds in an L-bp read in $O(x \times L)$ operations on average and in $O(x \times L^{2})$ operations in the worst case. We compared OSS against four state-of-the-art seed selection schemes and observed that OSS provides a 3-fold reduction of average seed frequency over the best previous seed selection optimizations. ","Computer Science - Computational Engineering, Finance, and Science ; Quantitative Biology - Genomics ; ","Xin, Hongyi ; Zhu, Richard ; Nahar, Sunny ; Emmons, John ; Pekhimenko, Gennady ; Kingsford, Carl ; Alkan, Can ; Mutlu, Onur ; "
http://arxiv.org/abs/1506.08238,Deciding Univariate Polynomial Problems Using Untrusted Certificates in   Isabelle/HOL,"  We present a proof procedure for univariate real polynomial problems in Isabelle/HOL. The core mathematics of our procedure is based on univariate cylindrical algebraic decomposition. We follow the approach of untrusted certificates, separating solving from verifying: efficient external tools perform expensive real algebraic computations, producing evidence that is formally checked within Isabelle's logic. This allows us to exploit highly-tuned computer algebra systems like Mathematica to guide our procedure without impacting the correctness of its results. We present experiments demonstrating the efficacy of this approach, in many cases yielding orders of magnitude improvements over previous methods. ",Computer Science - Logic in Computer Science ; ,"Li, Wenda ; Passmore, Grant Olney ; Paulson, Lawrence C. ; "
http://arxiv.org/abs/1506.08353,A note on patch-based low-rank minimization for fast image denoising,"  Patch-based low-rank minimization for image processing attracts much attention in recent years. The minimization of the matrix rank coupled with the Frobenius norm data fidelity can be solved by the hard thresholding filter with principle component analysis (PCA) or singular value decomposition (SVD). Based on this idea, we propose a patch-based low-rank minimization method for image denoising. The main denoising process is stated in three equivalent way: PCA, SVD and low-rank minimization. Compared to recent patch-based sparse representation methods, experiments demonstrate that the proposed method is rather rapid, and it is effective for a variety of natural grayscale images and color images, especially for texture parts in images. Further improvements of this method are also given. In addition, due to the simplicity of this method, we could provide an explanation of the choice of the threshold parameter, estimation of PSNR values, and give other insights into this method. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Hu, Haijuan ; Froment, Jacques ; Liu, Quansheng ; "
http://arxiv.org/abs/1506.08435,Large-scale Optimization-based Non-negative Computational Framework for   Diffusion Equations: Parallel Implementation and Performance Studies,"  It is well-known that the standard Galerkin formulation, which is often the formulation of choice under the finite element method for solving self-adjoint diffusion equations, does not meet maximum principles and the non-negative constraint for anisotropic diffusion equations. Recently, optimization-based methodologies that satisfy maximum principles and the non-negative constraint for steady-state and transient diffusion-type equations have been proposed. To date, these methodologies have been tested only on small-scale academic problems. The purpose of this paper is to systematically study the performance of the non-negative methodology in the context of high performance computing (HPC). PETSc and TAO libraries are, respectively, used for the parallel environment and optimization solvers. For large-scale problems, it is important for computational scientists to understand the computational performance of current algorithms available in these scientific libraries. The numerical experiments are conducted on the state-of-the-art HPC systems, and a single-core performance model is used to better characterize the efficiency of the solvers. Our studies indicate that the proposed non-negative computational framework for diffusion-type equations exhibits excellent strong scaling for real-world large-scale problems. ","Computer Science - Numerical Analysis ; Computer Science - Computational Engineering, Finance, and Science ; Computer Science - Performance ; ","Chang, J. ; Karra, S. ; Nakshatrala, K. B. ; "
http://arxiv.org/abs/1506.08544,Exact and approximate inference in graphical models: variable   elimination and beyond,"  Probabilistic graphical models offer a powerful framework to account for the dependence structure between variables, which is represented as a graph. However, the dependence between variables may render inference tasks intractable. In this paper we review techniques exploiting the graph structure for exact inference, borrowed from optimisation and computer science. They are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated. The so-called treewidth of the graph characterises this algorithmic complexity: low-treewidth graphs can be processed efficiently. The first message that we illustrate is therefore the idea that for inference in graphical model, the number of variables is not the limiting factor, and it is worth checking for the treewidth before turning to approximate methods. We show how algorithms providing an upper bound of the treewidth can be exploited to derive a 'good' elimination order enabling to perform exact inference. The second message is that when the treewidth is too large, algorithms for approximate inference linked to the principle of variable elimination, such as loopy belief propagation and variational approaches, can lead to accurate results while being much less time consuming than Monte-Carlo approaches. We illustrate the techniques reviewed in this article on benchmarks of inference problems in genetic linkage analysis and computer vision, as well as on hidden variables restoration in coupled Hidden Markov Models. ",Statistics - Machine Learning ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; ,"Peyrard, Nathalie ; Cros, Marie-Josée ; de Givry, Simon ; Franc, Alain ; Robin, Stéphane ; Sabbadin, Régis ; Schiex, Thomas ; Vignes, Matthieu ; "
http://arxiv.org/abs/1506.08547,Commutativity in the Algorithmic Lovasz Local Lemma,"  We consider the recent formulation of the Algorithmic Lov\'asz Local Lemma [10,2,3] for finding objects that avoid `bad features', or `flaws'. It extends the Moser-Tardos resampling algorithm [17] to more general discrete spaces. At each step the method picks a flaw present in the current state and goes to a new state according to some prespecified probability distribution (which depends on the current state and the selected flaw). However, it is less flexible than the Moser-Tardos method since [10,2,3] require a specific flaw selection rule, whereas [17] allows an arbitrary rule (and thus can potentially be implemented more efficiently).   We formulate a new ""commutativity"" condition, and prove that it is sufficient for an arbitrary rule to work. It also enables an efficient parallelization under an additional assumption. We then show that existing resampling oracles for perfect matchings and permutations do satisfy this condition. ",Computer Science - Data Structures and Algorithms ; ,"Kolmogorov, Vladimir ; "
http://arxiv.org/abs/1506.08752,On Tightly Bounding the Dubins Traveling Salesman's Optimum,"  The Dubins Traveling Salesman Problem (DTSP) has generated significant interest over the last decade due to its occurrence in several civil and military surveillance applications. Currently, there is no algorithm that can find an optimal solution to the problem. In addition, relaxing the motion constraints and solving the resulting Euclidean TSP (ETSP) provides the only lower bound available for the problem. However, in many problem instances, the lower bound computed by solving the ETSP is far below the cost of the feasible solutions obtained by some well-known algorithms for the DTSP. This article addresses this fundamental issue and presents the first systematic procedure for developing tight lower bounds for the DTSP. ",Mathematics - Optimization and Control ; Computer Science - Discrete Mathematics ; Computer Science - Data Structures and Algorithms ; Computer Science - Robotics ; ,"Manyam, Satyanarayana ; Rathinam, Sivakumar ; "
http://arxiv.org/abs/1506.08977,A comparative study of divisive hierarchical clustering algorithms,"  A general scheme for divisive hierarchical clustering algorithms is proposed. It is made of three main steps : first a splitting procedure for the subdivision of clusters into two subclusters, second a local evaluation of the bipartitions resulting from the tentative splits and, third, a formula for determining the nodes levels of the resulting dendrogram. A number of such algorithms is given. These algorithms are compared using the Goodman-Kruskal correlation coefficient. As a global criterion it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated to the given dissimilarities. Applied to a hundred of random data tables, these comparisons are in favor of two methods based on unusual ratio-type formulas for the splitting procedures, namely the Silhouette criterion and Dunn's criterion. These two criteria take into account both the within cluster and the between cluster mean dissimilarity. In general the results of these two algorithms are better than the classical Agglomerative Average Link method. ",Computer Science - Data Structures and Algorithms ; Quantitative Biology - Quantitative Methods ; 62-07 ; ,"Roux, Maurice ; "
http://arxiv.org/abs/1506.09140,Pure Strategies in Imperfect Information Stochastic Games,"  We consider imperfect information stochastic games where we require the players to use pure (i.e. non randomised) strategies. We consider reachability, safety, B\""uchi and co-B\""uchi objectives, and investigate the existence of almost-sure/positively winning strategies for the first player when the second player is perfectly informed or more informed than the first player. We obtain decidability results for positive reachability and almost-sure B\""uchi with optimal algorithms to decide existence of a pure winning strategy and to compute one if exists. We complete the picture by showing that positive safety is undecidable when restricting to pure strategies even if the second player is perfectly informed. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computer Science and Game Theory ; ,"Carayol, Arnaud ; Löding, Christof ; Serre, Olivier ; "
http://arxiv.org/abs/1506.09145,"Track Layouts, Layered Path Decompositions, and Leveled Planarity","  We investigate two types of graph layouts, track layouts and layered path decompositions, and the relations between their associated parameters track-number and layered pathwidth. We use these two types of layouts to characterize leveled planar graphs, which are the graphs with planar leveled drawings with no dummy vertices. It follows from the known NP-completeness of leveled planarity that track-number and layered pathwidth are also NP-complete, even for the smallest constant parameter values that make these parameters nontrivial. We prove that the graphs with bounded layered pathwidth include outerplanar graphs, Halin graphs, and squaregraphs, but that (despite having bounded track-number) series-parallel graphs do not have bounded layered pathwidth. Finally, we investigate the parameterized complexity of these layouts, showing that past methods used for book layouts do not work to parameterize the problem by treewidth or almost-tree number but that the problem is (non-uniformly) fixed-parameter tractable for tree-depth. ",Mathematics - Combinatorics ; Computer Science - Data Structures and Algorithms ; 05C10 ; ,"Bannister, Michael J. ; Devanny, William E. ; Dujmović, Vida ; Eppstein, David ; Wood, David R. ; "
http://arxiv.org/abs/1507.01088,Generic properties of subgroups of free groups and finite presentations,"  Asymptotic properties of finitely generated subgroups of free groups, and of finite group presentations, can be considered in several fashions, depending on the way these objects are represented and on the distribution assumed on these representations: here we assume that they are represented by tuples of reduced words (generators of a subgroup) or of cyclically reduced words (relators). Classical models consider fixed size tuples of words (e.g. the few-generator model) or exponential size tuples (e.g. Gromov's density model), and they usually consider that equal length words are equally likely. We generalize both the few-generator and the density models with probabilistic schemes that also allow variability in the size of tuples and non-uniform distributions on words of a given length.Our first results rely on a relatively mild prefix-heaviness hypothesis on the distributions, which states essentially that the probability of a word decreases exponentially fast as its length grows. Under this hypothesis, we generalize several classical results: exponentially generically a randomly chosen tuple is a basis of the subgroup it generates, this subgroup is malnormal and the tuple satisfies a small cancellation property, even for exponential size tuples. In the special case of the uniform distribution on words of a given length, we give a phase transition theorem for the central tree property, a combinatorial property closely linked to the fact that a tuple freely generates a subgroup. We then further refine our results when the distribution is specified by a Markovian scheme, and in particular we give a phase transition theorem which generalizes the classical results on the densities up to which a tuple of cyclically reduced words chosen uniformly at random exponentially generically satisfies a small cancellation property, and beyond which it presents a trivial group. ",Mathematics - Group Theory ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Bassino, Frédérique ; Nicaud, Cyril ; Weil, Pascal ; "
http://arxiv.org/abs/1507.01089,(Pure) transcendence bases in $\phi$-deformed shuffle bialgebras,"  Computations with integro-differential operators are often carried out in an associative algebra with unit, and they are essentially non-commutative computations. By adjoining a cocommutative co-product, one can have those operators perform act on a bialgebra isomorphic to an enveloping algebra. That gives an adequate framework for a computer-algebra implementation via monoidal factorization, (pure) transcendence bases and Poincar\'e--Birkhoff--Witt bases. In this paper, we systematically study these deformations, obtaining necessary and sufficient conditions for the operators to exist, and we give the most general cocommutative deformations of the shuffle co-product and an effective construction of pairs of bases in duality. The paper ends by the combinatorial setting of local systems of coordinates on the group of group-like series. ",Computer Science - Symbolic Computation ; Mathematical Physics ; Mathematics - Combinatorics ; Mathematics - Group Theory ; ,"Bui, Van Chiên ; Duchamp, Gérard H. E. ; Ngô, Quoc Hoan ; Minh, Vincel Hoang Ngoc ; Tollu, Christophe ; "
http://arxiv.org/abs/1507.01239,Experiments on Parallel Training of Deep Neural Network using Model   Averaging,"  In this work we apply model averaging to parallel training of deep neural network (DNN). Parallelization is done in a model averaging manner. Data is partitioned and distributed to different nodes for local model updates, and model averaging across nodes is done every few minibatches. We use multiple GPUs for data parallelization, and Message Passing Interface (MPI) for communication between nodes, which allows us to perform model averaging frequently without losing much time on communication. We investigate the effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and Restricted Boltzmann Machine (RBM) pretraining for parallel training in model-averaging framework, and explore the best setups in term of different learning rate schedules, averaging frequencies and minibatch sizes. It is shown that NG-SGD and RBM pretraining benefits parameter-averaging based model training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy loss. ",Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Su, Hang ; Chen, Haoyu ; "
http://arxiv.org/abs/1507.01279,Scan $B$-Statistic for Kernel Change-Point Detection,"  Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been used for this task which enjoy fewer assumptions on the distributions than the parametric approach and can handle high-dimensional data. In this paper we focus on the scenario when the amount of background data is large, and propose two related computationally efficient kernel-based statistics for change-point detection, which are inspired by the recently developed $B$-statistics. A novel theoretical result of the paper is the characterization of the tail probability of these statistics using the change-of-measure technique, which focuses on characterizing the tail of the detection statistics rather than obtaining its asymptotic distribution under the null distribution. Such approximations are crucial to control the false alarm rate, which corresponds to the significance level in offline change-point detection and the average-run-length in online change-point detection. Our approximations are shown to be highly accurate. Thus, they provide a convenient way to find detection thresholds for both offline and online cases without the need to resort to the more expensive simulations or bootstrapping. We show that our methods perform well on both synthetic data and real data. ",Computer Science - Machine Learning ; Mathematics - Statistics Theory ; Statistics - Machine Learning ; ,"Li, Shuang ; Xie, Yao ; Dai, Hanjun ; Song, Le ; "
http://arxiv.org/abs/1507.01345,DiffNodesets: An Efficient Structure for Fast Mining Frequent Itemsets,"  Mining frequent itemsets is an essential problem in data mining and plays an important role in many data mining applications. In recent years, some itemset representations based on node sets have been proposed, which have shown to be very efficient for mining frequent itemsets. In this paper, we propose DiffNodeset, a novel and more efficient itemset representation, for mining frequent itemsets. Based on the DiffNodeset structure, we present an efficient algorithm, named dFIN, to mining frequent itemsets. To achieve high efficiency, dFIN finds frequent itemsets using a set-enumeration tree with a hybrid search strategy and directly enumerates frequent itemsets without candidate generation under some case. For evaluating the performance of dFIN, we have conduct extensive experiments to compare it against with existing leading algorithms on a variety of real and synthetic datasets. The experimental results show that dFIN is significantly faster than these leading algorithms. ",Computer Science - Data Structures and Algorithms ; Computer Science - Databases ; ,"Deng, Zhi-Hong ; "
http://arxiv.org/abs/1507.01581,Joint Calibration for Semantic Segmentation,"  Semantic segmentation is the task of assigning a class-label to each pixel in an image. We propose a region-based semantic segmentation framework which handles both full and weak supervision, and addresses three common problems: (1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification. Our method outperforms the state-of-the-art on the popular SIFT Flow [18] dataset in both the fully and weakly supervised setting by a considerably margin (+6% and +10%, respectively). ",Computer Science - Computer Vision and Pattern Recognition ; 68T45 ; ,"Caesar, Holger ; Uijlings, Jasper ; Ferrari, Vittorio ; "
http://arxiv.org/abs/1507.01988,Automata and Quantum Computing,"  Quantum computing is a new model of computation, based on quantum physics. Quantum computers can be exponentially faster than conventional computers for problems such as factoring. Besides full-scale quantum computers, more restricted models such as quantum versions of finite automata have been studied. In this paper, we survey various models of quantum finite automata and their properties. We also provide some open questions and new directions for researchers.   Keywords: quantum finite automata, probabilistic finite automata, nondeterminism, bounded error, unbounded error, state complexity, decidability and undecidability, computational complexity ","Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Quantum Physics ; 68Q10, 68Q12, 68Q15, 68Q19, 68Q45 ; ","Ambainis, Andris ; Yakaryılmaz, Abuzer ; "
http://arxiv.org/abs/1507.02103,Measuring centrality by a generalization of degree,"  Network analysis has emerged as a key technique in communication studies, economics, geography, history and sociology, among others. A fundamental issue is how to identify key nodes, for which purpose a number of centrality measures have been developed. This paper proposes a new parametric family of centrality measures called generalized degree. It is based on the idea that a relationship to a more interconnected node contributes to centrality in a greater extent than a connection to a less central one. Generalized degree improves on degree by redistributing its sum over the network with the consideration of the global structure. Application of the measure is supported by a set of basic properties. A sufficient condition is given for generalized degree to be rank monotonic, excluding counter-intuitive changes in the centrality ranking after certain modifications of the network. The measure has a graph interpretation and can be calculated iteratively. Generalized degree is recommended to apply besides degree since it preserves most favourable attributes of degree, but better reflects the role of the nodes in the network and has an increased ability to distinguish among their importance. ","Computer Science - Social and Information Networks ; Physics - Physics and Society ; 15A06, 91D30 ; ","Csató, László ; "
http://arxiv.org/abs/1507.02178,"Directed multicut is W[1]-hard, even for four terminal pairs","  We prove that Multicut in directed graphs, parameterized by the size of the cutset, is W[1]-hard and hence unlikely to be fixed-parameter tractable even if restricted to instances with only four terminal pairs. This negative result almost completely resolves one of the central open problems in the area of parameterized complexity of graph separation problems, posted originally by Marx and Razgon [SIAM J. Comput. 43(2):355-388 (2014)], leaving only the case of three terminal pairs open.   Our gadget methodology allows us also to prove W[1]-hardness of the Steiner Orientation problem parameterized by the number of terminal pairs, resolving an open problem of Cygan, Kortsarz, and Nutov [SIAM J. Discrete Math. 27(3):1503-1513 (2013)]. ",Computer Science - Data Structures and Algorithms ; ,"Pilipczuk, Marcin ; Wahlström, Magnus ; "
http://arxiv.org/abs/1507.02180,A note on the definition of sliding block codes and the   Curtis-Hedlund-Lyndon Theorem,"  In this note we propose an alternative definition for sliding block codes between shift spaces. This definition coincides with the usual definition in the case that the shift space is defined on a finite alphabet, but it encompass a larger class of maps when the alphabet is infinite. In any case, the proposed definition keeps the idea that a sliding block code is a map with a local rule. Using this new definition we prove that the Curtis-Hedlund-Lyndon Theorem always holds for shift spaces over countable alphabets. ","Mathematics - Dynamical Systems ; Computer Science - Information Theory ; Mathematics - History and Overview ; 37B10, 37B15 ; ","Sobottka, Marcelo ; Gonçalves, Daniel ; "
http://arxiv.org/abs/1507.02184,"The ""art of trellis decoding"" is fixed-parameter tractable","  Given n subspaces of a finite-dimensional vector space over a fixed finite field $\mathbb F$, we wish to find a linear layout $V_1,V_2,\ldots,V_n$ of the subspaces such that $\dim((V_1+V_2+\cdots+V_i) \cap (V_{i+1}+\cdots+V_n))\le k$ for all i, such a linear layout is said to have width at most k. When restricted to 1-dimensional subspaces, this problem is equivalent to computing the trellis-width (or minimum trellis state-complexity) of a linear code in coding theory and computing the path-width of an $\mathbb F$-represented matroid in matroid theory.   We present a fixed-parameter tractable algorithm to construct a linear layout of width at most k, if it exists, for input subspaces of a finite-dimensional vector space over $\mathbb F$. As corollaries, we obtain a fixed-parameter tractable algorithm to produce a path-decomposition of width at most k for an input $\mathbb F$-represented matroid of path-width at most k, and a fixed-parameter tractable algorithm to find a linear rank-decomposition of width at most k for an input graph of linear rank-width at most k. In both corollaries, no such algorithms were known previously.   It was previously known that a fixed-parameter tractable algorithm exists for the decision version of the problem for matroid path-width, a theorem by Geelen, Gerards, and Whittle~(2002) implies that for each fixed finite field $\mathbb F$, there are finitely many forbidden $\mathbb F$-representable minors for the class of matroids of path-width at most k. An algorithm by Hlin\v{e}n\'y (2006) can detect a minor in an input $\mathbb F$-represented matroid of bounded branch-width. However, this indirect approach would not produce an actual path-decomposition. Our algorithm is the first one to construct such a path-decomposition and does not depend on the finiteness of forbidden minors. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Jeong, Jisu ; Kim, Eun Jung ; Oum, Sang-il ; "
http://arxiv.org/abs/1507.02250,Privacy-Preserving Nonlinear Observer Design Using Contraction Analysis,"  Real-time information processing applications such as those enabling a more intelligent infrastructure are increasingly focused on analyzing privacy-sensitive data obtained from individuals. To produce accurate statistics about the habits of a population of users of a system, this data might need to be processed through model-based estimators. Moreover, models of population dynamics, originating for example from epidemiology or the social sciences, are often necessarily nonlinear. Motivated by these trends, this paper presents an approach to design nonlinear privacy-preserving model-based observers, relying on additive input or output noise to give differential privacy guarantees to the individuals providing the input data. For the case of output perturbation, contraction analysis allows us to design convergent observers as well as set the level of privacy-preserving noise appropriately. Two examples illustrate the approach: estimating the edge formation probabilities in a dynamic social network, and syndromic surveillance relying on an epidemiological model. ",Computer Science - Systems and Control ; Computer Science - Information Theory ; Computer Science - Social and Information Networks ; ,"Ny, Jerome Le ; "
http://arxiv.org/abs/1507.02385,Towards Effective Codebookless Model for Image Classification,"  The bag-of-features (BoF) model for image classification has been thoroughly studied over the last decade. Different from the widely used BoF methods which modeled images with a pre-trained codebook, the alternative codebook free image modeling method, which we call Codebookless Model (CLM), attracted little attention. In this paper, we present an effective CLM that represents an image with a single Gaussian for classification. By embedding Gaussian manifold into a vector space, we show that the simple incorporation of our CLM into a linear classifier achieves very competitive accuracy compared with state-of-the-art BoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional Riemannian manifold, we further propose a joint learning method of low-rank transformation with support vector machine (SVM) classifier on the Gaussian manifold, in order to reduce computational and storage cost. To study and alleviate the side effect of background clutter on our CLM, we also present a simple yet effective partial background removal method based on saliency detection. Experiments are extensively conducted on eight widely used databases to demonstrate the effectiveness and efficiency of our CLM method. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Wang, Qilong ; Li, Peihua ; Zhang, Lei ; Zuo, Wangmeng ; "
http://arxiv.org/abs/1507.02908,On Existence and Properties of Approximate Pure Nash Equilibria in   Bandwidth Allocation Games,"  In \emph{bandwidth allocation games} (BAGs), the strategy of a player consists of various demands on different resources. The player's utility is at most the sum of these demands, provided they are fully satisfied. Every resource has a limited capacity and if it is exceeded by the total demand, it has to be split between the players. Since these games generally do not have pure Nash equilibria, we consider approximate pure Nash equilibria, in which no player can improve her utility by more than some fixed factor $\alpha$ through unilateral strategy changes. There is a threshold $\alpha_\delta$ (where $\delta$ is a parameter that limits the demand of each player on a specific resource) such that $\alpha$-approximate pure Nash equilibria always exist for $\alpha \geq \alpha_\delta$, but not for $\alpha < \alpha_\delta$. We give both upper and lower bounds on this threshold $\alpha_\delta$ and show that the corresponding decision problem is ${\sf NP}$-hard. We also show that the $\alpha$-approximate price of anarchy for BAGs is $\alpha+1$. For a restricted version of the game, where demands of players only differ slightly from each other (e.g. symmetric games), we show that approximate Nash equilibria can be reached (and thus also be computed) in polynomial time using the best-response dynamic. Finally, we show that a broader class of utility-maximization games (which includes BAGs) converges quickly towards states whose social welfare is close to the optimum. ",Computer Science - Computer Science and Game Theory ; ,"Drees, Maximilian ; Feldotto, Matthias ; Riechers, Sören ; Skopalik, Alexander ; "
http://arxiv.org/abs/1507.02954,An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel   Estimation,"  In this work we design a receiver that iteratively passes soft information between the channel estimation and data decoding stages. The receiver incorporates sparsity-based parametric channel estimation. State-of-the-art sparsity-based iterative receivers simplify the channel estimation problem by restricting the multipath delays to a grid. Our receiver does not impose such a restriction. As a result it does not suffer from the leakage effect, which destroys sparsity. Communication at near capacity rates in high SNR requires a large modulation order. Due to the close proximity of modulation symbols in such systems, the grid-based approximation is of insufficient accuracy. We show numerically that a state-of-the-art iterative receiver with grid-based sparse channel estimation exhibits a bit-error-rate floor in the high SNR regime. On the contrary, our receiver performs very close to the perfect channel state information bound for all SNR values. We also demonstrate both theoretically and numerically that parametric channel estimation works well in dense channels, i.e., when the number of multipath components is large and each individual component cannot be resolved. ",Computer Science - Information Theory ; Electrical Engineering and Systems Science - Signal Processing ; Statistics - Applications ; ,"Hansen, Thomas L. ; Jørgensen, Peter B. ; Badiu, Mihai-Alin ; Fleury, Bernard H. ; "
http://arxiv.org/abs/1507.02980,A hybrid mathematical model of collective motion under alignment and   chemotaxis,"  In this paper we propose and study a hybrid discrete in continuous mathematical model of collective motion under alignment and chemotaxis effect. Starting from the paper by Di Costanzo et al (2015a), in which the Cucker-Smale model (Cucker and Smale, 2007) was coupled with other cell mechanisms, to describe the cell migration and self-organization in the zebrafish lateral line primordium, we introduce a simplified model in which the coupling between an alignment and chemotaxis mechanism acts on a system of interacting particles. In particular we rely on a hybrid description in which the agents are discrete entities, while the chemoattractant is considered as a continuous signal. The proposed model is then studied both from an analytical and a numerical point of view. From the analytic point of view we prove, globally in time, existence and uniqueness of the solution. Then, the asymptotic behaviour of a linearised version of the system is investigated. Through a suitable Lyapunov functional we show that for $t\rightarrow +\infty$, the migrating aggregate exponentially converges to a state in which all the particles have a same position with zero velocity. Finally, we present a comparison between the analytical findings and some numerical results, concerning the behaviour of the full nonlinear system. ","Mathematics - Classical Analysis and ODEs ; Computer Science - Systems and Control ; Mathematics - Dynamical Systems ; Mathematics - Optimization and Control ; Quantitative Biology - Cell Behavior ; 82C22, 34D05, 92C17 ; ","Di Costanzo, Ezio ; Menci, Marta ; Messina, Eleonora ; Natalini, Roberto ; Vecchio, Antonia ; "
http://arxiv.org/abs/1507.03344,Entanglement in Reversible Quantum Computing,"  Similarly to the modelling of entanglement in the algebra of quantum computing, we also model entanglement as a synchronization among an event and its shadows in reversible quantum computing. We give the semantics and axioms of shadow constant for reversible quantum computing. ",Computer Science - Logic in Computer Science ; ,"Wang, Yong ; "
http://arxiv.org/abs/1507.03348,Deciding Circular-Arc Graph Isomorphism in Parameterized Logspace,"  We compute a canonical circular-arc representation for a given circular-arc (CA) graph which implies solving the isomorphism and recognition problem for this class. To accomplish this we split the class of CA graphs into uniform and non-uniform ones and employ a generalized version of the argument given by K\""obler et al (2013) that has been used to show that the subclass of Helly CA graphs can be canonized in logspace. For uniform CA graphs our approach works in logspace and in addition to that Helly CA graphs are a strict subset of uniform CA graphs. Thus our result is a generalization of the canonization result for Helly CA graphs. In the non-uniform case a specific set of ambiguous vertices arises. By choosing the parameter to be the cardinality of this set the obstacle can be solved by brute force. This leads to an O(k + log n) space algorithm to compute a canonical representation for non-uniform and therefore all CA graphs. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; G.2.2 ; ,"Chandoo, Maurice ; "
http://arxiv.org/abs/1507.03403,Time-Space Trade-offs for Triangulations and Voronoi Diagrams,"  Let $S$ be a planar $n$-point set. A triangulation for $S$ is a maximal plane straight-line graph with vertex set $S$. The Voronoi diagram for $S$ is the subdivision of the plane into cells such that all points in a cell have the same nearest neighbor in $S$. Classically, both structures can be computed in $O(n \log n)$ time and $O(n)$ space. We study the situation when the available workspace is limited: given a parameter $s \in \{1, \dots, n\}$, an $s$-workspace algorithm has read-only access to an input array with the points from $S$ in arbitrary order, and it may use only $O(s)$ additional words of $\Theta(\log n)$ bits for reading and writing intermediate data. The output should then be written to a write-only structure. We describe a deterministic $s$-workspace algorithm for computing an arbitrary triangulation of $S$ in time $O(n^2/s + n \log n \log s )$ and a randomized $s$-workspace algorithm for finding the Voronoi diagram of $S$ in expected time $O((n^2/s) \log s + n \log s \log^*s)$. ",Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; ,"Korman, Matias ; Mulzer, Wolfgang ; van Renssen, Andre ; Roeloffzen, Marcel ; Seiferth, Paul ; Stein, Yannik ; "
http://arxiv.org/abs/1507.03528,Visibility-Aware Optimal Contagion of Malware Epidemics,"  Recent innovations in the design of computer viruses have led to new trade-offs for the attacker. Multiple variants of a malware may spread at different rates and have different levels of visibility to the network. In this work we examine the optimal strategies for the attacker so as to trade off the extent of spread of the malware against the need for stealth. We show that in the mean-field deterministic regime, this spread-stealth trade-off is optimized by computationally simple single-threshold policies. Specifically, we show that only one variant of the malware is spread by the attacker at each time, as there exists a time up to which the attacker prioritizes maximizing the spread of the malware, and after which she prioritizes stealth. ",Computer Science - Cryptography and Security ; Computer Science - Systems and Control ; Mathematics - Optimization and Control ; ,"Eshghi, Soheil ; Sarkar, Saswati ; Venkatesh, Santosh S. ; "
http://arxiv.org/abs/1507.03838,Towards Green and Infinite Capacity in Wireless Communication Networks:   Beyond The Shannon Theorem,"  New and novel way for resources allocation in wireless communication has been proposed in this paper. Under this new method, it has been shown that the required power budget becomes independent of the number of served terminals in the downlink. However, the required power depends only of the coverage area, i.e. the channel losses at the cell boarder. Therefore, huge number (theoretically any number) of terminals could be supported concurrently at finite and small downlink power budget. This could be very useful to support the downlink signalling channels in HSPA+, LTE, and 5G. It can be very useful also to support huge D2D communication downlinks. Moreover, and based on the same concept, a new system configuration for a single link point-to-point communication has been presented. With this new configuration, the achieved data rate becomes independent of the required transmit power. This means that any data rate can be achieved at the target BER and with small and finite transmit power. This seems violating with some major results of the Shannon theorem. This issue will be discussed in details in this article. ",Computer Science - Information Theory ; ,"Elmusrati, Mohammed ; "
http://arxiv.org/abs/1507.04027,Fuzzy Overlapping Community Quality Metrics,"  Modularity is widely used to effectively measure the strength of the disjoint community structure found by community detection algorithms. Several overlapping extensions of modularity were proposed to measure the quality of overlapping community structure. However, all these extensions differ just in the way they define the belonging coefficient and belonging function. Yet, there is lack of systematic comparison of different extensions. To fill this gap, we overview overlapping extensions of modularity and generalize them with a uniform definition enabling application of different belonging coefficients and belonging functions to select the best. In addition, we extend localized modularity, modularity density, and eight local community quality metrics to enable their usages for overlapping communities. The experimental results on a large number of real networks and synthetic networks using overlapping extensions of modularity, overlapping modularity density, and local metrics show that the best results are obtained when the product of the belonging coefficients of two nodes is used as the belonging function. Moreover, the results may be used to guide researchers on which metrics to adopt when measuring the quality of overlapping community structure. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Chen, Mingming ; Szymanski, Boleslaw K. ; "
http://arxiv.org/abs/1507.04394,Time-triggered smart transducer networks,"  The time-triggered approach is a well-suited approach for building distributed hard real-time systems. Since many applications of transducer networks have real-time requirements, a time-triggered communication interface for smart transducers is desirable, however such a time-triggered interface must still support features for monitoring, maintenance, plug-and-play, etc. The approach of the OMG Smart Transducer Interface consists of clusters of time-triggered smart transducer nodes that contain special interfaces supporting configuration, diagnostics, and maintenance without affecting the deterministic real-time communication. This paper discusses the applicability of the time-triggered approach for smart transducer networks and presents a case study application of a time-triggered smart transducer network. ",Computer Science - Networking and Internet Architecture ; ,"Elmenreich, Wilfried ; "
http://arxiv.org/abs/1507.04500,The Complexity of All-switches Strategy Improvement,"  Strategy improvement is a widely-used and well-studied class of algorithms for solving graph-based infinite games. These algorithms are parameterized by a switching rule, and one of the most natural rules is ""all switches"" which switches as many edges as possible in each iteration. Continuing a recent line of work, we study all-switches strategy improvement from the perspective of computational complexity. We consider two natural decision problems, both of which have as input a game $G$, a starting strategy $s$, and an edge $e$. The problems are: 1.) The edge switch problem, namely, is the edge $e$ ever switched by all-switches strategy improvement when it is started from $s$ on game $G$? 2.) The optimal strategy problem, namely, is the edge $e$ used in the final strategy that is found by strategy improvement when it is started from $s$ on game $G$? We show $\mathtt{PSPACE}$-completeness of the edge switch problem and optimal strategy problem for the following settings: Parity games with the discrete strategy improvement algorithm of V\""oge and Jurdzi\'nski; mean-payoff games with the gain-bias algorithm [14,37]; and discounted-payoff games and simple stochastic games with their standard strategy improvement algorithms. We also show $\mathtt{PSPACE}$-completeness of an analogous problem to edge switch for the bottom-antipodal algorithm for finding the sink of an Acyclic Unique Sink Orientation on a cube. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; Computer Science - Computer Science and Game Theory ; Computer Science - Logic in Computer Science ; ,"Fearnley, John ; Savani, Rahul ; "
http://arxiv.org/abs/1507.04606,Extension of Modularity Density for Overlapping Community Structure,"  Modularity is widely used to effectively measure the strength of the disjoint community structure found by community detection algorithms. Although several overlapping extensions of modularity were proposed to measure the quality of overlapping community structure, there is lack of systematic comparison of different extensions. To fill this gap, we overview overlapping extensions of modularity to select the best. In addition, we extend the Modularity Density metric to enable its usage for overlapping communities. The experimental results on four real networks using overlapping extensions of modularity, overlapping modularity density, and six other community quality metrics show that the best results are obtained when the product of the belonging coefficients of two nodes is used as the belonging function. Moreover, our experiments indicate that overlapping modularity density is a better measure of the quality of overlapping community structure than other metrics considered. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Chen, Mingming ; Kuzmin, Konstantin ; Szymanski, Boleslaw K. ; "
http://arxiv.org/abs/1507.05014,Compositional Construction of Approximate Abstractions of Interconnected   Control Systems,"  We consider a compositional construction of approximate abstractions of interconnected control systems. In our framework, an abstraction acts as a substitute in the controller design process and is itself a continuous control system. The abstraction is related to the concrete control system via a so-called simulation function: a Lyapunov-like function, which is used to establish a quantitative bound between the behavior of the approximate abstraction and the concrete system. In the first part of the paper, we provide a small gain type condition that facilitates the compositional construction of an abstraction of an interconnected control system together with a simulation function from the abstractions and simulation functions of the individual subsystems. In the second part of the paper, we restrict our attention to linear control system and characterize simulation functions in terms of controlled invariant, externally stabilizable subspaces. Based on those characterizations, we propose a particular scheme to construct abstractions for linear control systems. We illustrate the compositional construction of an abstraction on an interconnected system consisting of four linear subsystems. We use the abstraction as a substitute to synthesize a controller to enforce a certain linear temporal logic specification. ",Mathematics - Optimization and Control ; Computer Science - Systems and Control ; 93C10 ; I.2.8 ; ,"Rungger, Matthias ; Zamani, Majid ; "
http://arxiv.org/abs/1507.05305,Computability and Complexity of Categorical Structures,"  We examine various categorical structures that can and cannot be constructed. We show that total computable functions can be mimicked by constructible functors. More generally, whatever can be done by a Turing machine can be constructed by categories. Since there are infinitary constructions in category theory, it is shown that category theory is strictly more powerful than Turing machines. In particular, categories can solve the Halting Problem for Turing machines. We also show that categories can solve any problem in the arithmetic hierarchy. ","Computer Science - Computational Complexity ; Computer Science - Logic in Computer Science ; Mathematics - Category Theory ; 18-XX, 03-XX, 03D15, 68Q30, 03D10 ; ","Yanofsky, Noson S. ; "
http://arxiv.org/abs/1507.05492,Parallel Toolkit for Measuring the Quality of Network Community   Structure,"  Many networks display community structure which identifies groups of nodes within which connections are denser than between them. Detecting and characterizing such community structure, which is known as community detection, is one of the fundamental issues in the study of network systems. It has received a considerable attention in the last years. Numerous techniques have been developed for both efficient and effective community detection. Among them, the most efficient algorithm is the label propagation algorithm whose computational complexity is O(|E|). Although it is linear in the number of edges, the running time is still too long for very large networks, creating the need for parallel community detection. Also, computing community quality metrics for community structure is computationally expensive both with and without ground truth. However, to date we are not aware of any effort to introduce parallelism for this problem. In this paper, we provide a parallel toolkit to calculate the values of such metrics. We evaluate the parallel algorithms on both distributed memory machine and shared memory machine. The experimental results show that they yield a significant performance gain over sequential execution in terms of total running time, speedup, and efficiency. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Chen, Mingming ; Liu, Sisi ; Szymanski, Boleslaw K. ; "
http://arxiv.org/abs/1507.05500,The Complexity of Non-Iterated Probabilistic Justification Logic,"  The logic PJ is a probabilistic logic defined by adding (non-iterated) probability operators to the basic justification logic J. In this paper we establish upper and lower bounds for the complexity of the derivability problem in the logic PJ. The main result of the paper is that the complex- ity of the derivability problem in PJ remains the same as the complexity of the derivability problem in the underlying logic J, which is {\Pi}p2-complete. This implies hat the probability operators do not increase the complex- ity of the logic, although they arguably enrich the expressiveness of the language. ",Computer Science - Logic in Computer Science ; ,"Kokkinis, Ioannis ; "
http://arxiv.org/abs/1507.05819,On Identifying Anomalies in Tor Usage with Applications in Detecting   Internet Censorship,"  We develop a means to detect ongoing per-country anomalies in the daily usage metrics of the Tor anonymous communication network, and demonstrate the applicability of this technique to identifying likely periods of internet censorship and related events. The presented approach identifies contiguous anomalous periods, rather than daily spikes or drops, and allows anomalies to be ranked according to deviation from expected behaviour.   The developed method is implemented as a running tool, with outputs published daily by mailing list. This list highlights per-country anomalous Tor usage, and produces a daily ranking of countries according to the level of detected anomalous behaviour. This list has been active since August 2016, and is in use by a number of individuals, academics, and NGOs as an early warning system for potential censorship events.   We focus on Tor, however the presented approach is more generally applicable to usage data of other services, both individually and in combination. We demonstrate that combining multiple data sources allows more specific identification of likely Tor blocking events. We demonstrate the our approach in comparison to existing anomaly detection tools, and against both known historical internet censorship events and synthetic datasets. Finally, we detail a number of significant recent anomalous events and behaviours identified by our tool. ",Computer Science - Computers and Society ; Computer Science - Machine Learning ; Computer Science - Networking and Internet Architecture ; ,"Wright, Joss ; Darer, Alexander ; Farnan, Oliver ; "
http://arxiv.org/abs/1507.05880,A study of the classification of low-dimensional data with supervised   manifold learning,"  Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets. ",Computer Science - Machine Learning ; ,"Vural, Elif ; Guillemot, Christine ; "
http://arxiv.org/abs/1507.05995,A Warm Restart Strategy for Solving Sudoku by Sparse Optimization   Methods,"  This paper is concerned with the popular Sudoku problem. We proposed a warm restart strategy for solving Sudoku puzzles, based on the sparse optimization technique. Furthermore, we defined a new difficulty level for Sudoku puzzles. The efficiency of the proposed method is tested using a dataset of Sudoku puzzles, and the numerical results show that the accurate recovery rate can be enhanced from 84%+ to 99%+ using the L1 sparse optimization method. ","Mathematics - Optimization and Control ; Computer Science - Distributed, Parallel, and Cluster Computing ; 90C05, 90C25 ; ","Tang, Yuchao ; Wu, Zhenggang ; Zhu, Chuanxi ; "
http://arxiv.org/abs/1507.06011,Using coarse GPS data to quantify city-scale transportation system   resilience to extreme events,"  This article proposes a method to quantitatively measure the resilience of transportation systems using GPS data from taxis. The granularity of the GPS data necessary for this analysis is relatively coarse; it only requires coordinates for the beginning and end of trips, the metered distance, and the total travel time. The method works by computing the historical distribution of pace (normalized travel times) between various regions of a city and measuring the pace deviations during an unusual event. This method is applied to a dataset of nearly 700 million taxi trips in New York City, which is used to analyze the transportation infrastructure resilience to Hurricane Sandy. The analysis indicates that Hurricane Sandy impacted traffic conditions for more than five days, and caused a peak delay of two minutes per mile. Practically, it identifies that the evacuation caused only minor disruptions, but significant delays were encountered during the post-disaster reentry process. Since the implementation of this method is very efficient, it could potentially be used as an online monitoring tool, representing a first step toward quantifying city scale resilience with coarse GPS data. ",Physics - Physics and Society ; Computer Science - Social and Information Networks ; ,"Donovan, Brian ; Work, Daniel B. ; "
http://arxiv.org/abs/1507.06111,COMs: Complexes of Oriented Matroids,"  In his seminal 1983 paper, Jim Lawrence introduced lopsided sets and featured them as asymmetric counterparts of oriented matroids, both sharing the key property of strong elimination. Moreover, symmetry of faces holds in both structures as well as in the so-called affine oriented matroids. These two fundamental properties (formulated for covectors) together lead to the natural notion of ""conditional oriented matroid"" (abbreviated COM). These novel structures can be characterized in terms of three cocircuits axioms, generalizing the familiar characterization for oriented matroids. We describe a binary composition scheme by which every COM can successively be erected as a certain complex of oriented matroids, in essentially the same way as a lopsided set can be glued together from its maximal hypercube faces. A realizable COM is represented by a hyperplane arrangement restricted to an open convex set. Among these are the examples formed by linear extensions of ordered sets, generalizing the oriented matroids corresponding to the permutohedra. Relaxing realizability to local realizability, we capture a wider class of combinatorial objects: we show that non-positively curved Coxeter zonotopal complexes give rise to locally realizable COMs. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Bandelt, Hans-Juergen ; Chepoi, Victor ; Knauer, Kolja ; "
http://arxiv.org/abs/1507.06576,Abstract Gringo,"  This paper defines the syntax and semantics of the input language of the ASP grounder GRINGO. The definition covers several constructs that were not discussed in earlier work on the semantics of that language, including intervals, pools, division of integers, aggregates with non-numeric values, and lparse-style aggregate expressions. The definition is abstract in the sense that it disregards some details related to representing programs by strings of ASCII characters. It serves as a specification for GRINGO from Version 4.5 on. ",Computer Science - Programming Languages ; ,"Gebser, Martin ; Harrison, Amelia ; Kaminski, Roland ; Lifschitz, Vladimir ; Schaub, Torsten ; "
http://arxiv.org/abs/1507.06616,Robust Monotone Submodular Function Maximization,"  We consider a robust formulation, introduced by Krause et al. (2008), of the classical cardinality constrained monotone submodular function maximization problem, and give the first constant factor approximation results. The robustness considered is w.r.t. adversarial removal of up to $\tau$ elements from the chosen set. For the fundamental case of $\tau=1$, we give a deterministic $(1-1/e)-1/\Theta(m)$ approximation algorithm, where $m$ is an input parameter and number of queries scale as $O(n^{m+1})$. In the process, we develop a deterministic $(1-1/e)-1/\Theta(m)$ approximate greedy algorithm for bi-objective maximization of (two) monotone submodular functions. Generalizing the ideas and using a result from Chekuri et al. (2010), we show a randomized $(1-1/e)-\epsilon$ approximation for constant $\tau$ and $\epsilon\leq \frac{1}{\tilde{\Omega}(\tau)}$, making $O(n^{1/\epsilon^3})$ queries. Further, for $\tau\ll \sqrt{k}$, we give a fast and practical 0.387 algorithm. Finally, we also give a black box result result for the much more general setting of robust maximization subject to an Independence System. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; Mathematics - Optimization and Control ; ,"Orlin, James B. ; Schulz, Andreas S. ; Udwani, Rajan ; "
http://arxiv.org/abs/1507.07045,A Truth Serum for Large-Scale Evaluations,"  A major challenge in obtaining large-scale evaluations, e.g., product or service reviews on online platforms, labeling images, grading in online courses, etc., is that of eliciting honest responses from agents in the absence of verifiability. We propose a new reward mechanism with strong incentive properties applicable in a wide variety of such settings. This mechanism has a simple and intuitive output agreement structure: an agent gets a reward only if her response for an evaluation matches that of her peer. But instead of the reward being the same across different answers, it is inversely proportional to a popularity index of each answer. This index is a second order population statistic that captures how frequently two agents performing the same evaluation agree on the particular answer. Rare agreements thus earn a higher reward than agreements that are relatively more common.   In the regime where there are a large number of evaluation tasks, we show that truthful behavior is a strict Bayes-Nash equilibrium of the game induced by the mechanism. Further, we show that the truthful equilibrium is approximately optimal in terms of expected payoffs to the agents across all symmetric equilibria, where the approximation error vanishes in the number of evaluation tasks. Moreover, under a mild condition on strategy space, we show that any symmetric equilibrium that gives a higher expected payoff than the truthful equilibrium must be close to being fully informative if the number of evaluations is large. These last two results are driven by a new notion of an agreement measure that is shown to be monotonic in information loss. This notion and its properties are of independent interest. ",Computer Science - Computer Science and Game Theory ; Computer Science - Artificial Intelligence ; Computer Science - Multiagent Systems ; ,"Kamble, Vijay ; Marn, David ; Shah, Nihar ; Parekh, Abhay ; Ramachandran, Kannan ; "
http://arxiv.org/abs/1507.07091,The Wiretap Channel with Generalized Feedback: Secure Communication and   Key Generation,"  It is a well-known fact that feedback does not increase the capacity of point-to-point memoryless channels, however, its effect in secure communications is not fully understood yet. In this work, an achievable scheme for the wiretap channel with generalized feedback is presented. This scheme, which uses the feedback signal to generate a shared secret key between the legitimate users, encrypts the message to be sent at the bit level. New capacity results for a class of channels are provided, as well as some new insights into the secret key agreement problem. Moreover, this scheme recovers previously reported rate regions from the literature, and thus it can be seen as a generalization that unifies several results in the field. ",Computer Science - Information Theory ; ,"Bassi, Germán ; Piantanida, Pablo ; Shamai, Shlomo ; "
http://arxiv.org/abs/1507.07402,Partial resampling to approximate covering integer programs,"  We consider column-sparse covering integer programs, a generalization of set cover, which have attracted a long line of research developing (randomized) approximation algorithms. We develop a new rounding scheme based on the Partial Resampling variant of the Lov\'{a}sz Local Lemma developed by Harris & Srinivasan (2013).   This achieves an approximation ratio of $1 + \frac{\ln (\Delta_1+1)}{a_{\text{min}}} + O\Big( \log(1 + \sqrt{ \frac{\log (\Delta_1+1)}{a_{\text{min}}}}) \Big)$, where $a_{\text{min}}$ is the minimum covering constraint and $\Delta_1$ is the maximum $\ell_1$-norm of any column of the covering matrix (whose entries are scaled to lie in $[0,1]$). When there are additional constraints on the sizes of the variables, we show an approximation ratio of $\ln \Delta_0 + O(\log \log \Delta_0)$ (where $\Delta_0$ is the maximum number of non-zero entries in any column of the covering matrix). We also show nearly-matching inapproximability and integrality-gap lower bounds. These results improve asymptotically, in several different ways, over results shown by Srinivasan (2006) and Kolliopoulos & Young (2005).   We show also that the rounding process leads to negative correlation among the variables. This allows us to automatically handle multi-criteria programs, efficiently achieving approximation ratios which are essentially equivalent to the single-criterion case and apply even when the number of criteria is large. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Chen, Antares ; Harris, David G. ; Srinivasan, Aravind ; "
http://arxiv.org/abs/1507.07583,Mapping Auto-context Decision Forests to Deep ConvNets for Semantic   Segmentation,"  We consider the task of pixel-wise semantic segmentation given a small set of labeled training images. Among two of the most popular techniques to address this task are Decision Forests (DF) and Neural Networks (NN). In this work, we explore the relationship between two special forms of these techniques: stacked DFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our main contribution is to show that Auto-context can be mapped to a deep ConvNet with novel architecture, and thereby trained end-to-end. This mapping can be used as an initialization of a deep ConvNet, enabling training even in the face of very limited amounts of training data. We also demonstrate an approximate mapping back from the refined ConvNet to a second stacked DF, with improved performance over the original. We experimentally verify that these mappings outperform stacked DFs for two different applications in computer vision and biology: Kinect-based body part labeling from depth images, and somite segmentation in microscopy images of developing zebrafish. Finally, we revisit the core mapping from a Decision Tree (DT) to a NN, and show that it is also possible to map a fuzzy DT, with sigmoidal split decisions, to a NN. This addresses multiple limitations of the previous mapping, and yields new insights into the popular Rectified Linear Unit (ReLU), and more recently proposed concatenated ReLU (CReLU), activation functions. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Richmond, David L. ; Kainmueller, Dagmar ; Yang, Michael Y. ; Myers, Eugene W. ; Rother, Carsten ; "
http://arxiv.org/abs/1507.07622,Fully-Online Suffix Tree and Directed Acyclic Word Graph Construction   for Multiple Texts,"  We consider construction of the suffix tree and the directed acyclic word graph (DAWG) indexing data structures for a collection $\mathcal{T}$ of texts, where a new symbol may be appended to any text in $\mathcal{T} = \{T_1, \ldots, T_K\}$, at any time. This fully-online scenario, which arises in dynamically indexing multi-sensor data, is a natural generalization of the long solved semi-online text indexing problem, where texts $T_1, \ldots, T_{k}$ are permanently fixed before the next text $T_{k+1}$ is processed for each $1 \leq k < K$. We present fully-online algorithms that construct the suffix tree and the DAWG for $\mathcal{T}$ in $O(N \log \sigma)$ time and $O(N)$ space, where $N$ is the total lengths of the strings in $\mathcal{T}$ and $\sigma$ is their alphabet size. The standard explicit representation of the suffix tree leaf edges and some DAWG edges must be relaxed in our fully-online scenario, since too many updates on these edges are required in the worst case. Instead, we provide access to the updated suffix tree leaf edge labels and the DAWG edges to be redirected via auxiliary data structures, in $O(\log \sigma)$ time per added character. ",Computer Science - Data Structures and Algorithms ; ,"Takagi, Takuya ; Inenaga, Shunsuke ; Arimura, Hiroki ; Breslauer, Dany ; Hendrian, Diptarama ; "
http://arxiv.org/abs/1507.07855,Generalized Twisted Gabidulin Codes,"  Let $\mathcal{C}$ be a set of $m$ by $n$ matrices over $\mathbb{F}_q$ such that the rank of $A-B$ is at least $d$ for all distinct $A,B\in \mathcal{C}$. Suppose that $m\leqslant n$. If $\#\mathcal{C}= q^{n(m-d+1)}$, then $\mathcal{C}$ is a maximum rank distance (MRD for short) code. Until 2016, there were only two known constructions of MRD codes for arbitrary $1<d<m-1$. One was found by Delsarte (1978) and Gabidulin (1985) independently, and it was later generalized by Kshevetskiy and Gabidulin (2005). We often call them (generalized) Gabidulin codes. Another family was recently obtained by Sheekey (2016), and its elements are called twisted Gabidulin codes. In the same paper, Sheekey also proposed a generalization of the twisted Gabidulin codes. However the equivalence problem for it is not considered, whence it is not clear whether there exist new MRD codes in this generalization. We call the members of this putative larger family generalized twisted Gabidulin codes. In this paper, we first compute the Delsarte duals and adjoint codes of them, then we completely determine the equivalence between different generalized twisted Gabidulin codes. In particular, it can be proven that, up to equivalence, generalized Gabidulin codes and twisted Gabidulin codes are both proper subsets of this family. ",Mathematics - Combinatorics ; Computer Science - Information Theory ; ,"Lunardon, Guglielmo ; Trombetti, Rocco ; Zhou, Yue ; "
http://arxiv.org/abs/1507.08861,Mobile Multi-View Object Image Search,"  High user interaction capability of mobile devices can help improve the accuracy of mobile visual search systems. At query time, it is possible to capture multiple views of an object from different viewing angles and at different scales with the mobile device camera to obtain richer information about the object compared to a single view and hence return more accurate results. Motivated by this, we developed a mobile multi-view object image search system, using a client-server architecture. Multi-view images of objects acquired by the mobile clients are processed and local features are sent to the server, which combines the query image representations with early/late fusion methods based on bag-of-visual-words and sends back the query results. We performed a comprehensive analysis of early and late fusion approaches using various similarity functions, on an existing single view and a new multi-view object image database. The experimental results show that multi-view search provides significantly better retrieval accuracy compared to single view search. ",Computer Science - Multimedia ; Computer Science - Computer Vision and Pattern Recognition ; ,"Calisir, Fatih ; Bastan, Muhammet ; Ulusoy, Ozgur ; Gudukbay, Ugur ; "
http://arxiv.org/abs/1508.00217,Indexing of CNN Features for Large Scale Image Search,"  The convolutional neural network (CNN) features can give a good description of image content, which usually represent images with unique global vectors. Although they are compact compared to local descriptors, they still cannot efficiently deal with large-scale image retrieval due to the cost of the linear incremental computation and storage. To address this issue, we build a simple but effective indexing framework based on inverted table, which significantly decreases both the search time and memory usage. In addition, several strategies are fully investigated under an indexing framework to adapt it to CNN features and compensate for quantization errors. First, we use multiple assignment for the query and database images to increase the probability of relevant images' co-existing in the same Voronoi cells obtained via the clustering algorithm. Then, we introduce embedding codes to further improve precision by removing false matches during a search. We demonstrate that by using hashing schemes to calculate the embedding codes and by changing the ranking rule, indexing framework speeds can be greatly improved. Extensive experiments conducted on several unsupervised and supervised benchmarks support these results and the superiority of the proposed indexing framework. We also provide a fair comparison between the popular CNN features. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Liu, Ruoyu ; Zhao, Yao ; Wei, Shikui ; Yang, Yi ; "
http://arxiv.org/abs/1508.00315,Low-rank spectral optimization via gauge duality,"  Various applications in signal processing and machine learning give rise to highly structured spectral optimization problems characterized by low-rank solutions. Two important examples that motivate this work are optimization problems from phase retrieval and from blind deconvolution, which are designed to yield rank-1 solutions. An algorithm is described that is based on solving a certain constrained eigenvalue optimization problem that corresponds to the gauge dual which, unlike the more typical Lagrange dual, has an especially simple constraint. The dominant cost at each iteration is the computation of rightmost eigenpairs of a Hermitian operator. A range of numerical examples illustrate the scalability of the approach. ","Mathematics - Optimization and Control ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; 90C15, 90C25 ; ","Friedlander, Michael P. ; Macedo, Ives ; "
http://arxiv.org/abs/1508.00510,Proving the Turing Universality of Oritatami Co-Transcriptional Folding   (Full Text),"  We study the oritatami model for molecular co-transcriptional folding. In oritatami systems, the transcript (the ""molecule"") folds as it is synthesized (transcribed), according to a local energy optimisation process, which is similar to how actual biomolecules such as RNA fold into complex shapes and functions as they are transcribed. We prove that there is an oritatami system embedding universal computation in the folding process itself.   Our result relies on the development of a generic toolbox, which is easily reusable for future work to design complex functions in oritatami systems. We develop ""low-level"" tools that allow to easily spread apart the encoding of different ""functions"" in the transcript, even if they are required to be applied at the same geometrical location in the folding. We build upon these low-level tools, a programming framework with increasing levels of abstraction, from encoding of instructions into the transcript to logical analysis. This framework is similar to the hardware-to-algorithm levels of abstractions in standard algorithm theory. These various levels of abstractions allow to separate the proof of correctness of the global behavior of our system, from the proof of correctness of its implementation. Thanks to this framework, we were able to computerize the proof of correctness of its implementation and produce certificates, in the form of a relatively small number of proof trees, compact and easily readable and checkable by human, while encapsulating huge case enumerations. We believe this particular type of certificates can be generalized to other discrete dynamical systems, where proofs involve large case enumerations as well. ",Computer Science - Computational Geometry ; Computer Science - Computational Complexity ; Computer Science - Emerging Technologies ; ,"Geary, Cody ; Meunier, Pierre-Étienne ; Schabanel, Nicolas ; Seki, Shinnosuke ; "
http://arxiv.org/abs/1508.00641,Episodic Multi-armed Bandits,"  We introduce a new class of reinforcement learning methods referred to as {\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\em episodes}, each composed of several {\em steps}, in which it chooses an action and observes a feedback signal. Moreover, in each step, it can take a special action, called the $stop$ action, that ends the current episode. After the $stop$ action is taken, the learner collects a terminal reward, and observes the costs and terminal rewards associated with each step of the episode. The goal of the learner is to maximize its cumulative gain (i.e., the terminal reward minus costs) over all episodes by learning to choose the best sequence of actions based on the feedback. First, we define an {\em oracle} benchmark, which sequentially selects the actions that maximize the expected immediate gain. Then, we propose our online learning algorithm, named {\em FeedBack Adaptive Learning} (FeedBAL), and prove that its regret with respect to the benchmark is bounded with high probability and increases logarithmically in expectation. Moreover, the regret only has polynomial dependence on the number of steps, actions and states. eMAB can be used to model applications that involve humans in the loop, ranging from personalized medical screening to personalized web-based education, where sequences of actions are taken in each episode, and optimal behavior requires adapting the chosen actions based on the feedback. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Tekin, Cem ; van der Schaar, Mihaela ; "
http://arxiv.org/abs/1508.00688,Accelerating R with high performance linear algebra libraries,"  Linear algebra routines are basic building blocks for the statistical software. In this paper we analyzed how can we can improve R performance for matrix computations. We benchmarked few matrix operations using the standard linear algebra libraries included in the R distribution and high performance libraries like OpenBLAS, GotoBLAS and MKL. Our tests showed the the best results are obtained with the MKL library, the other two libraries having similar performances, but lower than MKL ",Computer Science - Mathematical Software ; 68N99 ; H.3.4 ; ,"Oancea, Bogdan ; Andrei, Tudorel ; Dragoescu, Raluca Mariana ; "
http://arxiv.org/abs/1508.00945,Structured Prediction: From Gaussian Perturbations to Linear-Time   Principled Algorithms,"  Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Honorio, Jean ; Jaakkola, Tommi ; "
http://arxiv.org/abs/1508.01014,Computational complexity of distance edge labeling,"  The problem of Distance Edge Labeling is a variant of Distance Vertex Labeling (also known as $L_{2,1}$ labeling) that has been studied for more than twenty years and has many applications, such as frequency assignment.   The Distance Edge Labeling problem asks whether the edges of a given graph can be labeled such that the labels of adjacent edges differ by at least two and the labels of edges of distance two differ by at least one. Labels are chosen from the set $\{0,1,\dots,\lambda\}$ for $\lambda$ fixed.   We present a full classification of its computational complexity - a dichotomy between the polynomially solvable cases and the remaining cases which are NP-complete. We characterise graphs with $\lambda \le 4$ which leads to a polynomial-time algorithm recognizing the class and we show NP-completeness for $\lambda \ge 5$ by several reductions from Monotone Not All Equal 3-SAT. ",Computer Science - Discrete Mathematics ; Computer Science - Computational Complexity ; G.2.2 ; F.2.2 ; ,"Knop, Dušan ; Masařík, Tomáš ; "
http://arxiv.org/abs/1508.01059,Offline and Online Models of Budget Allocation for Maximizing Influence   Spread,"  The research of influence propagation in social networks via word-of-mouth processes has been given considerable attention in recent years. Arguably, the most fundamental problem in this domain is influence maximization, where the goal is to identify a seed set of individuals that can trigger a large cascade of influence in the network. While there has been significant progress regarding this problem and its variants, one basic shortcoming of the models is that they lack the flexibility in the way the budget is allocated to individuals. Indeed, budget allocation is a critical issue in advertising and viral marketing. Taking the other point of view, known models allowing flexible budget allocation do not take into account the influence spread in the network. We introduce a generalized model that captures both budgets and influence propagation simultaneously.   For the offline setting, we identify a large family of budget-based propagation functions that admit tight approximation guarantee. This family extends most of the previously studied influence models, including the well-known Triggering model. We establish that any function in this family implies an instance of a monotone submodular function maximization over the integer lattice subject to a knapsack constraint. This problem is known to admit an optimal (1-1/e)-approximation. We also study the price of anarchy of the multi-player game that extends the model and establish tight results.   For the online setting, in which an unknown subset of agents arrive in a random order and the algorithm needs to make an irrevocable budget allocation in each step, we develop a 1/(15e)-competitive algorithm. This setting extends the secretary problem, and its variant, the submodular knapsack secretary problem. Notably, our algorithm improves over the best known approximation for the latter problem, even though it applies to a more general setting. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; Computer Science - Social and Information Networks ; ,"Avigdor-Elgrabli, Noa ; Blocq, Gideon ; Gamzu, Iftah ; Orda, Ariel ; "
http://arxiv.org/abs/1508.01775,Cascading Power Outages Propagate Locally in an Influence Graph that is   not the Actual Grid Topology,"  In a cascading power transmission outage, component outages propagate non-locally, after one component outages, the next failure may be very distant, both topologically and geographically. As a result, simple models of topological contagion do not accurately represent the propagation of cascades in power systems. However, cascading power outages do follow patterns, some of which are useful in understanding and reducing blackout risk. This paper describes a method by which the data from many cascading failure simulations can be transformed into a graph-based model of influences that provides actionable information about the many ways that cascades propagate in a particular system. The resulting ""influence graph"" model is Markovian, in that component outage probabilities depend only on the outages that occurred in the prior generation. To validate the model we compare the distribution of cascade sizes resulting from $n-2$ contingencies in a $2896$ branch test case to cascade sizes in the influence graph. The two distributions are remarkably similar. In addition, we derive an equation with which one can quickly identify modifications to the proposed system that will substantially reduce cascade propagation. With this equation one can quickly identify critical components that can be improved to substantially reduce the risk of large cascading blackouts. ",Physics - Physics and Society ; Computer Science - Systems and Control ; ,"Hines, Paul D. H. ; Dobson, Ian ; Rezaei, Pooya ; "
http://arxiv.org/abs/1508.01841,Hypergraph coloring up to condensation,"  Improving a result of Dyer, Frieze and Greenhill [Journal of Combinatorial Theory, Series B, 2015], we determine the $q$-colorability threshold in random $k$-uniform hypergraphs up to an additive error of $\ln 2+\varepsilon_q$, where $\lim_{q\to\infty}\varepsilon_q=0$. The new lower bound on the threshold matches the ""condensation phase transition"" predicted by statistical physics considerations [Krzakala et al., PNAS 2007]. ",Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; ,"Ayre, Peter ; Coja-Oghlan, Amin ; Greenhill, Catherine ; "
http://arxiv.org/abs/1508.01950,Defending Against Stealthy Attacks on Multiple Nodes with Limited   Resources: A Game-Theoretic Analysis,"  Stealthy attacks are a major cyber-security threat. In practice, both attackers and defenders have resource constraints that could limit their capabilities. Hence, to develop robust defense strategies, a promising approach is to utilize game theory to understand the fundamental trade-offs involved. Previous works in this direction, however, mainly focus on the single-node case without considering strict resource constraints. In this paper, a game-theoretic model for protecting a system of multiple nodes against stealthy attacks is proposed. We consider the practical setting where the frequencies of both attack and defense are constrained by limited resources, and an asymmetric feedback structure where the attacker can fully observe the states of nodes while largely hiding its actions from the defender. We characterize the best response strategies for both attacker and defender in the space of both non-adaptive and adaptive strategies, and study the Nash Equilibria of the game. We further study a sequential game where the defender first announces its strategy and the attacker then responds accordingly, and design an algorithm that finds a nearly optimal strategy for the defender to commit to. ",Computer Science - Computer Science and Game Theory ; ,"Zhang, Ming ; Zheng, Zizhan ; Shroff, Ness B. ; "
http://arxiv.org/abs/1508.01993,Improving Decision Analytics with Deep Learning: The Case of Financial   Disclosures,"  Decision analytics commonly focuses on the text mining of financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%. ",Statistics - Machine Learning ; Computer Science - Computation and Language ; Computer Science - Machine Learning ; ,"Feuerriegel, Stefan ; Fehrer, Ralph ; "
http://arxiv.org/abs/1508.02340,The Pontryagin Maximum Principle for Nonlinear Infinite Horizon Optimal   Control Problems with State Constraints,"  The famous proof of the Pontryagin maximum principle for control problems on a finite horizon bases on the needle variation technique, as well as the separability concept of cones created by disturbances of the trajectories. In this preprint, we develop this method for infinite horizon optimal control problems. The results are necessary conditions for a strong local minimizer in form of the Pontryagin maximum principle, Arrow type sufficiency conditions and the validity of diverse transversality conditions. ","Mathematics - Optimization and Control ; Computer Science - Systems and Control ; 34, 46, 49 ; ","Tauchnitz, Nico ; "
http://arxiv.org/abs/1508.02521,Topology Control of wireless sensor network using Quantum Inspired   Genetic algorithm,"  In this work, an evolving Linked Quantum register has been introduced, which are group vector of binary pair of genes, which in its local proximity represent those nodes that will have high connectivity and keep the energy consumption at low, and which are taken into account for topology control. The register works in higher dimension. Here order-2 Quantum inspired genetic algorithm has been used and also higher order can be used to achieve greater versatility in topology control of nodes. Numerical result has been obtained, analysis is done as how the result has previously been obtained with Quantum genetic algorithm and results are compared too. For future work, factor is hinted which would exploit the algorithm to work in more computational intensive problem. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Networking and Internet Architecture ; ,"Ullah, Sajid ; Wahid, Mussarat ; "
http://arxiv.org/abs/1508.02570,A Combinatorial Model of Interference in Frequency Hopping Schemes,"  In a frequency hopping (FH) scheme users communicate simultaneously using FH sequences defined on the same set of frequency channels. An FH sequence specifies the frequency channel to be used as communication progresses. Much of the research on the performance of FH schemes is based on either pairwise mutual interference or adversarial interference but not both. In this paper, we evaluate the performance of an FH scheme with respect to both group-wise mutual interference and adversarial interference (jamming), bearing in mind that more than two users may be transmitting simultaneously in the presence of a jammer. We establish a correspondence between a cover-free code and an FH scheme. This gives a lower bound on the transmission capacity. Furthermore, we specify a jammer model and consider what additional properties a cover-free code should have to resist the jammer. We demonstrate that a purely combinatorial approach is inadequate against such a jammer, but that with the use of pseudorandomness, we can have a system that has high throughput as well as security against jamming. ","Computer Science - Information Theory ; Mathematics - Combinatorics ; 94A05, 94A55, 94B60 ; ","Nyirenda, Mwawi M. ; Ng, Siaw-Lynn ; Martin, Keith M. ; "
http://arxiv.org/abs/1508.02759,Technical Note: Split Algorithm in O(n) for the Capacitated Vehicle   Routing Problem,"  The Split algorithm is an essential building block of route-first cluster-second heuristics and modern genetic algorithms for vehicle routing problems. The algorithm is used to partition a solution, represented as a giant tour without occurrences of the depot, into separate routes with minimum cost. As highlighted by the recent survey of [Prins, Lacomme and Prodhon, Transport Res. C (40), 179-200], no less than 70 recent articles use this technique. In the vehicle routing literature, Split is usually assimilated to the search for a shortest path in a directed acyclic graph $\mathcal{G}$ and solved in $O(nB)$ using Bellman's algorithm, where $n$ is the number of delivery points and $B$ is the average number of feasible routes that start with a given customer in the giant tour. Some linear-time algorithms are also known for this problem as a consequence of a Monge property of $\mathcal{G}$. In this article, we highlight a stronger property of this graph, leading to a simple alternative algorithm in $O(n)$. Experimentally, we observe that the approach is faster than the classical Split for problem instances of practical size. We also extend the method to deal with a limited fleet and soft capacity constraints. ",Computer Science - Data Structures and Algorithms ; ,"Vidal, Thibaut ; "
http://arxiv.org/abs/1508.02793,A generalized Goulden-Jackson cluster method and lattice path   enumeration,"  The Goulden-Jackson cluster method is a powerful tool for obtaining generating functions for counting words in a free monoid by occurrences of a set of subwords. We introduce a generalization of the cluster method for monoid networks, which generalize the combinatorial framework of free monoids. As a sample application of the generalized cluster method, we compute bivariate and multivariate generating functions counting Motzkin paths---both with height bounded and unbounded---by statistics corresponding to the number of occurrences of various subwords, yielding both closed-form and continued fraction formulae. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Computer Science - Formal Languages and Automata Theory ; 05A15, 05A05, 05C50, 68R05 ; ","Zhuang, Yan ; "
http://arxiv.org/abs/1508.03117,Optimized Projections for Compressed Sensing via Direct Mutual Coherence   Minimization,"  Compressed Sensing (CS) is a novel technique for simultaneous signal sampling and compression based on the existence of a sparse representation of signal and a projected dictionary $PD$, where $P\in\mathbb{R}^{m\times d}$ is the projection matrix and $D\in\mathbb{R}^{d\times n}$ is the dictionary. To exactly recover the signal with a small number of measurements $m$, the projected dictionary $PD$ is expected to be of low mutual coherence. Several previous methods attempt to find the projection $P$ such that the mutual coherence of $PD$ can be as low as possible. However, they do not minimize the mutual coherence directly and thus their methods are far from optimal. Also the solvers they used lack of the convergence guarantee and thus there has no guarantee on the quality of their obtained solutions. This work aims to address these issues. We propose to find an optimal projection by minimizing the mutual coherence of $PD$ directly. This leads to a nonconvex nonsmooth minimization problem. We then approximate it by smoothing and solve it by alternate minimization. We further prove the convergence of our algorithm. To the best of our knowledge, this is the first work which directly minimizes the mutual coherence of the projected dictionary with a convergence guarantee. Numerical experiments demonstrate that the proposed method can recover sparse signals better than existing methods. ",Computer Science - Information Theory ; Computer Science - Machine Learning ; ,"Lu, Canyi ; Li, Huan ; Lin, Zhouchen ; "
http://arxiv.org/abs/1508.03263,Logic Programming with Macro Connectives,"  Logic programming such as Prolog is often sequential and slow because each execution step processes only a single, $micro$ connective. To fix this problem, we propose to use $macro$ connectives as the means of improving both readability and performance. ",Computer Science - Programming Languages ; ,"Kwon, Keehang ; "
http://arxiv.org/abs/1508.03269,A New Approach to an Old Problem: The Reconstruction of a Go Game   through a Series of Photographs,"  Given a series of photographs taken during a Go game, we describe the techniques we successfully employ for pinpointing the grid lines of the Go board and for tracking their small movements between consecutive photographs; then we discuss how to approximate the location and orientation of the observer's point of view, in order to compensate for projection effects. Finally we describe the different criteria that jointly form the algorithm for stones' detection, thus enabling us to automatically reconstruct the whole move sequence. ",Computer Science - Computer Vision and Pattern Recognition ; I.2.10 ; I.4.8 ; I.5.5 ; ,"Corsolini, Mario ; Carta, Andrea ; "
http://arxiv.org/abs/1508.03401,Binary Compressive Sensing via Analog Fountain Coding,"  In this paper, a compressive sensing (CS) approach is proposed for sparse binary signals' compression and reconstruction based on analog fountain codes (AFCs). In the proposed scheme, referred to as the analog fountain compressive sensing (AFCS), each measurement is generated from a linear combination of L randomly selected binary signal elements with real weight coefficients. The weight coefficients are chosen from a finite weight set and L, called measurement degree, is obtained based on a predefined degree distribution function. We propose a simple verification based reconstruction algorithm for the AFCS in the noiseless case. The proposed verification based decoder is analyzed through SUM-OR tree analytical approach and an optimization problem is formulated to find the optimum measurement degree to minimize the number of measurements required for the reconstruction of binary sparse signals. We show that in the AFCS, the number of required measurements is of O(-n log(1-k/n)), where n is the signal length and k is the signal sparsity level. We then consider the signal reconstruction of AFCS in the presence of additive white Gaussian noise (AWGN) and the standard message passing decoder is then used for the signal recovery. Simulation results show that the AFCS can perfectly recover all non-zero elements of the sparse binary signal with a significantly reduced number of measurements, compared to the conventional binary CS and L1-minimization approaches in a wide range of signal to noise ratios (SNRs). Finally, we show a practical application of the AFCS for the sparse event detection in wireless sensor networks (WSNs), where the sensors' readings can be treated as measurements from the CS point of view. ",Computer Science - Information Theory ; ,"Shirvanimoghaddam, Mahyar ; Li, Yonghui ; Vucetic, Branka ; Yuan, Jinhong ; "
http://arxiv.org/abs/1508.03773,No acute tetrahedron is an 8-reptile,"  An $r$-gentiling is a dissection of a shape into $r \geq 2$ parts which are all similar to the original shape. An $r$-reptiling is an $r$-gentiling of which all parts are mutually congruent. This article shows that no acute tetrahedron is an $r$-gentile or $r$-reptile for any $r < 9$, by showing that no acute spherical diangle can be dissected into less than nine acute spherical triangles. ",Computer Science - Computational Geometry ; Mathematics - Metric Geometry ; ,"Haverkort, Herman ; "
http://arxiv.org/abs/1508.03837,Incorporating User Interaction into Imperative Languages,"  In this paper, we present two new forms of the $write$ statement: one of the form $write(x);G$ where $G$ is a statement and the other of the form $write(x);D$ where $D$ is a module. The former is a generalization of traditional $write$ statement and is quite useful. The latter is useful for implementing interactive modules. ",Computer Science - Programming Languages ; ,"Kwon, Keehang ; "
http://arxiv.org/abs/1508.03878,A Pessimistic Approximation for the Fisher Information Measure,"  The problem of determining the intrinsic quality of a signal processing system with respect to the inference of an unknown deterministic parameter $\theta$ is considered. While the Fisher information measure $F(\theta)$ forms a classical tool for such a problem, direct computation of the information measure can become difficult in various situations. For the estimation theoretic performance analysis of nonlinear measurement systems, the form of the likelihood function can make the calculation of the information measure $F(\theta)$ challenging. In situations where no closed-form expression of the statistical system model is available, the analytical derivation of $F(\theta)$ is not possible at all. Based on the Cauchy-Schwarz inequality, we derive an alternative information measure $S(\theta)$. It provides a lower bound on the Fisher information $F(\theta)$ and has the property of being evaluated with the mean, the variance, the skewness and the kurtosis of the system model at hand. These entities usually exhibit good mathematical tractability or can be determined at low-complexity by real-world measurements in a calibrated setup. With various examples, we show that $S(\theta)$ provides a good conservative approximation for $F(\theta)$ and outline different estimation theoretic problems where the presented information bound turns out to be useful. ",Computer Science - Information Theory ; Electrical Engineering and Systems Science - Signal Processing ; ,"Stein, Manuel ; Nossek, Josef A. ; "
http://arxiv.org/abs/1508.03891,REBA: A Refinement-Based Architecture for Knowledge Representation and   Reasoning in Robotics,"  This paper describes an architecture for robots that combines the complementary strengths of probabilistic graphical models and declarative programming to represent and reason with logic-based and probabilistic descriptions of uncertainty and domain knowledge. An action language is extended to support non-boolean fluents and non-deterministic causal laws. This action language is used to describe tightly-coupled transition diagrams at two levels of granularity, with a fine-resolution transition diagram defined as a refinement of a coarse-resolution transition diagram of the domain. The coarse-resolution system description, and a history that includes (prioritized) defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this action. A probabilistic representation of the uncertainty in sensing and actuation is then included in this zoomed fine-resolution system description, and used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract action as a sequence of concrete actions, with the corresponding observations being recorded in the coarse-resolution history and used for subsequent reasoning. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains. ",Computer Science - Robotics ; Computer Science - Artificial Intelligence ; Computer Science - Logic in Computer Science ; ,"Sridharan, Mohan ; Gelfond, Michael ; Zhang, Shiqi ; Wyatt, Jeremy ; "
http://arxiv.org/abs/1508.04095,Algorithmic Aspects of Optimal Channel Coding,"  A central question in information theory is to determine the maximum success probability that can be achieved in sending a fixed number of messages over a noisy channel. This was first studied in the pioneering work of Shannon who established a simple expression characterizing this quantity in the limit of multiple independent uses of the channel. Here we consider the general setting with only one use of the channel. We observe that the maximum success probability can be expressed as the maximum value of a submodular function. Using this connection, we establish the following results:   1. There is a simple greedy polynomial-time algorithm that computes a code achieving a (1-1/e)-approximation of the maximum success probability. Moreover, for this problem it is NP-hard to obtain an approximation ratio strictly better than (1-1/e).   2. Shared quantum entanglement between the sender and the receiver can increase the success probability by a factor of at most 1/(1-1/e). In addition, this factor is tight if one allows an arbitrary non-signaling box between the sender and the receiver.   3. We give tight bounds on the one-shot performance of the meta-converse of Polyanskiy-Poor-Verdu. ",Computer Science - Information Theory ; Computer Science - Data Structures and Algorithms ; Quantum Physics ; ,"Barman, Siddharth ; Fawzi, Omar ; "
http://arxiv.org/abs/1508.04417,Social Influence in the Concurrent Diffusion of Information and   Behaviors in Online Social Networks,"  The emergence of online social networks has greatly facilitated the diffusion of information and behaviors. While the two diffusion processes are often intertwined, ""talking the talk"" does not necessarily mean ""walking the talk""--those who share information about an action may not actually participate in it. We do not know if the diffusion of information and behaviors are similar, or if social influence plays an equally important role in these processes. Integrating text mining, social network analyses, and survival analysis, this research examines the concurrent spread of information and behaviors related to the Ice Bucket Challenge on Twitter. We show that the two processes follow different patterns. Unilateral social influence contributes to the diffusion of information, but not to the diffusion of behaviors; bilateral influence conveyed via the communication process is a significant and positive predictor of the diffusion of behaviors, but not of information. These results have implications for theories of social influence, social networks, and contagion. ",Computer Science - Social and Information Networks ; Physics - Physics and Society ; ,"Zhao, Kang ; Wang, Shiyao ; Vasi, Ion B. ; Zhang, Qi ; "
http://arxiv.org/abs/1508.04596,Large scale three-dimensional topology optimisation of heat sinks cooled   by natural convection,"  This work presents the application of density-based topology optimisation to the design of three-dimensional heat sinks cooled by natural convection. The governing equations are the steady-state incompressible Navier-Stokes equations coupled to the thermal convection-diffusion equation through the Bousinessq approximation. The fully coupled non-linear multiphysics system is solved using stabilised trilinear equal-order finite elements in a parallel framework allowing for the optimisation of large scale problems with order of 40-330 million state degrees of freedom. The flow is assumed to be laminar and several optimised designs are presented for Grashof numbers between $10^3$ and $10^6$. Interestingly, it is observed that the number of branches in the optimised design increases with increasing Grashof numbers, which is opposite to two-dimensional optimised designs. ","Physics - Fluid Dynamics ; Computer Science - Computational Engineering, Finance, and Science ; ","Alexandersen, Joe ; Sigmund, Ole ; Aage, Niels ; "
http://arxiv.org/abs/1508.04606,Distributed Event-Triggered Control for Asymptotic Synchronization of   Dynamical Networks,"  This paper studies synchronization of dynamical networks with event-based communication. Firstly, two estimators are introduced into each node, one to estimate its own state, and the other to estimate the average state of its neighbours. Then, with these two estimators, a distributed event-triggering rule (ETR) with a dwell time is designed such that the network achieves synchronization asymptotically with no Zeno behaviours. The designed ETR only depends on the information that each node can obtain, and thus can be implemented in a decentralized way. ",Computer Science - Systems and Control ; Computer Science - Multiagent Systems ; Mathematics - Dynamical Systems ; ,"Liu, Tao ; Cao, Ming ; De Persis, Claudio ; Hendrickx, Julien M. ; "
http://arxiv.org/abs/1508.04720,Quickest Detection for Changes in Maximal kNN Coherence of Random   Matrices,"  This paper addresses the problem of quickest detection of a change in the maximal coherence between columns of a $n\times p$ random matrix based on a sequence of matrix observations having a single unknown change point. The random matrix is assumed to have identically distributed rows and the maximal coherence is defined as the largest of the $p \choose 2$ correlation coefficients associated with any row. Likewise, the $k$ nearest neighbor (kNN) coherence is defined as the $k$-th largest of these correlation coefficients. The forms of the pre- and post-change distributions of the observed matrices are assumed to belong to the family of elliptically contoured densities with sparse dispersion matrices but are otherwise unknown. A non-parametric stopping rule is proposed that is based on the maximal k-nearest neighbor sample coherence between columns of each observed random matrix. This is a summary statistic that is related to a test of the existence of a hub vertex in a sample correlation graph having a degree at least $k$. Performance bounds on the delay and false alarm performance of the proposed stopping rule are obtained in the purely high dimensional regime where $p\rightarrow \infty$ and $n$ is fixed. When the pre-change dispersion matrix is diagonal it is shown that, among all functions of the proposed summary statistic, the proposed stopping rule is asymptotically optimal under a minimax quickest change detection (QCD) model as the stopping threshold approaches infinity. The theory developed also applies to sequential hypothesis testing and fixed sample size tests. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; Mathematics - Probability ; ,"Banerjee, Taposh ; Firouzi, Hamed ; Hero III, Alfred O. ; "
http://arxiv.org/abs/1508.05117,The backtracking survey propagation algorithm for solving random K-SAT   problems,"  Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with $K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables. ",Computer Science - Computational Complexity ; Computer Science - Artificial Intelligence ; Computer Science - Data Structures and Algorithms ; ,"Marino, Raffaele ; Parisi, Giorgio ; Ricci-Tersenghi, Federico ; "
http://arxiv.org/abs/1508.05232,Variable-mixing parameter quantized kernel robust mixed-norm algorithms   for combating impulsive interference,"  Although the kernel robust mixed-norm (KRMN) algorithm outperforms the kernel least mean square (KLMS) algorithm in impulsive noise, it still has two major problems as follows: (1) The choice of the mixing parameter in the KRMN is crucial to obtain satisfactory performance. (2) The structure of the KRMN algorithm grows linearly as the iteration goes on, thus it has high computational complexity and memory requirements. To solve the parameter selection problem, two variable-mixing parameter KRMN (VPKRMN) algorithms are developed in this paper. Moreover, a sparsification algorithm, quantized VPKRMN (QVPKRMN) algorithm is introduced for nonlinear system identification with impulsive interferences. The energy conservation relation (ECR) and convergence property of the QVPKRMN algorithm are analyzed. Simulation results in the context of nonlinear system identification under impulsive interference demonstrate the superior performance of the proposed VPKRMN and QVPKRMN algorithms as compared with the existing algorithms. ",Computer Science - Systems and Control ; ,"Lu, Lu ; Zhao, Haiquan ; Chen, Badong ; "
http://arxiv.org/abs/1508.05465,A representation of antimatroids by Horn rules and its application to   educational systems,"  We study a representation of an antimatroid by Horn rules, motivated by its recent application to computer-aided educational systems. We associate any set $\mathcal{R}$ of Horn rules with the unique maximal antimatroid $\mathcal{A}(\mathcal{R})$ that is contained in the union-closed family $\mathcal{K}(\mathcal{R})$ naturally determined by ${\cal R}$. We address algorithmic and Boolean function theoretic aspects on the association ${\cal R} \mapsto \mathcal{A}(\mathcal{R})$, where ${\cal R}$ is viewed as the input. We present linear time algorithms to solve the membership problem and the inference problem for ${\cal A}({\cal R})$. We also provide efficient algorithms for generating all members and all implicates of ${\cal A}({\cal R})$. We show that this representation is essentially equivalent to the Korte-Lov\'{a}sz representation of antimatroids by rooted sets. Based on the equivalence, we provide a quadratic time algorithm to construct the uniquely-determined minimal representation. % These results have potential applications to computer-aided educational systems, where an antimatroid is used as a model of the space of possible knowledge states of learners, and is constructed by giving Horn queries to a human expert. ",Mathematics - Combinatorics ; Computer Science - Logic in Computer Science ; ,"Yoshikawa, Hiyori ; Hirai, Hiroshi ; Makino, Kazuhisa ; "
http://arxiv.org/abs/1508.05559,Structured Interactive Music Scores,"  Interactive Scores is a formalism for the design and performance of interactive scenarios that provides temporal relations (TRs) among the objects of the scenario. We can model TRs among objects in Time Stream Petri nets, but it is difficult to represent global constraints. This can be done explicitly in the Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We want to formalize a heterogeneous system that controls in one subsystem the concurrent execution of the objects using ntcc, and audio and video processing in the other. We also plan to develop an automatic verifier for ntcc. ",Computer Science - Logic in Computer Science ; D.1.6 ; F.4.1 ; ,"Toro, Mauricio ; "
http://arxiv.org/abs/1508.05766,$n$-permutability and linear Datalog implies symmetric Datalog,"  We show that if $\mathbb A$ is a core relational structure such that CSP($\mathbb A$) can be solved by a linear Datalog program, and $\mathbb A$ is $n$-permutable for some $n$, then CSP($\mathbb A$) can be solved by a symmetric Datalog program (and thus CSP($\mathbb A$) lies in deterministic logspace). At the moment, it is not known for which structures $\mathbb A$ will CSP($\mathbb A$) be solvable by a linear Datalog program. However, once somebody obtains a characterization of linear Datalog, our result immediately gives a characterization of symmetric Datalog. ","Computer Science - Computational Complexity ; 68Q17, 68R05, 03C05 ; ","Kazda, Alexandr ; "
http://arxiv.org/abs/1508.06269,A systematic process for evaluating structured perfect Bayesian   equilibria in dynamic games with asymmetric information,"  We consider finite-horizon and infinite-horizon versions of a dynamic game with $N$ selfish players who observe their types privately and take actions that are publicly observed. Players' types evolve as conditionally independent Markov processes, conditioned on their current actions. Their actions and types jointly determine their instantaneous rewards. In dynamic games with asymmetric information, a widely used concept of equilibrium is perfect Bayesian equilibrium (PBE), which consists of a strategy and belief pair that simultaneously satisfy sequential rationality and belief consistency. In general, there does not exist a universal algorithm that decouples the interdependence of strategies and beliefs over time in calculating PBE. In this paper, for the finite-horizon game with independent types we develop a two-step backward-forward recursive algorithm that sequentially decomposes the problem (w.r.t. time) to obtain a subset of PBEs, which we refer to as structured Bayesian perfect equilibria (SPBE). In such equilibria, a player's strategy depends on its history only through a common public belief and its current private type. The backward recursive part of this algorithm defines an equilibrium generating function. Each period in the backward recursion involves solving a fixed-point equation on the space of probability simplexes for every possible belief on types. Using this function, equilibrium strategies and beliefs are generated through a forward recursion. We then extend this methodology to the infinite-horizon model, where we propose a time-invariant single-shot fixed-point equation, which in conjunction with a forward recursive step, generates the SPBE. Sufficient conditions for the existence of SPBE are provided. With our proposed method, we find equilibria that exhibit signaling behavior. This is illustrated with the help of a concrete public goods example. ",Mathematics - Optimization and Control ; Computer Science - Computer Science and Game Theory ; Computer Science - Systems and Control ; ,"Vasal, Deepanshu ; Sinha, Abhinav ; Anastasopoulos, Achilleas ; "
http://arxiv.org/abs/1508.06464,SPF-CellTracker: Tracking multiple cells with strongly-correlated moves   using a spatial particle filter,"  Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: (i) cells are imaged as globular-like objects, (ii) it is difficult to distinguish cells based only on shape and size, (iii) the number of imaged cells ranges in several hundreds, (iv) moves of nearly-located cells are strongly correlated and (v) cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. (2014). ",Computer Science - Computer Vision and Pattern Recognition ; ,"Hirose, Osamu ; Kawaguchi, Shotaro ; Tokunaga, Terumasa ; Toyoshima, Yu ; Teramoto, Takayuki ; Kuge, Sayuri ; Ishihara, Takeshi ; Iino, Yuichi ; Yoshida, Ryo ; "
http://arxiv.org/abs/1508.06589,Cooperative Spectrum Sharing Relaying Protocols With Energy Harvesting   Cognitive User,"  The theory of wireless information and power transfer in energy constrained wireless networks has caught the interest of researchers due to its potential in increasing the lifetime of sensor nodes and mitigate the environment hazards caused by conventional cell batteries. Similarly, the advancements in areas of cooperative spectrum sharing protocols has enabled efficient use of frequency spectrum between a licensed primary user and a secondary user. In this paper, we consider an energy constrained secondary user which harvests energy from the primary signal and relays the primary signal in exchange for the spectrum access. We consider Nakagami-m fading model and propose two key protocols, namely time-splitting cooperative spectrum sharing (TS-CSS) and power-sharing cooperative spectrum sharing (PS-CSS), and derive expressions for the outage probabilities of the primary and secondary user in decode-forward and amplify-forward relaying modes. From the obtained results, it has been shown that the secondary user can carry its own transmission without adversely affecting the performance of the primary user and that PS-CSS protocol outperforms the TS-PSS protocol in terms of outage probability over a wide range of Signal to noise ratio(SNRs). The effect of various system parameters on the outage performance of these protocols have also been studied. ",Computer Science - Networking and Internet Architecture ; Computer Science - Information Theory ; ,"Kalluri, Tarun ; Peer, Mansi ; Bohara, Vivek Ashok ; da Costa, Daniel B. ; Dias, Ugo S. ; "
http://arxiv.org/abs/1508.07065,A dual descent algorithm for node-capacitated multiflow problems and its   applications,"  In this paper, we develop an $O((m \log k) {\rm MSF} (n,m,1))$-time algorithm to find a half-integral node-capacitated multiflow of the maximum total flow-value in a network with $n$ nodes, $m$ edges, and $k$ terminals, where ${\rm MSF} (n',m',\gamma)$ denotes the time complexity of solving the maximum submodular flow problem in a network with $n'$ nodes, $m'$ edges, and the complexity $\gamma$ of computing the exchange capacity of the submodular function describing the problem. By using Fujishige-Zhang algorithm for submodular flow, we can find a maximum half-integral multiflow in $O(m n^3 \log k)$ time. This is the first combinatorial strongly polynomial time algorithm for this problem. Our algorithm is built on a developing theory of discrete convex functions on certain graph structures. Applications include ""ellipsoid-free"" combinatorial implementations of a 2-approximation algorithm for the minimum node-multiway cut problem by Garg, Vazirani, and Yannakakis. ","Computer Science - Data Structures and Algorithms ; Mathematics - Optimization and Control ; 90C27, 05C21 ; ","Hirai, Hiroshi ; "
http://arxiv.org/abs/1508.07435,Subdifferential-based implicit return-mapping operators in Mohr-Coulomb   plasticity,"  The paper is devoted to a constitutive solution, limit load analysis and Newton-like methods in elastoplastic problems containing the Mohr-Coulomb yield criterion. Within the constitutive problem, we introduce a self-contained derivation of the implicit return-mapping solution scheme using a recent subdifferential-based treatment. Unlike conventional techniques based on Koiter's rules, the presented scheme a priori detects a position of the unknown stress tensor on the yield surface even if the constitutive solution cannot be found in closed form. This fact eliminates blind guesswork from the scheme, enables to analyze properties of the constitutive operator, and simplifies construction of the consistent tangent operator which is important for the semismooth Newton method applied on the incremental boundary value elastoplastic problem. The incremental problem in Mohr-Coulomb plasticity is combined with the limit load analysis. Beside a conventional direct method of the incremental limit analysis, a recent indirect one is introduced and its advantages are described. The paper contains 2D and 3D numerical experiments on slope stability with publicly available Matlab implementations. ","Computer Science - Computational Engineering, Finance, and Science ; ","Sysala, Stanislav ; Cermak, Martin ; "
http://arxiv.org/abs/1509.00144,Automatic Software Diversity in the Light of Test Suites,"  A few works address the challenge of automating software diversification, and they all share one core idea: using automated test suites to drive diversification. However, there is is lack of solid understanding of how test suites, programs and transformations interact one with another in this process. We explore this intricate interplay in the context of a specific diversification technique called ""sosiefication"". Sosiefication generates sosie programs, i.e., variants of a program in which some statements are deleted, added or replaced but still pass the test suite of the original program. Our investigation of the influence of test suites on sosiefication exploits the following observation: test suites cover the different regions of programs in very unequal ways. Hence, we hypothesize that sosie synthesis has different performances on a statement that is covered by one hundred test case and on a statement that is covered by a single test case. We synthesize 24583 sosies on 6 popular open-source Java programs. Our results show that there are two dimensions for diversification. The first one lies in the specification: the more test cases cover a statement, the more difficult it is to synthesize sosies. Yet, to our surprise, we are also able to synthesize sosies on highly tested statements (up to 600 test cases), which indicates an intrinsic property of the programs we study. The second dimension is in the code: we manually explore dozens of sosies and characterize new types of forgiving code regions that are prone to diversification. ",Computer Science - Software Engineering ; D.2.5 ; ,"Baudry, Benoit ; Allier, Simon ; Rodriguez-Cancio, Marcelino ; Monperrus, Martin ; "
http://arxiv.org/abs/1509.00773,A Big Data Analyzer for Large Trace Logs,"  Current generation of Internet-based services are typically hosted on large data centers that take the form of warehouse-size structures housing tens of thousands of servers. Continued availability of a modern data center is the result of a complex orchestration among many internal and external actors including computing hardware, multiple layers of intricate software, networking and storage devices, electrical power and cooling plants. During the course of their operation, many of these components produce large amounts of data in the form of event and error logs that are essential not only for identifying and resolving problems but also for improving data center efficiency and management. Most of these activities would benefit significantly from data analytics techniques to exploit hidden statistical patterns and correlations that may be present in the data. The sheer volume of data to be analyzed makes uncovering these correlations and patterns a challenging task. This paper presents BiDAl, a prototype Java tool for log-data analysis that incorporates several Big Data technologies in order to simplify the task of extracting information from data traces produced by large clusters and server farms. BiDAl provides the user with several analysis languages (SQL, R and Hadoop MapReduce) and storage backends (HDFS and SQLite) that can be freely mixed and matched so that a custom tool for a specific task can be easily constructed. BiDAl has a modular architecture so that it can be extended with other backends and analysis languages in the future. In this paper we present the design of BiDAl and describe our experience using it to analyze publicly-available traces from Google data clusters, with the goal of building a realistic model of a complex data center. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Balliu, Alkida ; Olivetti, Dennis ; Babaoglu, Ozalp ; Marzolla, Moreno ; Sîrbu, Alina ; "
http://arxiv.org/abs/1509.00864,Strong Pseudoprimes to Twelve Prime Bases,  Let $\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases. This value is known for $1 \leq m \leq 11$. We extend this by finding $\psi_{12}$ and $\psi_{13}$. We also present an algorithm to find all integers $n\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a reasonable heuristic assumption we can show that it takes at most $B^{2/3+o(1)}$ time. ,"Mathematics - Number Theory ; Computer Science - Data Structures and Algorithms ; Computer Science - Mathematical Software ; Primary 11Y16, 11Y16, Secondary 11A41, 68W40, 68W10 ; ","Sorenson, Jonathan P. ; Webster, Jonathan ; "
http://arxiv.org/abs/1509.00926,Using Inclusion Diagrams as an Alternative to Venn Diagrams to Determine   the Validity of Categorical Syllogisms,"  Inclusion diagrams are introduced as an alternative to using Venn diagrams to determine the validity of categorical syllogisms, and are used here for the analysis of diverse categorical syllogisms. As a preliminary example of a possible generalization of the use of inclusion diagrams, consideration is given also to an argument that includes more than two premises and more than three terms, the classic major, middle and minor terms in categorical syllogisms. ","Computer Science - Logic in Computer Science ; Mathematics - Logic ; 03B05, 03B10, 97E30, 00A66 ; ","Skliar, Osvaldo ; Monge, Ricardo E. ; Gapper, Sherry ; "
http://arxiv.org/abs/1509.01347,Verificarlo: checking floating point accuracy through Monte Carlo   Arithmetic,"  Numerical accuracy of floating point computation is a well studied topic which has not made its way to the end-user in scientific computing. Yet, it has become a critical issue with the recent requirements for code modernization to harness new highly parallel hardware and perform higher resolution computation. To democratize numerical accuracy analysis, it is important to propose tools and methodologies to study large use cases in a reliable and automatic way. In this paper, we propose verificarlo, an extension to the LLVM compiler to automatically use Monte Carlo Arithmetic in a transparent way for the end-user. It supports all the major languages including C, C++, and Fortran. Unlike source-to-source approaches, our implementation captures the influence of compiler optimizations on the numerical accuracy. We illustrate how Monte Carlo Arithmetic using the verificarlo tool outperforms the existing approaches on various use cases and is a step toward automatic numerical analysis. ",Computer Science - Mathematical Software ; Computer Science - Numerical Analysis ; ,"Denis, Christophe ; Castro, Pablo De Oliveira ; Petit, Eric ; "
http://arxiv.org/abs/1509.01676,Optimum Traffic Allocation in Bundled Energy Efficient Ethernet Links,"  The energy demands of Ethernet links have been an active focus of research in the recent years. This work has enabled a new generation of Energy Efficient Ethernet (EEE) interfaces able to adapt their power consumption to the actual traffic demands, thus yielding significant energy savings. With the energy consumption of single network connections being a solved problem, in this paper we focus on the energy demands of link aggregates that are commonly used to increase the capacity of a network connection. We build on known energy models of single EEE links to derive the energy demands of the whole aggregate as a function on how the traffic load is spread among its powered links. We then provide a practical method to share the load that minimizes overall energy consumption with controlled packet delay, and prove that it is valid for a wide range of EEE links. Finally, we validate our method with both synthetic and real traffic traces captured in Internet backbones. ",Computer Science - Networking and Internet Architecture ; ,"Pérez, Miguel Rodríguez ; Veiga, Manuel Fernández ; Alonso, Sergio Herrería ; Hmila, Mariem ; García, Cándido López ; "
http://arxiv.org/abs/1509.01683,Inference From Visible Information And Background Knowledge,"  We provide a wide-ranging study of the scenario where a subset of the relations in a relational vocabulary are visible to a user --- that is, their complete contents are known --- while the remaining relations are invisible. We also have a background theory --- invariants given by logical sentences --- which may relate the visible relations to invisible ones, and also may constrain both the visible and invisible relations in isolation. We want to determine whether some other information, given as a positive existential formula, can be inferred using only the visible information and the background theory. This formula whose inference we are concered with is denoted as the \emph{query}. We consider whether positive information about the query can be inferred, and also whether negative information -- the sentence does not hold -- can be inferred. We further consider both the instance-level version of the problem, where both the query and the visible instance are given, and the schema-level version, where we want to know whether truth or falsity of the query can be inferred in some instance of the schema. ",Computer Science - Logic in Computer Science ; ,"Benedikt, Michael ; Bourhis, Pierre ; Cate, Balder ten ; Puppis, Gabriele ; Boom, Michael Vanden ; "
http://arxiv.org/abs/1509.02223,Diffusion tensor imaging with deterministic error bounds,"  Errors in the data and the forward operator of an inverse problem can be handily modelled using partial order in Banach lattices. We present some existing results of the theory of regularisation in this novel framework, where errors are represented as bounds by means of the appropriate partial order.   We apply the theory to Diffusion Tensor Imaging, where correct noise modelling is challenging: it involves the Rician distribution and the nonlinear Stejskal-Tanner equation. Linearisation of the latter in the statistical framework would complicate the noise model even further. We avoid this using the error bounds approach, which preserves simple error structure under monotone transformations. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Numerical Analysis ; ,"Gorokh, Artur ; Korolev, Yury ; Valkonen, Tuomo ; "
http://arxiv.org/abs/1509.02479,Hofstadter's problem for curious readers,"  This document summarizes the proofs made during a Coq development inSummer 2015. This development investigates the function G introducedby Hofstadter in his famous ""G{\""o}del, Escher, Bach"" bookas well as a related infinite tree. The left/right flipped variantof this G tree has also been studied here, followingHofstadter's ""problem for the curious reader"".The initial G function is refered as sequence A005206 inOEIS, while the flipped version is the sequence A123070. ",Computer Science - Logic in Computer Science ; Mathematics - History and Overview ; ,"Letouzey, Pierre ; "
http://arxiv.org/abs/1509.02601,On the $O_\beta$-hull of a planar point set,"  We study the $O_\beta$-hull of a planar point set, a generalization of the Orthogonal Convex Hull where the coordinate axes form an angle $\beta$. Given a set $P$ of $n$ points in the plane, we show how to maintain the $O_\beta$-hull of $P$ while $\beta$ runs from $0$ to $\pi$ in $O(n \log n)$ time and $O(n)$ space. With the same complexity, we also find the values of $\beta$ that maximize the area and the perimeter of the $O_\beta$-hull and, furthermore, we find the value of $\beta$ achieving the best fitting of the point set $P$ with a two-joint chain of alternate interior angle $\beta$. ",Computer Science - Computational Geometry ; 68U05 ; I.3.5 ; ,"Alegría-Galicia, Carlos ; Orden, David ; Seara, Carlos ; Urrutia, Jorge ; "
http://arxiv.org/abs/1509.02709,A Topological Approach to Meta-heuristics: Analytical Results on the BFS   vs. DFS Algorithm Selection Problem,"  Search is a central problem in artificial intelligence, and breadth-first search (BFS) and depth-first search (DFS) are the two most fundamental ways to search. In this paper we derive estimates for average BFS and DFS runtime. The average runtime estimates can be used to allocate resources or judge the hardness of a problem. They can also be used for selecting the best graph representation, and for selecting the faster algorithm out of BFS and DFS. They may also form the basis for an analysis of more advanced search methods. The paper treats both tree search and graph search. For tree search, we employ a probabilistic model of goal distribution; for graph search, the analysis depends on an additional statistic of path redundancy and average branching factor. As an application, we use the results to predict BFS and DFS runtime on two concrete grammar problems and on the N-puzzle. Experimental verification shows that our analytical approximations come close to empirical reality. ",Computer Science - Artificial Intelligence ; I.2.8 ; ,"Everitt, Tom ; Hutter, Marcus ; "
http://arxiv.org/abs/1509.02900,"Statistical Inference, Learning and Models in Big Data","  The need for new methods to deal with big data is a common theme in most scientific fields, although its definition tends to vary with the context. Statistical ideas are an essential part of this, and as a partial response, a thematic program on statistical inference, learning, and models in big data was held in 2015 in Canada, under the general direction of the Canadian Statistical Sciences Institute, with major funding from, and most activities located at, the Fields Institute for Research in Mathematical Sciences. This paper gives an overview of the topics covered, describing challenges and strategies that seem common to many different areas of application, and including some examples of applications to make these challenges and strategies more concrete. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; 62-07 ; I.2.6 ; I.2.3 ; I.5.1 ; G.3 ; ,"Franke, Beate ; Plante, Jean-François ; Roscher, Ribana ; Lee, Annie ; Smyth, Cathal ; Hatefi, Armin ; Chen, Fuqi ; Gil, Einat ; Schwing, Alexander ; Selvitella, Alessandro ; Hoffman, Michael M. ; Grosse, Roger ; Hendricks, Dieter ; Reid, Nancy ; "
http://arxiv.org/abs/1509.03258,Entropic CLT and phase transition in high-dimensional Wishart matrices,"  We consider high dimensional Wishart matrices $\mathbb{X} \mathbb{X}^{\top}$ where the entries of $\mathbb{X} \in {\mathbb{R}^{n \times d}}$ are i.i.d. from a log-concave distribution. We prove an information theoretic phase transition: such matrices are close in total variation distance to the corresponding Gaussian ensemble if and only if $d$ is much larger than $n^3$. Our proof is entropy-based, making use of the chain rule for relative entropy along with the recursive structure in the definition of the Wishart ensemble. The proof crucially relies on the well known relation between Fisher information and entropy, a variational representation for Fisher information, concentration bounds for the spectral norm of a random matrix, and certain small ball probability estimates for log-concave measures. ",Mathematics - Probability ; Computer Science - Information Theory ; Mathematics - Functional Analysis ; Mathematics - Statistics Theory ; ,"Bubeck, Sébastien ; Ganguly, Shirshendu ; "
http://arxiv.org/abs/1509.03476,Relational reasoning via probabilistic coupling,"  Probabilistic coupling is a powerful tool for analyzing pairs of probabilistic processes. Roughly, coupling two processes requires finding an appropriate witness process that models both processes in the same probability space. Couplings are powerful tools proving properties about the relation between two processes, include reasoning about convergence of distributions and stochastic dominance---a probabilistic version of a monotonicity property.   While the mathematical definition of coupling looks rather complex and cumbersome to manipulate, we show that the relational program logic pRHL---the logic underlying the EasyCrypt cryptographic proof assistant---already internalizes a generalization of probabilistic coupling. With this insight, constructing couplings is no harder than constructing logical proofs. We demonstrate how to express and verify classic examples of couplings in pRHL, and we mechanically verify several couplings in EasyCrypt. ",Computer Science - Logic in Computer Science ; Computer Science - Programming Languages ; ,"Barthe, Gilles ; Espitau, Thomas ; Grégoire, Benjamin ; Hsu, Justin ; Stefanesco, Léo ; Strub, Pierre-Yves ; "
http://arxiv.org/abs/1509.03484,Local structure can identify and quantify influential global spreaders   in large scale social networks,"  Measuring and optimizing the influence of nodes in big-data online social networks are important for many practical applications, such as the viral marketing and the adoption of new products. As the viral spreading on social network is a global process, it is commonly believed that measuring the influence of nodes inevitably requires the knowledge of the entire network. Employing percolation theory, we show that the spreading process displays a nucleation behavior: once a piece of information spread from the seeds to more than a small characteristic number of nodes, it reaches a point of no return and will quickly reach the percolation cluster, regardless of the entire network structure, otherwise the spreading will be contained locally. Thus, we find that, without the knowledge of entire network, any nodes' global influence can be accurately measured using this characteristic number, which is independent of the network size. This motivates an efficient algorithm with constant time complexity on the long standing problem of best seed spreaders selection, with performance remarkably close to the true optimum. ",Physics - Physics and Society ; Computer Science - Computers and Society ; Computer Science - Data Structures and Algorithms ; Computer Science - Social and Information Networks ; ,"Hu, Yanqing ; Ji, Shenggong ; Jin, Yuliang ; Feng, Ling ; Stanley, H. Eugene ; Havlin, Shlomo ; "
http://arxiv.org/abs/1509.03784,Solving underdetermined systems with error-correcting codes,"  In an underdetermined system of equations $Ax=y$, where $A$ is an $m\times n$ matrix, only $u$ of the entries of $y$ with $u < m$ are known. Thus $E_jw$, called `measurements', are known for certain $j\in J \subset \{0,1,\ldots,m-1\}$ where $\{E_i, i=0,1,\ldots, m-1\}$ are the rows of $A$ and $|J|=u$. It is required, if possible, to solve the system uniquely when $x$ has at most $t$ non-zero entries with $u\geq 2t$.   Here such systems are considered from an error-correcting coding point of view. The unknown $x$ can be shown to be the error vector of a code subject to certain conditions on the rows of the matrix $A$. This reduces the problem to finding a suitable decoding algorithm which then finds $x$.   Decoding workable algorithms are shown to exist, from which the unknown $x$ may be determined, in cases where the known $2t$ values are evenly spaced (that is, when the elements of $J$ are in arithmetic progression) for classes of matrices satisfying certain row properties. These cases include Fourier $n\times n $ matrices where the arithmetic difference $k$ satisfies $\gcd(n,k)=1$, and classes of Vandermonde matrices $V(x_1,x_2,\ldots,x_n)$ (with $x_i\neq 0$) with arithmetic difference $k$ where the ratios $x_i/x_j$ for $i\neq j$ are not $k^{th}$ roots of unity. The decoding algorithm has complexity $O(nt)$ and in some cases, including the Fourier matrix cases, the complexity is $O(t^2)$.   Matrices which have the property that the determinant of any square submatrix is non-zero are of particular interest. Randomly choosing rows of such matrices can then give $t$ error-correcting pairs to generate a `measuring' code $C^\perp=\{E_j | j\in J\}$ with a decoding algorithm which finds $x$.   This has applications to signal processing and compressed sensing. ","Computer Science - Information Theory ; 94A99, 15B99 ; ","Hurley, Ted ; "
http://arxiv.org/abs/1509.03915,An Impossibility Result for Housing Markets with Fractional Endowments,"  The housing market setting constitutes a fundamental model of exchange economies of goods. Most of the work concerning housing markets does not cater for randomized assignments or allocation of time-shares. House allocation with fractional endowments of houses was considered by Athanassoglou and Sethuraman (2011) who posed the open problem whether individual rationality, weak strategyproofness, and efficiency are compatible for the setting. We show that the three axioms are incompatible. ","Computer Science - Computer Science and Game Theory ; Computer Science - Data Structures and Algorithms ; 91A12, 68Q15 ; F.2 ; J.4 ; ","Aziz, Haris ; "
http://arxiv.org/abs/1509.04037,Measuring Partial Balance in Signed Networks,"  Is the enemy of an enemy necessarily a friend? If not, to what extent does this tend to hold? Such questions were formulated in terms of signed (social) networks and necessary and sufficient conditions for a network to be ""balanced"" were obtained around 1960. Since then the idea that signed networks tend over time to become more balanced has been widely used in several application areas. However, investigation of this hypothesis has been complicated by the lack of a standard measure of partial balance, since complete balance is almost never achieved in practice. We formalize the concept of a measure of partial balance, discuss various measures, compare the measures on synthetic datasets, and investigate their axiomatic properties. The synthetic data involves Erd\H{o}s-R\'enyi and specially structured random graphs. We show that some measures behave better than others in terms of axioms and ability to differentiate between graphs. We also use well-known datasets from the sociology and biology literature, such as Read's New Guinean tribes, gene regulatory networks related to two organisms, and a network involving senate bill co-sponsorship. Our results show that substantially different levels of partial balance is observed under cycle-based, eigenvalue-based, and frustration-based measures. We make some recommendations for measures to be used in future work. ","Computer Science - Social and Information Networks ; Physics - Physics and Society ; 05C22, 05C38, 91D30, 90B10 ; ","Aref, Samin ; Wilson, Mark C. ; "
http://arxiv.org/abs/1509.04634,Modeling and interpolation of the ambient magnetic field by Gaussian   processes,"  Anomalies in the ambient magnetic field can be used as features in indoor positioning and navigation. By using Maxwell's equations, we derive and present a Bayesian non-parametric probabilistic modeling approach for interpolation and extrapolation of the magnetic field. We model the magnetic field components jointly by imposing a Gaussian process (GP) prior on the latent scalar potential of the magnetic field. By rewriting the GP model in terms of a Hilbert space representation, we circumvent the computational pitfalls associated with GP modeling and provide a computationally efficient and physically justified modeling tool for the ambient magnetic field. The model allows for sequential updating of the estimate and time-dependent changes in the magnetic field. The model is shown to work well in practice in different applications: we demonstrate mapping of the magnetic field both with an inexpensive Raspberry Pi powered robot and on foot using a standard smartphone. ",Computer Science - Robotics ; Statistics - Machine Learning ; ,"Solin, Arno ; Kok, Manon ; Wahlström, Niklas ; Schön, Thomas B. ; Särkkä, Simo ; "
http://arxiv.org/abs/1509.04857,Markov modeling of online inter-arrival times,"  In this paper, we investigate the arising communication patterns on social media, and in particular the series of events happening for a single user. While the distribution of inter-event times is often assimilated to power-law density functions, a debate persists on the nature of an underlying model that explains the observed distribution. In the present, we propose an intuitive explanation to understand the observed dependence of subsequent waiting times. Our contribution is twofold. The first idea consists of separating the short waiting times -- out of scope for power-law distributions -- from the long ones. The model is further enhanced by introducing a two-state Markovian process to incorporate memory. ","Statistics - Applications ; Computer Science - Social and Information Networks ; Physics - Physics and Society ; 62P25 (Primary), 91C20(Secondary) ; ","Kerckhove, Corentin Vande ; Gerencsér, Balázs ; Hendrickx, Julien M. ; Blondel, Vincent D. ; "
http://arxiv.org/abs/1509.04880,An FPT 2-Approximation for Tree-Cut Decomposition,"  The tree-cut width of a graph is a graph parameter defined by Wollan [J. Comb. Theory, Ser. B, 110:47-66, 2015] with the help of tree-cut decompositions. In certain cases, tree-cut width appears to be more adequate than treewidth as an invariant that, when bounded, can accelerate the resolution of intractable problems. While designing algorithms for problems with bounded tree-cut width, it is important to have a parametrically tractable way to compute the exact value of this parameter or, at least, some constant approximation of it. In this paper we give a parameterized 2-approximation algorithm for the computation of tree-cut width; for an input $n$-vertex graph $G$ and an integer $w$, our algorithm either confirms that the tree-cut width of $G$ is more than $w$ or returns a tree-cut decomposition of $G$ certifying that its tree-cut width is at most $2w$, in time $2^{O(w^2\log w)} \cdot n^2$. Prior to this work, no constructive parameterized algorithms, even approximated ones, existed for computing the tree-cut width of a graph. As a consequence of the Graph Minors series by Robertson and Seymour, only the existence of a decision algorithm was known. ","Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; 68R10, 05C85 ; G.2.2 ; F.2.2 ; ","Kim, Eunjung ; Oum, Sang-il ; Paul, Christophe ; Sau, Ignasi ; Thilikos, Dimitrios M. ; "
http://arxiv.org/abs/1509.05001,Solving constrained quadratic binary problems via quantum adiabatic   evolution,"  Quantum adiabatic evolution is perceived as useful for binary quadratic programming problems that are a priori unconstrained. For constrained problems, it is a common practice to relax linear equality constraints as penalty terms in the objective function. However, there has not yet been proposed a method for efficiently dealing with inequality constraints using the quantum adiabatic approach. In this paper, we give a method for solving the Lagrangian dual of a binary quadratic programming (BQP) problem in the presence of inequality constraints and employ this procedure within a branch-and-bound framework for constrained BQP (CBQP) problems. ",Mathematics - Optimization and Control ; Computer Science - Emerging Technologies ; Quantum Physics ; ,"Ronagh, Pooya ; Woods, Brad ; Iranmanesh, Ehsan ; "
http://arxiv.org/abs/1509.05498,Supervisor Localization of Discrete-Event Systems under Partial   Observation,"  Recently we developed supervisor localization, a top-down approach to distributed control of discrete-event systems. Its essence is the allocation of monolithic (global) control action among the local control strategies of individual agents. In this paper, we extend supervisor localization by considering partial observation; namely not all events are observable. Specifically, we employ the recently proposed concept of relative observability to compute a partial-observation monolithic supervisor, and then design a suitable localization procedure to decompose the supervisor into a set of local controllers. In the resulting local controllers, only observable events can cause state change. Further, to deal with large-scale systems, we combine the partial-observation supervisor localization with an efficient architectural synthesis approach: first compute a heterarchical array of partial-observation decentralized supervisors and coordinators, and then localize each of these supervisors/coordinators into local controllers. ",Computer Science - Systems and Control ; ,"Zhang, Renyuan ; Cai, Kai ; Wonham, W. M. ; "
http://arxiv.org/abs/1509.05572,Randomised enumeration of small witnesses using a decision oracle,"  Many combinatorial problems involve determining whether a universe of $n$ elements contains a witness consisting of $k$ elements which have some specified property. In this paper we investigate the relationship between the decision and enumeration versions of such problems: efficient methods are known for transforming a decision algorithm into a search procedure that finds a single witness, but even finding a second witness is not so straightforward in general. We show that, if the decision version of the problem can be solved in time $f(k) \cdot poly(n)$, there is a randomised algorithm which enumerates all witnesses in time $e^{k + o(k)} \cdot f(k) \cdot poly(n) \cdot N$, where $N$ is the total number of witnesses. If the decision version of the problem is solved by a randomised algorithm which may return false negatives, then the same method allows us to output a list of witnesses in which any given witness will be included with high probability. The enumeration algorithm also gives rise to an efficient algorithm to count the total number of witnesses when this number is small. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; ,"Meeks, Kitty ; "
http://arxiv.org/abs/1509.05664,Automated Synthesis of Distributed Self-Stabilizing Protocols,"  In this paper, we introduce an SMT-based method that automatically synthesizes a distributed self-stabilizing protocol from a given high-level specification and network topology. Unlike existing approaches, where synthesis algorithms require the explicit description of the set of legitimate states, our technique only needs the temporal behavior of the protocol. We extend our approach to synthesize ideal-stabilizing protocols, where every state is legitimate. We also extend our technique to synthesize monotonic-stabilizing protocols, where during recovery, each process can execute an most once one action. Our proposed methods are fully implemented and we report successful synthesis of well-known protocols such as Dijkstra's token ring, a self-stabilizing version of Raymond's mutual exclusion algorithm, ideal-stabilizing leader election and local mutual exclusion, as well as monotonic-stabilizing maximal independent set and distributed Grundy coloring. ","Computer Science - Software Engineering ; Computer Science - Distributed, Parallel, and Cluster Computing ; ","Faghih, Fathiyeh ; Bonakdarpour, Borzoo ; Tixeuil, Sebastien ; Kulkarni, Sandeep ; "
http://arxiv.org/abs/1509.05821,New bounds on curve tangencies and orthogonalities,"  We establish new bounds on the number of tangencies and orthogonal intersections determined by an arrangement of curves. First, given a set of $n$ algebraic plane curves, we show that there are $O(n^{3/2})$ points where two or more curves are tangent. In particular, if no three curves are mutually tangent at a common point, then there are $O(n^{3/2})$ curve-curve tangencies. Second, given a family of algebraic plane curves and a set of $n$ curves from this family, we show that either there are $O(n^{3/2})$ points where two or more curves are orthogonal, or the family of curves has certain special properties.   We obtain these bounds by transforming the arrangement of plane curves into an arrangement of space curves so that tangency (or orthogonality) of the original plane curves corresponds to intersection of space curves. We then bound the number of intersections of the corresponding space curves. For the case of curve-curve tangency, we use a polynomial method technique that is reminiscent of Guth and Katz's proof of the joints theorem. For the case of orthogonal curve intersections, we employ a bound of Guth and the third author to control the number of two-rich points in space curve arrangements. ",Mathematics - Combinatorics ; Computer Science - Computational Geometry ; ,"Ellenberg, Jordan S. ; Solymosi, Jozsef ; Zahl, Joshua ; "
http://arxiv.org/abs/1509.06191,Product Space Models of Correlation: Between Noise Stability and   Additive Combinatorics,"  There is a common theme to some research questions in additive combinatorics and noise stability. Both study the following basic question: Let $\mathcal{P}$ be a probability distribution over a space $\Omega^\ell$ with all $\ell$ marginals equal. Let $\underline{X}^{(1)}, \ldots, \underline{X}^{(\ell)}$ where $\underline{X}^{(j)} = (X_1^{(j)}, \ldots, X_n^{(j)})$ be random vectors such that for every coordinate $i \in [n]$ the tuples $(X_i^{(1)}, \ldots, X_i^{(\ell)})$ are i.i.d. according to $\mathcal{P}$.   A central question that is addressed in both areas is:   - Does there exist a function $c_{\mathcal{P}}()$ independent of $n$ such that for every $f: \Omega^n \to [0, 1]$ with $\mathrm{E}[f(X^{(1)})] = \mu > 0$: \begin{align*} \mathrm{E} \left[ \prod_{j=1}^\ell f(X^{(j)}) \right]   \ge c(\mu) > 0 \, ? \end{align*}   Instances of this question include the finite field model version of Roth's and Szemer\'edi's theorems as well as Borell's result about the optimality of noise stability of half-spaces.   Our goal in this paper is to interpolate between the noise stability theory and the finite field additive combinatorics theory and address the question above in further generality than considered before. In particular, we settle the question for $\ell = 2$ and when $\ell > 2$ and $\mathcal{P}$ has bounded correlation $\rho(\mathcal{P}) < 1$. Under the same conditions we also characterize the _obstructions_ for similar lower bounds in the case of $\ell$ different functions. Part of the novelty in our proof is the combination of analytic arguments from the theories of influences and hyper-contraction with arguments from additive combinatorics. ",Computer Science - Discrete Mathematics ; ,"Hązła, Jan ; Holenstein, Thomas ; Mossel, Elchanan ; "
http://arxiv.org/abs/1509.06357,Using Contracted Solution Graphs for Solving Reconfiguration Problems,"  We introduce in a general setting a dynamic programming method for solving reconfiguration problems. Our method is based on contracted solution graphs, which are obtained from solution graphs by performing an appropriate series of edge contractions that decrease the graph size without losing any critical information needed to solve the reconfiguration problem under consideration. Our general framework captures the approach behind known reconfiguration results of Bonsma (2012) and Hatanaka, Ito and Zhou (2014). As a third example, we apply the method to the following problem: given two $k$-colorings $\alpha$ and $\beta$ of a graph $G$, can $\alpha$ be modified into $\beta$ by recoloring one vertex of $G$ at a time, while maintaining a $k$-coloring throughout? This problem is known to be PSPACE-hard even for bipartite planar graphs and $k=4$. By applying our method in combination with a thorough exploitation of the graph structure we obtain a polynomial time algorithm for $(k-2)$-connected chordal graphs. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computational Complexity ; ,"Bonsma, Paul ; Paulusma, Daniel ; "
http://arxiv.org/abs/1509.06559,Shape Aware Matching of Implicit Surfaces based on Thin Shell Energies,"  A shape sensitive, variational approach for the matching of surfaces considered as thin elastic shells is investigated. The elasticity functional to be minimized takes into account two different types of nonlinear energies: a membrane energy measuring the rate of tangential distortion when deforming the reference shell into the template shell, and a bending energy measuring the bending under the deformation in terms of the change of the shape operators from the undeformed into the deformed configuration. The variational method applies to surfaces described as level sets. It is mathematically well-posed and an existence proof of an optimal matching deformation is given. The variational model is implemented using a finite element discretization combined with a narrow band approach on an efficient hierarchical grid structure. For the optimization a regularized nonlinear conjugate gradient scheme and a cascadic multilevel strategy are used. The features of the proposed approach are studied for synthetic test cases and a collection of geometry processing examples. ","Mathematics - Optimization and Control ; Computer Science - Computational Geometry ; 65D18, 49J45, 74K25 ; ","Iglesias, José A. ; Rumpf, Martin ; Scherzer, Otmar ; "
http://arxiv.org/abs/1509.07127,Universal recovery maps and approximate sufficiency of quantum relative   entropy,"  The data processing inequality states that the quantum relative entropy between two states $\rho$ and $\sigma$ can never increase by applying the same quantum channel $\mathcal{N}$ to both states. This inequality can be strengthened with a remainder term in the form of a distance between $\rho$ and the closest recovered state $(\mathcal{R} \circ \mathcal{N})(\rho)$, where $\mathcal{R}$ is a recovery map with the property that $\sigma = (\mathcal{R} \circ \mathcal{N})(\sigma)$. We show the existence of an explicit recovery map that is universal in the sense that it depends only on $\sigma$ and the quantum channel $\mathcal{N}$ to be reversed. This result gives an alternate, information-theoretic characterization of the conditions for approximate quantum error correction. ",Quantum Physics ; Computer Science - Information Theory ; Mathematical Physics ; ,"Junge, Marius ; Renner, Renato ; Sutter, David ; Wilde, Mark M. ; Winter, Andreas ; "
http://arxiv.org/abs/1509.07314,Adaptive-Robust Control of a Class of Uncertain Nonlinear Systems   Utilizing Time-Delayed Input and Position Feedback,"  In this paper, the tracking control problem of a class of Euler-Lagrange systems subjected to unknown uncertainties is addressed and an adaptive-robust control strategy, christened as Time-Delayed Adaptive Robust Control (TARC) is presented. The proposed control strategy approximates the unknown dynamics through time-delayed logic, and the switching logic provides robustness against the approximation error. The novel adaptation law for the switching gain, in contrast to the conventional adaptive-robust control methodologies, does not require either nominal modelling or predefined bounds of the uncertainties. Also, the proposed adaptive law circumvents the overestimation-underestimation problem of switching gain. The state derivatives in the proposed control law is estimated from past data of the state to alleviate the measurement error when state derivatives are not available directly. Moreover, a new stability notion for time-delayed control is proposed which in turn provides a selection criterion for controller gain and sampling interval. Experimental result of the proposed methodology using a nonholonomic wheeled mobile robot (WMR) is presented and improved tracking accuracy of the proposed control law is noted compared to time-delayed control and adaptive sliding mode control. ",Computer Science - Systems and Control ; ,"Roy, Spandan ; Kar, Indra Narayan ; "
http://arxiv.org/abs/1509.07330,Pricing Policies for Selling Indivisible Storable Goods to Strategic   Consumers,"  We study the dynamic pricing problem faced by a monopolistic retailer who sells a storable product to forward-looking consumers. In this framework, the two major pricing policies (or mechanisms) studied in the literature are the preannounced (commitment) pricing policy and the contingent (threat or history dependent) pricing policy. We analyse and compare these pricing policies in the setting where the good can be purchased along a finite time horizon in indivisible atomic quantities. First, we show that, given linear storage costs, the retailer can compute an optimal preannounced pricing policy in polynomial time by solving a dynamic program. Moreover, under such a policy, we show that consumers do not need to store units in order to anticipate price rises. Second, under the contingent pricing policy rather than the preannounced pricing mechanism, (i) prices could be lower, (ii) retailer revenues could be higher, and (iii) consumer surplus could be higher. This result is surprising, in that these three facts are in complete contrast to the case of a retailer selling divisible storable goods Dudine et al. (2006). Third, we quantify exactly how much more profitable a contingent policy could be with respect to a preannounced policy. Specifically, for a market with $N$ consumers, a contingent policy can produce a multiplicative factor of $\Omega(\log N)$ more revenues than a preannounced policy, and this bound is tight. ",Computer Science - Computer Science and Game Theory ; ,"Berbeglia, Gerardo ; Rayaprolu, Gautam ; Vetta, Adrian ; "
http://arxiv.org/abs/1509.07417,Deterministic Sparse Suffix Sorting in the Restore Model,"  Given a text $T$ of length $n$, we propose a deterministic online algorithm computing the sparse suffix array and the sparse longest common prefix array of $T$ in $O(c \sqrt{\lg n} + m \lg m \lg n \lg^* n)$ time with $O(m)$ words of space under the premise that the space of $T$ is rewritable, where $m \le n$ is the number of suffixes to be sorted (provided online and arbitrarily), and $c$ is the number of characters with $m \le c \le n$ that must be compared for distinguishing the designated suffixes. ",Computer Science - Data Structures and Algorithms ; ,"Fischer, Johannes ; I, Tomohiro ; Köppl, Dominik ; "
http://arxiv.org/abs/1509.07552,Formal Support for Standardizing Protocols with State,"  Many cryptographic protocols are designed to achieve their goals using only messages passed over an open network. Numerous tools, based on well-understood foundations, exist for the design and analysis of protocols that rely purely on message passing. However, these tools encounter difficulties when faced with protocols that rely on non-local, mutable state to coordinate several local sessions.   We adapt one of these tools, {\cpsa}, to provide automated support for reasoning about state. We use Ryan's Envelope Protocol as an example to demonstrate how the message-passing reasoning can be integrated with state reasoning to yield interesting and powerful results.   Keywords: protocol analysis tools, stateful protocols, TPM, PKCS#11. ",Computer Science - Cryptography and Security ; ,"Guttman, Joshua D. ; Liskov, Moses D. ; Ramsdell, John D. ; Rowe, Paul D. ; "
http://arxiv.org/abs/1509.07808,A (1+epsilon)-Approximation for Makespan Scheduling with Precedence   Constraints using LP Hierarchies,"  In a classical problem in scheduling, one has $n$ unit size jobs with a precedence order and the goal is to find a schedule of those jobs on $m$ identical machines as to minimize the makespan. It is one of the remaining four open problems from the book of Garey & Johnson whether or not this problem is $\mathbf{NP}$-hard for $m=3$.   We prove that for any fixed $\varepsilon$ and $m$, an LP-hierarchy lift of the time-indexed LP with a slightly super poly-logarithmic number of $r = (\log(n))^{\Theta(\log \log n)}$ rounds provides a $(1 + \varepsilon)$-approximation. For example Sherali-Adams suffices as hierarchy. This implies an algorithm that yields a $(1+\varepsilon)$-approximation in time $n^{O(r)}$. The previously best approximation algorithms guarantee a $2 - \frac{7}{3m+1}$-approximation in polynomial time for $m \geq 4$ and $\frac{4}{3}$ for $m=3$. Our algorithm is based on a recursive scheduling approach where in each step we reduce the correlation in form of long chains. Our method adds to the rather short list of examples where hierarchies are actually useful to obtain better approximation algorithms. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Levey, Elaine ; Rothvoss, Thomas ; "
http://arxiv.org/abs/1509.08102,Discriminative Learning of the Prototype Set for Nearest Neighbor   Classification,"  The nearest neighbor rule is a classic yet essential classification model, particularly in problems where the supervising information is given by pairwise dissimilarities and the embedding function are not easily obtained. Prototype selection provides means of generalization and improving efficiency of the nearest neighbor model, but many existing methods assume and rely on the analyses of the input vector space. In this paper, we explore a dissimilarity-based, parametrized model of the nearest neighbor rule. In the proposed model, the selection of the nearest prototypes is influenced by the parameters of the respective prototypes. It provides a formulation for minimizing the violation of the extended nearest neighbor rule over the training set in a tractable form to exploit numerical techniques. We show that the minimization problem reduces to a large-margin principle learning and demonstrate its advantage by empirical comparisons with other prototype selection methods. ",Computer Science - Machine Learning ; ,"Ando, Shin ; "
http://arxiv.org/abs/1509.08346,UB-ANC Drone: A Flexible Airborne Networking and Communications Testbed,"  We present the University at Buffalo's Airborne Networking and Communications Testbed (UB-ANC Drone). UB-ANC Drone is an open software/hardware platform that aims to facilitate rapid testing and repeatable comparative evaluation of airborne networking and communications protocols at different layers of the protocol stack. It combines quadcopters capable of autonomous flight with sophisticated command and control capabilities and embedded software-defined radios (SDRs), which enable flexible deployment of novel communications and networking protocols. This is in contrast to existing airborne network testbeds, which rely on standard inflexible wireless technologies, e.g., Wi-Fi or Zigbee. UB-ANC Drone is designed with emphasis on modularity and extensibility, and is built around popular open-source projects and standards developed by the research and hobby communities. This makes UB-ANC Drone highly customizable, while also simplifying its adoption. In this paper, we describe UB-ANC Drone's hardware and software architecture. ",Computer Science - Networking and Internet Architecture ; Computer Science - Robotics ; ,"Modares, Jalil ; Mastronarde, Nicholas ; "
http://arxiv.org/abs/1509.08690,Kempe's Universality Theorem for Rational Space Curves,  We prove that every bounded rational space curve of degree d and circularity c can be drawn by a linkage with 9/2 d - 6c + 1 revolute joints. Our proof is based on two ingredients. The first one is the factorization theory of motion polynomials. The second one is the construction of a motion polynomial of minimum degree with given orbit. Our proof also gives the explicity construction of the linkage. ,"Computer Science - Computational Geometry ; Computer Science - Robotics ; Computer Science - Symbolic Computation ; Mathematics - Algebraic Geometry ; Mathematics - Rings and Algebras ; 70B05, 13F20, 65D17, 68U07 ; ","Li, Zijia ; Schicho, Josef ; Schröcker, Hans-Peter ; "
http://arxiv.org/abs/1509.08764,On the Min-cost Traveling Salesman Problem with Drone,"  Over the past few years, unmanned aerial vehicles (UAV), also known as drones, have been adopted as part of a new logistic method in the commercial sector called ""last-mile delivery"". In this novel approach, they are deployed alongside trucks to deliver goods to customers to improve the quality of service and reduce the transportation cost. This approach gives rise to a new variant of the traveling salesman problem (TSP), called TSP with drone (TSP-D). A variant of this problem that aims to minimize the time at which truck and drone finish the service (or, in other words, to maximize the quality of service) was studied in the work of Murray and Chu (2015). In contrast, this paper considers a new variant of TSP-D in which the objective is to minimize operational costs including total transportation cost and one created by waste time a vehicle has to wait for the other. The problem is first formulated mathematically. Then, two algorithms are proposed for the solution. The first algorithm (TSP-LS) was adapted from the approach proposed by Murray and Chu (2015), in which an optimal TSP solution is converted to a feasible TSP-D solution by local searches. The second algorithm, a Greedy Randomized Adaptive Search Procedure (GRASP), is based on a new split procedure that optimally splits any TSP tour into a TSP-D solution. After a TSP-D solution has been generated, it is then improved through local search operators. Numerical results obtained on various instances of both objective functions with different sizes and characteristics are presented. The results show that GRASP outperforms TSP-LS in terms of solution quality under an acceptable running time. ",Computer Science - Artificial Intelligence ; ,"Ha, Quang Minh ; Deville, Yves ; Pham, Quang Dung ; Hà, Minh Hoàng ; "
http://arxiv.org/abs/1509.08892,A data-dependent weighted LASSO under Poisson noise,"  Sparse linear inverse problems appear in a variety of settings, but often the noise contaminating observations cannot accurately be described as bounded by or arising from a Gaussian distribution. Poisson observations in particular are a feature of several real-world applications. Previous work on sparse Poisson inverse problems encountered several limiting technical hurdles. This paper describes a novel alternative analysis approach for sparse Poisson inverse problems that (a) sidesteps the technical challenges in previous work, (b) admits estimators that can readily be computed using off-the-shelf LASSO algorithms, and (c) hints at a general framework for broad classes of noise in sparse linear inverse problems. At the heart of this new approach lies a weighted LASSO estimator for which data-dependent weights are based on Poisson concentration inequalities. Unlike previous analyses of the weighted LASSO, the proposed analysis depends on conditions which can be checked or shown to hold in general settings with high probability. ",Mathematics - Statistics Theory ; Computer Science - Information Theory ; ,"Jiang, Xin ; Reynaud-Bouret, Patricia ; Rivoirard, Vincent ; Sansonnet, Laure ; Willett, Rebecca ; "
http://arxiv.org/abs/1509.08979,Fixpoint Node Selection Query Languages for Trees,"  The study of node selection query languages for (finite) trees has been a major topic in the recent research on query languages for Web documents. On one hand, there has been an extensive study of XPath and its various extensions. On the other hand, query languages based on classical logics, such as first-order logic (FO) or Monadic Second-Order Logic (MSO), have been considered. Results in this area typically relate an XPath-based language to a classical logic. What has yet to emerge is an XPath-related language that is as expressive as MSO, and at the same time enjoys the computational properties of XPath, which are linear time query evaluation and exponential time query-containment test. In this paper we propose muXPath, which is the alternation-free fragment of XPath extended with fixpoint operators. Using two-way alternating automata, we show that this language does combine desired expressiveness and computational properties, placing it as an attractive candidate for the definite node-selection query language for trees. ",Computer Science - Databases ; Computer Science - Logic in Computer Science ; ,"Calvanese, Diego ; De Giacomo, Giuseppe ; Lenzerini, Maurizio ; Vardi, Moshe Y. ; "
http://arxiv.org/abs/1509.09121,"The ""handedness"" of language: Directional symmetry breaking of sign   usage in words","  Language, which allows complex ideas to be communicated through symbolic sequences, is a characteristic feature of our species and manifested in a multitude of forms. Using large written corpora for many different languages and scripts, we show that the occurrence probability distributions of signs at the left and right ends of words have a distinct heterogeneous nature. Characterizing this asymmetry using quantitative inequality measures, viz. information entropy and the Gini index, we show that the beginning of a word is less restrictive in sign usage than the end. This property is not simply attributable to the use of common affixes as it is seen even when only word roots are considered. We use the existence of this asymmetry to infer the direction of writing in undeciphered inscriptions that agrees with the archaeological evidence. Unlike traditional investigations of phonotactic constraints which focus on language-specific patterns, our study reveals a property valid across languages and writing systems. As both language and writing are unique aspects of our species, this universal signature may reflect an innate feature of the human cognitive phenomenon. ",Computer Science - Computation and Language ; ,"Ashraf, Md Izhar ; Sinha, Sitabhra ; "
http://arxiv.org/abs/1509.09188,Approximate Spectral Clustering: Efficiency and Guarantees,"  Approximate Spectral Clustering (ASC) is a popular and successful heuristic for partitioning the nodes of a graph $G$ into clusters for which the ratio of outside connections compared to the volume (sum of degrees) is small. ASC consists of the following two subroutines: i) compute an approximate Spectral Embedding via the Power method; and ii) partition the resulting vector set with an approximate $k$-means clustering algorithm. The resulting $k$-means partition naturally induces a $k$-way node partition of $G$.   We give a comprehensive analysis of ASC building on the work of Peng et al.~(SICOMP'17), Boutsidis et al.~(ICML'15) and Ostrovsky et al.~(JACM'13). We show that ASC i) runs efficiently, and ii) yields a good approximation of an optimal $k$-way node partition of $G$. Moreover, we strengthen the quality guarantees of a structural result of Peng et al. by a factor of $k$, and simultaneously weaken the eigenvalue gap assumption. Further, we show that ASC finds a $k$-way node partition of $G$ with the strengthened quality guarantees. ",Computer Science - Discrete Mathematics ; ,"Kolev, Pavel ; Mehlhorn, Kurt ; "
http://arxiv.org/abs/1509.09236,On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix   Approximation,"  The low-rank matrix approximation problem with respect to the component-wise $\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principal component analysis (PCA), has become a very popular tool in data mining and machine learning. Robust PCA aims at recovering a low-rank matrix that was perturbed with sparse noise, with applications for example in foreground-background video separation. Although $\ell_1$-LRA is strongly believed to be NP-hard, there is, to the best of our knowledge, no formal proof of this fact. In this paper, we prove that $\ell_1$-LRA is NP-hard, already in the rank-one case, using a reduction from MAX CUT. Our derivations draw interesting connections between $\ell_1$-LRA and several other well-known problems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, a particular densest bipartite subgraph problem, the computation of the cut norm of $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove to be NP-hard. ",Computer Science - Machine Learning ; Computer Science - Computational Complexity ; Mathematics - Numerical Analysis ; Mathematics - Optimization and Control ; ,"Gillis, Nicolas ; Vavasis, Stephen A. ; "
http://arxiv.org/abs/1510.00215,FPT Approximation Schemes for Maximizing Submodular Functions,"  We investigate the existence of approximation algorithms for maximization of submodular functions, that run in fixed parameter tractable (FPT) time. Given a non-decreasing submodular set function $v: 2^X \to \mathbb{R}$ the goal is to select a subset $S$ of $K$ elements from $X$ such that $v(S)$ is maximized. We identify three properties of set functions, referred to as $p$-separability properties, and we argue that many real-life problems can be expressed as maximization of submodular, $p$-separable functions, with low values of the parameter $p$. We present FPT approximation schemes for the minimization and maximization variants of the problem, for several parameters that depend on characteristics of the optimized set function, such as $p$ and $K$. We confirm that our algorithms are applicable to a broad class of problems, in particular to problems from computational social choice, such as item selection or winner determination under several multiwinner election systems. ",Computer Science - Data Structures and Algorithms ; ,"Skowron, Piotr ; "
http://arxiv.org/abs/1510.00347,Coding Theorem and Converse for Abstract Channels with Time Structure   and Memory,"  A coding theorem and converse are proved for a large class of abstract stationary channels with time structure including the result by Kadota and Wyner (1972) on continuous-time real-valued channels as special cases. As main contribution the coding theorem is proved for a significantly weaker condition on the channel output memory - called total ergodicity w.r.t. finite alphabet block-memoryless input sources - and under a crucial relaxation of the measurability requirement for the channel. These improvements are achieved by introducing a suitable characterization of information rate capacity. It is shown that the $\psi$-mixing output memory condition used by Kadota and Wyner is quite restrictive and excludes important channel models, in particular for the class of Gaussian channels. In fact, it is proved that for Gaussian (e.g., fading or additive noise) channels the $\psi$-mixing condition is equivalent to finite output memory. Further, it is demonstrated that the measurability requirement of Kadota and Wyner is not satisfied for relevant continuous-time channel models such as linear filters, whereas the condition used in this paper is satisfied for these models. Moreover, a weak converse is derived for all stationary channels with time structure. Intersymbol interference as well as input constraints are taken into account in a general and flexible way, including amplitude and average power constraints as special case. Formulated in rigorous mathematical terms complete, explicit, and transparent proofs are presented. As a side product a gap in the proof of Kadota and Wyner - illustrated by a counterexample - is closed by providing a corrected proof of a lemma on the monotonicity of some sequence of normalized mutual information quantities. An abstract framework is established to treat discrete- and continuous-time channels with memory and arbitrary alphabets in a unified way. ",Computer Science - Information Theory ; ,"Mittelbach, Martin ; Jorswieck, Eduard A. ; "
http://arxiv.org/abs/1510.00549,Bishellable drawings of $K_n$,"  The Harary--Hill conjecture, still open after more than 50 years, asserts that the crossing number of the complete graph $K_n$ is $ H(n) = \frac 1 4 \left\lfloor\frac{\mathstrut n}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-1}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-2}{\mathstrut 2}\right\rfloor \left\lfloor\frac{\mathstrut n-3}{\mathstrut 2}\right \rfloor$. \'Abrego et al. introduced the notion of shellability of a drawing $D$ of $K_n$. They proved that if $D$ is $s$-shellable for some $s\geq\lfloor\frac{n}{2}\rfloor$, then $D$ has at least $H(n)$ crossings. This is the first combinatorial condition on a drawing that guarantees at least $H(n)$ crossings. In this work, we generalize the concept of $s$-shellability to bishellability, where the former implies the latter in the sense that every $s$-shellable drawing is, for any $b \leq s-2$, also $b$-bishellable. Our main result is that $(\lfloor \frac{n}{2} \rfloor\!-\!2)$-bishellability of a drawing $D$ of $K_n$ also guarantees, with a simpler proof than for $s$-shellability, that $D$ has at least $H(n)$ crossings. We exhibit a drawing of $K_{11}$ that has $H(11)$ crossings, is 3-bishellable, and is not $s$-shellable for any $s\geq5$. This shows that we have properly extended the class of drawings for which the Harary-Hill Conjecture is proved. Moreover, we provide an infinite family of drawings of $K_n$ that are $(\lfloor \frac{n}{2} \rfloor\!-\!2)$-bishellable, but not $s$-shellable for any $s\geq\lfloor\frac{n}{2}\rfloor$. ","Mathematics - Combinatorics ; Computer Science - Computational Geometry ; 05C10, 68R10 ; ","Ábrego, Bernardo M. ; Aichholzer, Oswin ; Fernández-Merchant, Silvia ; McQuillan, Dan ; Mohar, Bojan ; Mutzel, Petra ; Ramos, Pedro ; Richter, R. Bruce ; Vogtenhuber, Birgit ; "
http://arxiv.org/abs/1510.01072,Routing in Unit Disk Graphs,"  Let $S \subset \mathbb{R}^2$ be a set of $n$ sites. The unit disk graph $\text{UD}(S)$ on $S$ has vertex set $S$ and an edge between two distinct sites $s,t \in S$ if and only if $s$ and $t$ have Euclidean distance $|st| \leq 1$.   A routing scheme $R$ for $\text{UD}(S)$ assigns to each site $s \in S$ a label $\ell(s)$ and a routing table $\rho(s)$. For any two sites $s, t \in S$, the scheme $R$ must be able to route a packet from $s$ to $t$ in the following way: given a current site $r$ (initially, $r = s$), a header $h$ (initially empty), and the label $\ell(t)$ of the target, the scheme $R$ consults the routing table $\rho(r)$ to compute a neighbor $r'$ of $r$, a new header $h'$, and the label $\ell(t')$ of an intermediate target $t'$. (The label of the original target may be stored at the header $h'$.) The packet is then routed to $r'$, and the procedure is repeated until the packet reaches $t$. The resulting sequence of sites is called the routing path. The stretch of $R$ is the maximum ratio of the (Euclidean) length of the routing path produced by $R$ and the shortest path in $\text{UD}(S)$, over all pairs of distinct sites in $S$.   For any given $\varepsilon > 0$, we show how to construct a routing scheme for $\text{UD}(S)$ with stretch $1+\varepsilon$ using labels of $O(\log n)$ bits and routing tables of $O(\varepsilon^{-5}\log^2 n \log^2 D)$ bits, where $D$ is the (Euclidean) diameter of $\text{UD}(S)$. The header size is $O(\log n \log D)$ bits. ",Computer Science - Computational Geometry ; Computer Science - Data Structures and Algorithms ; ,"Kaplan, Haim ; Mulzer, Wolfgang ; Roditty, Liam ; Seiferth, Paul ; "
http://arxiv.org/abs/1510.01210,Trading Networks with Bilateral Contracts,"  We consider a model of matching in trading networks in which firms can enter into bilateral contracts. In trading networks, stable outcomes, which are immune to deviations of arbitrary sets of firms, may not exist. We define a new solution concept called trail stability. Trail-stable outcomes are immune to consecutive, pairwise deviations between linked firms. We show that any trading network with bilateral contracts has a trail-stable outcome whenever firms' choice functions satisfy the full substitutability condition. For trail-stable outcomes, we prove results on the lattice structure, the rural hospitals theorem, strategy-proofness, and comparative statics of firm entry and exit. We also introduce weak trail stability which is implied by trail stability under full substitutability. We describe relationships between the solution concepts. ",Computer Science - Computer Science and Game Theory ; Economics - General Economics ; J.4 ; G.2.1 ; ,"Fleiner, Tamás ; Jankó, Zsuzsanna ; Tamura, Akihisa ; Teytelboym, Alexander ; "
http://arxiv.org/abs/1510.01429,Distance-2 MDS codes and latin colorings in the Doob graphs,"  The maximum independent sets in the Doob graphs D(m,n) are analogs of the distance-2 MDS codes in Hamming graphs and of the latin hypercubes. We prove the characterization of these sets stating that every such set is semilinear or reducible. As related objects, we study vertex sets with maximum cut (edge boundary) in D(m,n) and prove some facts on their structure. We show that the considered two classes (the maximum independent sets and the maximum-cut sets) can be defined as classes of completely regular sets with specified 2-by-2 quotient matrices. It is notable that for a set from the considered classes, the eigenvalues of the quotient matrix are the maximum and the minimum eigenvalues of the graph. For D(m,0), we show the existence of a third, intermediate, class of completely regular sets with the same property. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; Computer Science - Information Theory ; 05B30 ; ,"Krotov, Denis ; Bespalov, Evgeny ; "
http://arxiv.org/abs/1510.01518,DC Decomposition of Nonconvex Polynomials with Algebraic Techniques,"  We consider the problem of decomposing a multivariate polynomial as the difference of two convex polynomials. We introduce algebraic techniques which reduce this task to linear, second order cone, and semidefinite programming. This allows us to optimize over subsets of valid difference of convex decompositions (dcds) and find ones that speed up the convex-concave procedure (CCP). We prove, however, that optimizing over the entire set of dcds is NP-hard. ",Mathematics - Optimization and Control ; Computer Science - Data Structures and Algorithms ; Statistics - Machine Learning ; ,"Ahmadi, Amir Ali ; Hall, Georgina ; "
http://arxiv.org/abs/1510.01591,On the codes over the Z_3+vZ_3+v^2Z_3,"  In this paper, we study the structure of cyclic, quasi-cyclic, constacyclic codes and their skew codes over the finite ring R=Z_3+vZ_3+v^2Z_3, v^3=v. The Gray images of cyclic, quasi-cyclic, skew cyclic, skew quasi-cyclic and skew constacyclic codes over R are obtained. A necessary and sufficient condition for cyclic (negacyclic) codes over R that contains its dual has been given. The parameters of quantum error correcting codes are obtained from both cyclic and negacyclic codes over R. It is given some examples. Firstly, quasi-constacyclic and skew quasi-constacyclic codes are introduced. By giving two product, it is investigated their duality. A sufficient condition for 1-generator skew quasi-constacyclic codes to be free is determined. ",Computer Science - Information Theory ; Quantum Physics ; ,"Dertli, Abdullah ; Cengellenmis, Yasein ; Eren, Senol ; "
http://arxiv.org/abs/1510.01671,Cross-boundary Behavioural Reprogrammability Reveals Evidence of   Pervasive Universality,"  We exhaustively explore the reprogrammability capabilities and the intrinsic universality of the Cartesian product $P \times C$ of the space $P$ of all possible computer programs of increasing size and the space $C$ of all possible compilers of increasing length such that $p \in P$ emulates $p^\prime \in P$ with $T|p^\prime|=|p|$ under a coarse-graining transformation $T$. Our approach yields a novel perspective on the complexity, controllability, causality and (re)programmability discrete dynamical systems. We find evidence that the density of (qualitatively different) computer programs that can be reprogrammed grows asymptotically as a function of program and compiler size. To illustrate these findings we show a series of behavioural boundary crossing results, including emulations (for all initial conditions) of Wolfram class 2 Elementary Cellular Automata (ECA) by Class 1 ECA, emulations of Classes 1, 2 and 3 ECA by Class 2 and 3 ECA, and of Classes 1, 2 and 3 by Class 3 ECA, along with results of even greater emulability for general CA (neighbourhood $r=3/2$), including Class 1 CA emulating Classes 2 and 3, and Classes 3 and 4 emulating all other classes (1, 2, 3 and 4). The emulations occur with only a linear overhead and can be considered computationally efficient. We also found that there is no hacking strategy to compress the search space based on compiler profiling in terms of e.g. similarity or complexity, suggesting that no strategy other than exhaustive search is viable. We also introduce emulation networks, derive a topologically-based measure of complexity based upon out- and in-degree connectivity, and establish bridges to fundamental ideas of complexity, universality, causality and dynamical systems. ",Computer Science - Formal Languages and Automata Theory ; Computer Science - Computational Complexity ; Nonlinear Sciences - Cellular Automata and Lattice Gases ; ,"Riedel, Jürgen ; Zenil, Hector ; "
http://arxiv.org/abs/1510.01819,Balanced Islands in Two Colored Point Sets in the Plane,"  Let $S$ be a set of $n$ points in general position in the plane, $r$ of which are red and $b$ of which are blue. In this paper we prove that there exist: for every $\alpha \in \left [ 0,\frac{1}{2} \right ]$, a convex set containing exactly $\lceil \alpha r\rceil$ red points and exactly $\lceil \alpha b \rceil$ blue points of $S$; a convex set containing exactly $\left \lceil \frac{r+1}{2}\right \rceil$ red points and exactly $\left \lceil \frac{b+1}{2}\right \rceil$ blue points of $S$. Furthermore, we present polynomial time algorithms to find these convex sets. In the first case we provide an $O(n^4)$ time algorithm and an $O(n^2\log n)$ time algorithm in the second case. Finally, if $\lceil \alpha r\rceil+\lceil \alpha b\rceil$ is small, that is, not much larger than $\frac{1}{3}n$, we improve the running time to $O(n \log n)$. ",Computer Science - Computational Geometry ; ,"Aichholzer, Oswin ; Atienza, Nieves ; Fabila-Monroy, Ruy ; Perez-Lantero, Pablo ; Dıaz-Báñez, Jose M. ; Flores-Peñaloza, David ; Vogtenhuber, Birgit ; Urrutia, Jorge ; "
http://arxiv.org/abs/1510.01844,Linear Bounds between Contraction Coefficients for $f$-Divergences,"  Data processing inequalities for $f$-divergences can be sharpened using constants called ""contraction coefficients"" to produce strong data processing inequalities. For any discrete source-channel pair, the contraction coefficients for $f$-divergences are lower bounded by the contraction coefficient for $\chi^2$-divergence. In this paper, we elucidate that this lower bound can be achieved by driving the input $f$-divergences of the contraction coefficients to zero. Then, we establish a linear upper bound on the contraction coefficients for a certain class of $f$-divergences using the contraction coefficient for $\chi^2$-divergence, and refine this upper bound for the salient special case of Kullback-Leibler (KL) divergence. Furthermore, we present an alternative proof of the fact that the contraction coefficients for KL and $\chi^2$-divergences are equal for a Gaussian source with an additive Gaussian noise channel (where the former coefficient can be power constrained). Finally, we generalize the well-known result that contraction coefficients of channels (after extremizing over all possible sources) for all $f$-divergences with non-linear operator convex $f$ are equal. In particular, we prove that the so called ""less noisy"" preorder over channels can be equivalently characterized by any non-linear operator convex $f$-divergence. ",Computer Science - Information Theory ; Mathematics - Probability ; Mathematics - Statistics Theory ; ,"Makur, Anuran ; Zheng, Lizhong ; "
http://arxiv.org/abs/1510.01913,On the Uniform Computational Content of the Baire Category Theorem,"  We study the uniform computational content of different versions of the Baire Category Theorem in the Weihrauch lattice. The Baire Category Theorem can be seen as a pigeonhole principle that states that a complete (i.e., ""large"") metric space cannot be decomposed into countably many nowhere dense (i.e., ""small"") pieces. The Baire Category Theorem is an illuminating example of a theorem that can be used to demonstrate that one classical theorem can have several different computational interpretations. For one, we distinguish two different logical versions of the theorem, where one can be seen as the contrapositive form of the other one. The first version aims to find an uncovered point in the space, given a sequence of nowhere dense closed sets. The second version aims to find the index of a closed set that is somewhere dense, given a sequence of closed sets that cover the space. Even though the two statements behind these versions are equivalent to each other in classical logic, they are not equivalent in intuitionistic logic and likewise they exhibit different computational behavior in the Weihrauch lattice. Besides this logical distinction, we also consider different ways how the sequence of closed sets is ""given"". Essentially, we can distinguish between positive and negative information on closed sets. We discuss all the four resulting versions of the Baire Category Theorem. Somewhat surprisingly it turns out that the difference in providing the input information can also be expressed with the jump operation. Finally, we also relate the Baire Category Theorem to notions of genericity and computably comeager sets. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; ,"Brattka, Vasco ; Hendtlass, Matthew ; Kreuzer, Alexander P. ; "
http://arxiv.org/abs/1510.02438,Decomposing 1-Sperner hypergraphs,"  A hypergraph is Sperner if no hyperedge contains another one. A Sperner hypergraph is equilizable (resp., threshold) if the characteristic vectors of its hyperedges are the (minimal) binary solutions to a linear equation (resp., inequality) with positive coefficients. These combinatorial notions have many applications and are motivated by the theory of Boolean functions and integer programming. We introduce in this paper the class of $1$-Sperner hypergraphs, defined by the property that for every two hyperedges the smallest of their two set differences is of size one. We characterize this class of Sperner hypergraphs by a decomposition theorem and derive several consequences from it. In particular, we obtain bounds on the size of $1$-Sperner hypergraphs and their transversal hypergraphs, show that the characteristic vectors of the hyperedges are linearly independent over the reals, and prove that $1$-Sperner hypergraphs are both threshold and equilizable. The study of $1$-Sperner hypergraphs is motivated also by their applications in graph theory, which we present in a companion paper. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C65, 94C10 ; ","Boros, Endre ; Gurvich, Vladimir ; Milanič, Martin ; "
http://arxiv.org/abs/1510.02659,Windrose Planarity: Embedding Graphs with Direction-Constrained Edges,"  Given a planar graph $G$ and a partition of the neighbors of each vertex $v$ in four sets $UR(v)$, $UL(v)$, $DL(v)$, and $DR(v)$, the problem Windrose Planarity asks to decide whether $G$ admits a windrose-planar drawing, that is, a planar drawing in which (i) each neighbor $u \in UR(v)$ is above and to the right of $v$, (ii) each neighbor $u \in UL(v)$ is above and to the left of $v$, (iii) each neighbor $u \in DL(v)$ is below and to the left of $v$, (iv) each neighbor $u \in DR(v)$ is below and to the right of $v$, and (v) edges are represented by curves that are monotone with respect to each axis. By exploiting both the horizontal and the vertical relationship among vertices, windrose-planar drawings allow to simultaneously visualize two partial orders defined by means of the edges of the graph.   Although the problem is NP-hard in the general case, we give a polynomial-time algorithm for testing whether there exists a windrose-planar drawing that respects a given combinatorial embedding. This algorithm is based on a characterization of the plane triangulations admitting a windrose-planar drawing. Furthermore, for any embedded graph with $n$ vertices that has a windrose-planar drawing, we can construct one with at most one bend per edge and with at most $2n-5$ bends in total, which lies on the $3n \times 3n$ grid. The latter result contrasts with the fact that straight-line windrose-planar drawings may require exponential area. ",Computer Science - Computational Geometry ; ,"Angelini, Patrizio ; Da Lozzo, Giordano ; Di Battista, Giuseppe ; Di Donato, Valentino ; Kindermann, Philipp ; Rote, Günter ; Rutter, Ignaz ; "
http://arxiv.org/abs/1510.02786,Recovering a Hidden Community Beyond the Kesten-Stigum Threshold in   $O(|E| \log^*|V|)$ Time,"  Community detection is considered for a stochastic block model graph of n vertices, with K vertices in the planted community, edge probability p for pairs of vertices both in the community, and edge probability q for other pairs of vertices.   The main focus of the paper is on weak recovery of the community based on the graph G, with o(K) misclassified vertices on average, in the sublinear regime $n^{1-o(1)} \leq K \leq o(n).$ A critical parameter is the effective signal-to-noise ratio $\lambda=K^2(p-q)^2/((n-K)q)$, with $\lambda=1$ corresponding to the Kesten-Stigum threshold. We show that a belief propagation algorithm achieves weak recovery if $\lambda>1/e$, beyond the Kesten-Stigum threshold by a factor of $1/e.$ The belief propagation algorithm only needs to run for $\log^\ast n+O(1) $ iterations, with the total time complexity $O(|E| \log^*n)$, where $\log^*n$ is the iterated logarithm of $n.$ Conversely, if $\lambda \leq 1/e$, no local algorithm can asymptotically outperform trivial random guessing. Furthermore, a linear message-passing algorithm that corresponds to applying power iteration to the non-backtracking matrix of the graph is shown to attain weak recovery if and only if $\lambda>1$. In addition, the belief propagation algorithm can be combined with a linear-time voting procedure to achieve the information limit of exact recovery (correctly classify all vertices with high probability) for all $K \ge \frac{n}{\log n} \left( \rho_{\rm BP} +o(1) \right),$ where $\rho_{\rm BP}$ is a function of $p/q$. ",Statistics - Machine Learning ; Computer Science - Computational Complexity ; Computer Science - Social and Information Networks ; Mathematics - Probability ; ,"Hajek, Bruce ; Wu, Yihong ; Xu, Jiaming ; "
http://arxiv.org/abs/1510.02787,The grounding for Continuum,"  It is a ubiquitous opinion among mathematicians that a real number is just a point in the line. If this rough definition is not enough, then a mathematician may provide a formal definition of the real numbers in the set theoretic and axiomatic fashion, i.e. via Cauchy sequences or Dedekind cuts, or as the collection of axioms characterizing exactly (up to isomorphism) the set of real numbers as the complete and totally ordered Archimedean field. Actually, the above notions of the real numbers are abstract and do not have a constructive grounding. Definition of Cauchy sequences, and equivalence classes of these sequences explicitly use the actual infinity. The same is for Dedekind cuts, where the set of rational numbers is used as actual infinity. Although there is no direct constructive grounding for these abstract notions, there are so called intuitions on which they are based. A rigorous approach to express these very intuition in a constructive way is proposed. It is based on the concept of the adjacency relation that seems to be a missing primitive concept in type theory. The approach corresponds to the intuitionistic view of Continuum proposed by Brouwer. The famous and controversial Brouwer Continuity Theorem is discussed on the basis of different principle than the Axiom of Continuity. The real numbers are the cornerstone of Calculus, Homotopy theory, Riemannian geometry, and many others important subjects. The proposed grounding of Continuum is discussed in the context of these subjects. ",Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03D ; F.4.1 ; ,"Ambroszkiewicz, Stanislaw ; "
http://arxiv.org/abs/1510.02807,Avoiding fractional powers over the natural numbers,"  We study the lexicographically least infinite $a/b$-power-free word on the alphabet of non-negative integers. Frequently this word is a fixed point of a uniform morphism, or closely related to one. For example, the lexicographically least $7/4$-power-free word is a fixed point of a $50847$-uniform morphism. We identify the structure of the lexicographically least $a/b$-power-free word for three infinite families of rationals $a/b$ as well many ""sporadic"" rationals that do not seem to belong to general families. To accomplish this, we develop an automated procedure for proving $a/b$-power-freeness for morphisms of a certain form, both for explicit and symbolic rational numbers $a/b$. Finally, we establish a connection to words on a finite alphabet. Namely, the lexicographically least $27/23$-power-free word is in fact a word on the finite alphabet $\{0, 1, 2\}$, and its sequence of letters is $353$-automatic. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Pudwell, Lara ; Rowland, Eric ; "
http://arxiv.org/abs/1510.02833,On the Definiteness of Earth Mover's Distance and Its Relation to Set   Intersection,"  Positive definite kernels are an important tool in machine learning that enable efficient solutions to otherwise difficult or intractable problems by implicitly linearizing the problem geometry. In this paper we develop a set-theoretic interpretation of the Earth Mover's Distance (EMD) and propose Earth Mover's Intersection (EMI), a positive definite analog to EMD for sets of different sizes. We provide conditions under which EMD or certain approximations to EMD are negative definite. We also present a positive-definite-preserving transformation that can be applied to any kernel and can also be used to derive positive definite EMD-based kernels and show that the Jaccard index is simply the result of this transformation. Finally, we evaluate kernels based on EMI and the proposed transformation versus EMD in various computer vision tasks and show that EMD is generally inferior even with indefinite kernel techniques. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Gardner, Andrew ; Duncan, Christian A. ; Kanno, Jinko ; Selmic, Rastko R. ; "
http://arxiv.org/abs/1510.02923,On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image   Rician Denoising,"  Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a Bayesian or generalized Tikhonov framework using Total Variation (TV) leads naturally to the consideration of nonlinear elliptic equations. These involve the so called $1$-Laplacian operator and special care is needed to properly formulate the problem. The rician statistics of the data are introduced through a singular equation with a reaction term defined in terms of modified first order Bessel functions. An existence theory is provided here together with other qualitative properties of the solutions. Remarkably, each positive global minimum of the associated functional is one of such solutions. Moreover, we directly solve this non--smooth non--convex minimization problem using a convergent Proximal Point Algorithm. Numerical results based on synthetic and real MRI demonstrate a better performance of the proposed method when compared to previous TV based models for rician denoising which regularize or convexify the problem. Finally, an application on real Diffusion Tensor Images, a strongly affected by rician noise MRI modality, is presented and discussed. ",Mathematics - Analysis of PDEs ; Computer Science - Computer Vision and Pattern Recognition ; Mathematics - Numerical Analysis ; ,"Martin, Adrian ; Schiavi, Emanuele ; de Leon, Sergio Segura ; "
http://arxiv.org/abs/1510.03170,Fair and Square: Cake-Cutting in Two Dimensions,"  We consider the classic problem of fairly dividing a heterogeneous good (""cake"") among several agents with different valuations. Classic cake-cutting procedures either allocate each agent a collection of disconnected pieces, or assume that the cake is a one-dimensional interval. In practice, however, the two-dimensional shape of the allotted pieces is important. In particular, when building a house or designing an advertisement in printed or electronic media, squares are more usable than long and narrow rectangles. We thus introduce and study the problem of fair two-dimensional division wherein the allotted pieces must be of some restricted two-dimensional geometric shape(s), particularly squares and fat rectangles. Adding such geometric constraints re-opens most questions and challenges related to cake-cutting. Indeed, even the most elementary fairness criterion --- proportionality --- can no longer be guaranteed. In this paper we thus examine the level of proportionality that can be guaranteed, providing both impossibility results and constructive division procedures. ",Computer Science - Computer Science and Game Theory ; Computer Science - Computational Geometry ; ,"Segal-Halevi, Erel ; Nitzan, Shmuel ; Hassidim, Avinatan ; Aumann, Yonatan ; "
http://arxiv.org/abs/1510.03271,A Core Model for Choreographic Programming,"  Choreographic Programming is a programming paradigm for building concurrent programs that are deadlock-free by construction, as a result of programming communications declaratively and then synthesising process implementations automatically. Despite strong interest on choreographies, a foundational model that explains which computations can be performed with the hallmark constructs of choreographies is still missing.   In this work, we introduce Core Choreographies (CC), a model that includes only the core primitives of choreographic programming. Every computable function can be implemented as a choreography in CC, from which we can synthesise a process implementation where independent computations run in parallel. We discuss the design of CC and argue that it constitutes a canonical model for choreographic programming. ",Computer Science - Programming Languages ; ,"Cruz-Filipe, Luís ; Montesi, Fabrizio ; "
http://arxiv.org/abs/1510.03637,Applied Choreographies,"  Choreographic Programming is a correct-by-construction paradigm for distributed programming, where global declarative descriptions of communications (choreographies) are used to synthesise deadlock-free processes. Choreographies are global descriptions of communications in concurrent systems, which have been used in different methodologies for the verification or synthesis of programs. However, there is no formalisation that provides a chain of correctness from choreographies to their implementations. This problem originates from the gap between previous theoretical models, which abstract communications using channel names (\`a la CCS/$\pi$-calculus), and their implementations, which use low-level mechanisms for message routing. As a solution, we propose the framework of Applied Choreographies (AC). In AC, programmers write choreographies in a language that follows the standard syntax and semantics of previous works. Then, choreographies are compiled to a real-world execution model for Service-Oriented Computing (SOC). To manage the complexity of this task, our compilation happens in three steps, respectively dealing with: implementing name-based communications using the concrete mechanism found in SOC, projecting a choreography to a set of processes, and translating processes to a distributed implementation in terms of services. For each step a suitable correspondence result guarantees that the behaviour is preserved, thus ensuring the correctness of the global compilation process. This is the first correctness result of an end-to-end translation from standard choreographies to programs based on a ""real-world"" communication mechanism. ",Computer Science - Programming Languages ; ,"Giallorenzo, Saverio ; Montesi, Fabrizio ; Gabbrielli, Maurizio ; "
http://arxiv.org/abs/1510.03840,Dynamic Spectrum Sensing Through Accelerated Particle Swarm Optimization,"  In this paper, a novel optimization algorithm, called the acceleration-aided particle swarm optimization (AAPSO), is proposed for reliable dynamic spectrum sensing in cognitive radio networks. In A-APSO, the acceleration variable of the particles in the swarm is also considered in the search space of the optimization problem. We show that the proposed A-APSO based spectrum sensing technique is more efficient in terms of performance than the corresponding one based on the standard particle swarm optimization algorithm. ",Mathematics - Optimization and Control ; Computer Science - Information Theory ; Statistics - Applications ; ,"Paschos, Alexandros E. ; Kapinas, Vasileios M. ; Ntouni, Georgia D. ; Hadjileontiadis, Leontios J. ; Karagiannidis, George K. ; "
http://arxiv.org/abs/1510.03895,A faster subquadratic algorithm for finding outlier correlations,"  We study the problem of detecting outlier pairs of strongly correlated variables among a collection of $n$ variables with otherwise weak pairwise correlations. After normalization, this task amounts to the geometric task where we are given as input a set of $n$ vectors with unit Euclidean norm and dimension $d$, and for some constants $0<\tau<\rho<1$, we are asked to find all the outlier pairs of vectors whose inner product is at least $\rho$ in absolute value, subject to the promise that all but at most $q$ pairs of vectors have inner product at most $\tau$ in absolute value.   Improving on an algorithm of G. Valiant [FOCS 2012; J. ACM 2015], we present a randomized algorithm that for Boolean inputs ($\{-1,1\}$-valued data normalized to unit Euclidean length) runs in time \[ \tilde O\bigl(n^{\max\,\{1-\gamma+M(\Delta\gamma,\gamma),\,M(1-\gamma,2\Delta\gamma)\}}+qdn^{2\gamma}\bigr)\,, \] where $0<\gamma<1$ is a constant tradeoff parameter and $M(\mu,\nu)$ is the exponent to multiply an $\lfloor n^\mu\rfloor\times\lfloor n^\nu\rfloor$ matrix with an $\lfloor n^\nu\rfloor\times \lfloor n^\mu\rfloor$ matrix and $\Delta=1/(1-\log_\tau\rho)$. As corollaries we obtain randomized algorithms that run in time \[ \tilde O\bigl(n^{\frac{2\omega}{3-\log_\tau\rho}}+qdn^{\frac{2(1-\log_\tau\rho)}{3-\log_\tau\rho}}\bigr) \] and in time \[ \tilde O\bigl(n^{\frac{4}{2+\alpha(1-\log_\tau\rho)}}+qdn^{\frac{2\alpha(1-\log_\tau\rho)}{2+\alpha(1-\log_\tau\rho)}}\bigr)\,, \] where $2\leq\omega<2.38$ is the exponent for square matrix multiplication and $0.3<\alpha\leq 1$ is the exponent for rectangular matrix multiplication. The notation $\tilde O(\cdot)$ hides polylogarithmic factors in $n$ and $d$ whose degree may depend on $\rho$ and $\tau$. We present further corollaries for the light bulb problem and for learning sparse Boolean functions. ","Computer Science - Data Structures and Algorithms ; 65F30, 68W20, 62H20, 68T05, 68Q32 ; F.2.1 ; I.1.2 ; G.3 ; H.2.8 ; H.3.3 ; I.2.6 ; ","Karppa, Matti ; Kaski, Petteri ; Kohonen, Jukka ; "
http://arxiv.org/abs/1510.03903,Fair Cake-Cutting among Families,"  We study the fair division of a continuous resource, such as a land-estate or a time-interval, among pre-specified groups of agents, such as families. Each family is given a piece of the resource and this piece is used simultaneously by all family members, while different members may have different value functions. Three ways to assess the fairness of such a division are examined. (a) *Average fairness* means that each family's share is fair according to the ""family value function"", defined as the arithmetic mean of the value functions of the family members. (b) *Unanimous fairness* means that all members in all families feel that their family received a fair share according to their personal value function. (c) *Democratic fairness* means that in each family, at least half the members feel that their family's share is fair. We compare these criteria based on the number of connected components in the resulting division, and based on their compatibility with Pareto-efficiency. ",Computer Science - Computer Science and Game Theory ; ,"Segal-Halevi, Erel ; Nitzan, Shmuel ; "
http://arxiv.org/abs/1510.03935,Surface Approximation via Asymptotic Optimal Geometric Partition,"  In this paper, we present a surface remeshing method with high approximation quality based on Principal Component Analysis. Given a triangular mesh and a user assigned polygon/vertex budget, traditional methods usually require the extra curvature metric field for the desired anisotropy to best approximate the surface, even though the estimated curvature metric is known to be imperfect and already self-contained in the surface. In our approach, this anisotropic control is achieved through the optimal geometry partition without this explicit metric field. The minimization of our proposed partition energy has the following properties: Firstly, on a C2 surface, it is theoretically guaranteed to have the optimal aspect ratio and cluster size as specified in approximation theory for L1 piecewise linear approximation. Secondly, it captures sharp features on practical models without any pre-tagging. We develop an effective merging-swapping framework to seek the optimal partition and construct polygonal/triangular mesh afterwards. The effectiveness and efficiency of our method are demonstrated through the comparison with other state-of-the-art remeshing methods. ",Computer Science - Graphics ; ,"Cai, Yiqi ; Guo, Xiaohu ; Liu, Yang ; Wang, Wenping ; Mao, Weihua ; Zhong, Zichun ; "
http://arxiv.org/abs/1510.04249,Random Irregular Block-hierarchical Networks: Algorithms for Computation   of Main Properties,"  In this paper, the class of random irregular block-hierarchical networks is defined and algorithms for generation and calculation of network properties are described. The algorithms presented for this class of networks are more efficient than known algorithms both in computation time and memory usage and can be used to analyze topological properties of such networks. The algorithms are implemented in the system created by the authors for the study of topological and statistical properties of random networks. ",Computer Science - Data Structures and Algorithms ; ,"Avetisyan, Svetlana ; Samvelyan, Mikayel ; Karapetyan, Martun ; "
http://arxiv.org/abs/1510.04389,Sketch-based Manga Retrieval using Manga109 Dataset,"  Manga (Japanese comics) are popular worldwide. However, current e-manga archives offer very limited search support, including keyword-based search by title or author, or tag-based categorization. To make the manga search experience more intuitive, efficient, and enjoyable, we propose a content-based manga retrieval system. First, we propose a manga-specific image-describing framework. It consists of efficient margin labeling, edge orientation histogram feature description, and approximate nearest-neighbor search using product quantization. Second, we propose a sketch-based interface as a natural way to interact with manga content. The interface provides sketch-based querying, relevance feedback, and query retouch. For evaluation, we built a novel dataset of manga images, Manga109, which consists of 109 comic books of 21,142 pages drawn by professional manga artists. To the best of our knowledge, Manga109 is currently the biggest dataset of manga images available for research. We conducted a comparative study, a localization evaluation, and a large-scale qualitative study. From the experiments, we verified that: (1) the retrieval accuracy of the proposed method is higher than those of previous methods; (2) the proposed method can localize an object instance with reasonable runtime and accuracy; and (3) sketch querying is useful for manga search. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Information Retrieval ; Computer Science - Multimedia ; ,"Matsui, Yusuke ; Ito, Kota ; Aramaki, Yuji ; Yamasaki, Toshihiko ; Aizawa, Kiyoharu ; "
http://arxiv.org/abs/1510.04390,Dual Principal Component Pursuit,"  We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex $\ell_1$ minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; ,"Tsakiris, Manolis C. ; Vidal, Rene ; "
http://arxiv.org/abs/1510.04524,"Old Bands, New Tracks---Revisiting the Band Model for Robust Hypothesis   Testing","  The density band model proposed by Kassam for robust hypothesis testing is revisited in this paper. First, a novel criterion for the general characterization of least favorable distributions is proposed, which unifies existing results. This criterion is then used to derive an implicit definition of the least favorable distributions under band uncertainties. In contrast to the existing solution, it only requires two scalar values to be determined and eliminates the need for case-by-case statements. Based on this definition, a generic fixed-point algorithm is proposed that iteratively calculates the least favorable distributions for arbitrary band specifications. Finally, three different types of robust tests that emerge from band models are discussed and a numerical example is presented to illustrate their potential use in practice. ",Computer Science - Information Theory ; 62C20 ; ,"Fauß, Michael ; Zoubir, Abdelhak M. ; "
http://arxiv.org/abs/1510.04780,A Graph Traversal Based Approach to Answer Non-Aggregation Questions   Over DBpedia,"  We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems. ",Computer Science - Computation and Language ; Computer Science - Information Retrieval ; ,"Zhu, Chenhao ; Ren, Kan ; Liu, Xuan ; Wang, Haofen ; Tian, Yiding ; Yu, Yong ; "
http://arxiv.org/abs/1510.05071,Distributed Robust Adaptive Frequency Control of Power Systems with   Dynamic Loads,  This paper investigates the frequency control of multi-machine power systems subject to uncertain and dynamic net loads. We propose distributed internal model controllers which coordinate synchronous generators and demand response to tackle the unpredictable nature of net loads. Frequency stability is formally guaranteed via Lyapunov analysis. Numerical simulations on the IEEE 68-bus test system as well as the Minni-WECC system demonstrate the effectiveness of the controllers and performance under a three-phase fault and load-switching during light/peak loads. ,Computer Science - Systems and Control ; ,"Kim, Hunmin ; Zhu, Minghui ; Lian, Jianming ; "
http://arxiv.org/abs/1510.05175,Top-k Query Processing on Encrypted Databases with Strong Security   Guarantees,"  Privacy concerns in outsourced cloud databases have become more and more important recently and many efficient and scalable query processing methods over encrypted data have been proposed. However, there is very limited work on how to securely process top-k ranking queries over encrypted databases in the cloud. In this paper, we focus exactly on this problem: secure and efficient processing of top-k queries over outsourced databases. In particular, we propose the first efficient and provable secure top-k query processing construction that achieves adaptively CQA security. We develop an encrypted data structure called EHL and describe several secure sub-protocols under our security model to answer top-k queries. Furthermore, we optimize our query algorithms for both space and time efficiency. Finally, in the experiments, we empirically analyze our protocol using real world datasets and demonstrate that our construction is efficient and practical. ",Computer Science - Cryptography and Security ; H.2.7 ; K.4.4 ; H.2.4 ; ,"Meng, Xianrui ; Zhu, Haohan ; Kollios, George ; "
http://arxiv.org/abs/1510.05229,Monotonicity and Competitive Equilibrium in Cake-cutting,"  We study the monotonicity properties of solutions in the classic problem of fair cake-cutting --- dividing a heterogeneous resource among agents with different preferences. Resource- and population-monotonicity relate to scenarios where the cake, or the number of participants who divide the cake, changes. It is required that the utility of all participants change in the same direction: either all of them are better-off (if there is more to share or fewer to share among) or all are worse-off (if there is less to share or more to share among).   We formally introduce these concepts to the cake-cutting problem and examine whether they are satisfied by various common division rules. We prove that the Nash-optimal rule, which maximizes the product of utilities, is resource-monotonic and population-monotonic, in addition to being Pareto-optimal, envy-free and satisfying a strong competitive-equilibrium condition. Moreover, we prove that it is the only rule among a natural family of welfare-maximizing rules that is both proportional and resource-monotonic. ",Computer Science - Computer Science and Game Theory ; ,"Segal-Halevi, Erel ; Sziklai, Balázs ; "
http://arxiv.org/abs/1510.05278,A First Practical Fully Homomorphic Crypto-Processor Design: The Secret   Computer is Nearly Here,"  Following a sequence of hardware designs for a fully homomorphic crypto-processor - a general purpose processor that natively runs encrypted machine code on encrypted data in registers and memory, resulting in encrypted machine states - proposed by the authors in 2014, we discuss a working prototype of the first of those, a so-called `pseudo-homomorphic' design. This processor is in principle safe against physical or software-based attacks by the owner/operator of the processor on user processes running in it. The processor is intended as a more secure option for those emerging computing paradigms that require trust to be placed in computations carried out in remote locations or overseen by untrusted operators.   The prototype has a single-pipeline superscalar architecture that runs OpenRISC standard machine code in two distinct modes. The processor runs in the encrypted mode (the unprivileged, `user' mode, with a long pipeline) at 60-70% of the speed in the unencrypted mode (the privileged, `supervisor' mode, with a short pipeline), emitting a completed encrypted instruction every 1.67-1.8 cycles on average in real trials. ",Computer Science - Cryptography and Security ; ,"Breuer, Peter ; Bowen, Jonathan ; "
http://arxiv.org/abs/1510.05461,Confidence Sets for the Source of a Diffusion in Regular Trees,"  We study the problem of identifying the source of a diffusion spreading over a regular tree. When the degree of each node is at least three, we show that it is possible to construct confidence sets for the diffusion source with size independent of the number of infected nodes. Our estimators are motivated by analogous results in the literature concerning identification of the root node in preferential attachment and uniform attachment trees. At the core of our proofs is a probabilistic analysis of P\'{o}lya urns corresponding to the number of uninfected neighbors in specific subtrees of the infection tree. We also provide an example illustrating the shortcomings of source estimation techniques in settings where the underlying graph is asymmetric. ",Mathematics - Statistics Theory ; Computer Science - Discrete Mathematics ; Computer Science - Social and Information Networks ; Mathematics - Probability ; Statistics - Machine Learning ; 62M99 ; ,"Khim, Justin ; Loh, Po-Ling ; "
http://arxiv.org/abs/1510.05751,Semi-Implicit Time Integration of Atmospheric Flows with   Characteristic-Based Flux Partitioning,"  This paper presents a characteristic-based flux partitioning for the semi-implicit time integration of atmospheric flows. Nonhydrostatic models require the solution of the compressible Euler equations. The acoustic time-scale is significantly faster than the advective scale, yet it is typically not relevant to atmospheric and weather phenomena. The acoustic and advective components of the hyperbolic flux are separated in the characteristic space. High-order, conservative additive Runge-Kutta methods are applied to the partitioned equations so that the acoustic component is integrated in time implicitly with an unconditionally stable method, while the advective component is integrated explicitly. The time step of the overall algorithm is thus determined by the advective scale. Benchmark flow problems are used to demonstrate the accuracy, stability, and convergence of the proposed algorithm. The computational cost of the partitioned semi-implicit approach is compared with that of explicit time integration. ","Computer Science - Computational Engineering, Finance, and Science ; Mathematics - Numerical Analysis ; 65M06, 86A10, 76N15 ; ","Ghosh, Debojyoti ; Constantinescu, Emil M. ; "
http://arxiv.org/abs/1510.06423,Optimization as Estimation with Gaussian Processes in Bandit Settings,"  Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Wang, Zi ; Zhou, Bolei ; Jegelka, Stefanie ; "
http://arxiv.org/abs/1510.06482,Triangular Alignment (TAME): A Tensor-based Approach for Higher-order   Network Alignment,"  Network alignment has extensive applications in comparative interactomics. Traditional approaches aim to simultaneously maximize the number of conserved edges and the underlying similarity of aligned entities. We propose a novel formulation of the network alignment problem that extends topological similarity to higher-order structures and provides a new objective function that maximizes the number of aligned substructures. This objective function corresponds to an integer programming problem, which is NP-hard. Consequently, we identify a closely related surrogate function whose maximization results in a tensor eigenvector problem. Based on this formulation, we present an algorithm called Triangular AlignMEnt (TAME), which attempts to maximize the number of aligned triangles across networks. Using a case study on the NAPAbench dataset, we show that triangular alignment is capable of producing mappings with high node correctness. We further evaluate our method by aligning yeast and human interactomes. Our results indicate that TAME outperforms the state-of-art alignment methods in terms of conserved triangles. In addition, we show that the number of conserved triangles is more significantly correlated, compared to the conserved edge, with node correctness and co-expression of edges. Our formulation and resulting algorithms can be easily extended to arbitrary motifs. ","Computer Science - Computational Engineering, Finance, and Science ; ","Mohammadi, Shahin ; Gleich, David ; Kolda, Tamara ; Grama, Ananth ; "
http://arxiv.org/abs/1510.06684,Dual Free Adaptive Mini-batch SDCA for Empirical Risk Minimization,"  In this paper we develop dual free mini-batch SDCA with adaptive probabilities for regularized empirical risk minimization. This work is motivated by recent work of Shai Shalev-Shwartz on dual free SDCA method, however, we allow a non-uniform selection of ""dual"" coordinates in SDCA. Moreover, the probability can change over time, making it more efficient than fix uniform or non-uniform selection. We also propose an efficient procedure to generate a random non-uniform mini-batch through iterative process. The work is concluded with multiple numerical experiments to show the efficiency of proposed algorithms. ",Mathematics - Optimization and Control ; Computer Science - Machine Learning ; ,"He, Xi ; Takáč, Martin ; "
http://arxiv.org/abs/1510.06750,Quantum vs Classical Proofs and Subset Verification,"  We study the ability of efficient quantum verifiers to decide properties of exponentially large subsets given either a classical or quantum witness. We develop a general framework that can be used to prove that QCMA machines, with only classical witnesses, cannot verify certain properties of subsets given implicitly via an oracle. We use this framework to prove an oracle separation between QCMA and QMA using an ""in-place"" permutation oracle, making the first progress on this question since Aaronson and Kuperberg in 2007. We also use the framework to prove a particularly simple standard oracle separation between QCMA and AM. ",Quantum Physics ; Computer Science - Computational Complexity ; ,"Fefferman, Bill ; Kimmel, Shelby ; "
http://arxiv.org/abs/1510.07135,Induced minors and well-quasi-ordering,"  A graph $H$ is an induced minor of a graph $G$ if it can be obtained from an induced subgraph of $G$ by contracting edges. Otherwise, $G$ is said to be $H$-induced minor-free. Robin Thomas showed that $K_4$-induced minor-free graphs are well-quasi-ordered by induced minors [Graphs without $K_4$ and well-quasi-ordering, Journal of Combinatorial Theory, Series B, 38(3):240 -- 247, 1985].   We provide a dichotomy theorem for $H$-induced minor-free graphs and show that the class of $H$-induced minor-free graphs is well-quasi-ordered by the induced minor relation if and only if $H$ is an induced minor of the gem (the path on 4 vertices plus a dominating vertex) or of the graph obtained by adding a vertex of degree 2 to the complete graph on 4 vertices. To this end we proved two decomposition theorems which are of independent interest.   Similar dichotomy results were previously given for subgraphs by Guoli Ding in [Subgraphs and well-quasi-ordering, Journal of Graph Theory, 16(5):489--502, 1992] and for induced subgraphs by Peter Damaschke in [Induced subgraphs and well-quasi-ordering, Journal of Graph Theory, 14(4):427--435, 1990]. ","Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; 05C, 06A07 ; G.2.2 ; ","Błasiok, Jarosław ; Kamiński, Marcin ; Raymond, Jean-Florent ; Trunck, Théophile ; "
http://arxiv.org/abs/1510.07357,Distributed Bare-Bones Communication in Wireless Networks,"  We consider wireless networks with the SINR model of interference when nodes have very limited individual knowledge and capabilities. In particular, nodes do not know their geographic coordinates and their neighborhoods, nor even the size $n$ of the network, nor can they sense collisions. Each node is equipped only with a unique integer name, where $N$ is an upper bound on their range. We study distributed algorithms for communication problems in the following three settings. In the single-node-start case, when one node starts an execution and the other nodes are awoken by receiving messages from already awoken nodes, we present a randomized broadcast algorithm which wakes up all the nodes in $O(n \log^2 N)$ rounds with high probability. Let $\Delta$ denote the maximum number of nodes that successfully receive a message transmitted by a node when no other nodes transmit. For the synchronized-start case, when all the nodes start an execution simultaneously, we give a randomized algorithm that computes a backbone of the network in $O(\Delta\log^{7} N)$ rounds with high probability. Finally, in the partly-coordinated-start case, when a number of nodes start an execution together and other nodes are awoken by receiving messages from the already awoken nodes, we develop an algorithm that creates a backbone network in time $O(n\log^2 N +\Delta\log^{7} N)$ with high probability. ","Computer Science - Distributed, Parallel, and Cluster Computing ; ","Chlebus, Bogdan S. ; Kowalski, Dariusz R. ; Vaya, Shailesh ; "
http://arxiv.org/abs/1510.07380,SLAP: Simultaneous Localization and Planning Under Uncertainty for   Physical Mobile Robots via Dynamic Replanning in Belief Space: Extended   version,"  Simultaneous localization and Planning (SLAP) is a crucial ability for an autonomous robot operating under uncertainty. In its most general form, SLAP induces a continuous POMDP (partially-observable Markov decision process), which needs to be repeatedly solved online. This paper addresses this problem and proposes a dynamic replanning scheme in belief space. The underlying POMDP, which is continuous in state, action, and observation space, is approximated offline via sampling-based methods, but operates in a replanning loop online to admit local improvements to the coarse offline policy. This construct enables the proposed method to combat changing environments and large localization errors, even when the change alters the homotopy class of the optimal trajectory. It further outperforms the state-of-the-art FIRM (Feedback-based Information RoadMap) method by eliminating unnecessary stabilization steps. Applying belief space planning to physical systems brings with it a plethora of challenges. A key focus of this paper is to implement the proposed planner on a physical robot and show the SLAP solution performance under uncertainty, in changing environments and in the presence of large disturbances, such as a kidnapped robot situation. ",Computer Science - Robotics ; Computer Science - Systems and Control ; ,"Agha-mohammadi, Ali-akbar ; Agarwal, Saurav ; Kim, Sung-Kyun ; Chakravorty, Suman ; Amato, Nancy M. ; "
http://arxiv.org/abs/1510.07391,Vehicle Color Recognition using Convolutional Neural Network,"  Vehicle color information is one of the important elements in ITS (Intelligent Traffic System). In this paper, we present a vehicle color recognition method using convolutional neural network (CNN). Naturally, CNN is designed to learn classification method based on shape information, but we proved that CNN can also learn classification based on color distribution. In our method, we convert the input image to two different color spaces, HSV and CIE Lab, and run it to some CNN architecture. The training process follow procedure introduce by Krizhevsky, that learning rate is decreasing by factor of 10 after some iterations. To test our method, we use publicly vehicle color recognition dataset provided by Chen. The results, our model outperform the original system provide by Chen with 2% higher overall accuracy. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Rachmadi, Reza Fuad ; Purnama, I Ketut Eddy ; "
http://arxiv.org/abs/1510.07424,The Impossibility of Extending Random Dictatorship to Weak Preferences,"  Random dictatorship has been characterized as the only social decision scheme that satisfies efficiency and strategyproofness when individual preferences are strict. We show that no extension of random dictatorship to weak preferences satisfies these properties, even when significantly weakening the required degree of strategyproofness. ",Computer Science - Multiagent Systems ; Computer Science - Computer Science and Game Theory ; C.6 ; D.7 ; D.8 ; ,"Brandl, Florian ; Brandt, Felix ; Suksompong, Warut ; "
http://arxiv.org/abs/1510.08183,Raptor Codes in the Low SNR Regime,"  In this paper, we revisit the design of Raptor codes for binary input additive white Gaussian noise (BIAWGN) channels, where we are interested in very low signal to noise ratios (SNRs). A linear programming degree distribution optimization problem is defined for Raptor codes in the low SNR regime through several approximations. We also provide an exact expression for the polynomial representation of the degree distribution with infinite maximum degree in the low SNR regime, which enables us to calculate the exact value of the fractions of output nodes of small degrees. A more practical degree distribution design is also proposed for Raptor codes in the low SNR regime, where we include the rate efficiency and the decoding complexity in the optimization problem, and an upper bound on the maximum rate efficiency is derived for given design parameters. Simulation results show that the Raptor code with the designed degree distributions can approach rate efficiencies larger than 0.95 in the low SNR regime. ",Computer Science - Information Theory ; ,"Shirvanimoghaddam, Mahyar ; Johnson, Sarah J. ; "
http://arxiv.org/abs/1510.08202,Oblivious Fronthaul-Constrained Relay for a Gaussian Channel,"  We consider systems in which the transmitter conveys messages to the receiver through a capacity-limited relay station. The channel between the transmitter and the relay-station is assumed to be a frequency selective additive Gaussian noise channel. It is assumed that the transmitter can shape the spectrum and adapt the coding technique so as to optimize performance. The relay operation is oblivious (nomadic transmitters), that is, the specific codebooks used are unknown. We find the reliable information rate that can be achieved with Gaussian signaling in this setting, and to that end, employ Gaussian bottleneck results combined with Shannon's incremental frequency approach. We also prove that, unlike classical water-pouring, the allocated spectrum (power and bit-rate) of the optimal solution could frequently be discontinuous. These results can be applied to a MIMO transmission scheme. We also investigate the case of an entropy limited relay. We present lower and upper bounds on the optimal performance (in terms of mutual information), and derive an analytical approximation. ",Computer Science - Information Theory ; ,"Homri, Adi ; Peleg, Michael ; Shamai, Shlomo ; "
http://arxiv.org/abs/1510.08368,Switching control for incremental stabilization of nonlinear systems via   contraction theory,"  In this paper we present a switching control strategy to incrementally stabilize a class of nonlinear dynamical systems. Exploiting recent results on contraction analysis of switched Filippov systems derived using regularization, sufficient conditions are presented to prove incremental stability of the closed-loop system. Furthermore, based on these sufficient conditions, a design procedure is proposed to design a switched control action that is active only where the open-loop system is not sufficiently incrementally stable in order to reduce the required control effort. The design procedure to either locally or globally incrementally stabilize a dynamical system is then illustrated by means of a representative example. ",Computer Science - Systems and Control ; ,"di Bernardo, Mario ; Fiore, Davide ; "
http://arxiv.org/abs/1510.08417,Monotone Projection Lower Bounds from Extended Formulation Lower Bounds,"  In this short note, we reduce lower bounds on monotone projections of polynomials to lower bounds on extended formulations of polytopes. Applying our reduction to the seminal extended formulation lower bounds of Fiorini, Massar, Pokutta, Tiwari, & de Wolf (STOC 2012; J. ACM, 2015) and Rothvoss (STOC 2014; J. ACM, 2017), we obtain the following interesting consequences.   1. The Hamiltonian Cycle polynomial is not a monotone subexponential-size projection of the permanent; this both rules out a natural attempt at a monotone lower bound on the Boolean permanent, and shows that the permanent is not complete for non-negative polynomials in VNP$_{{\mathbb R}}$ under monotone p-projections.   2. The cut polynomials and the perfect matching polynomial (or ""unsigned Pfaffian"") are not monotone p-projections of the permanent. The latter, over the Boolean and-or semi-ring, rules out monotone reductions in one of the natural approaches to reducing perfect matchings in general graphs to perfect matchings in bipartite graphs.   As the permanent is universal for monotone formulas, these results also imply exponential lower bounds on the monotone formula size and monotone circuit size of these polynomials. ","Computer Science - Computational Complexity ; 68Q15, 68Q17, 90C05, 15A15, 05C70 ; F.1.3 ; F.2.1 ; G.1.6 ; ","Grochow, Joshua A. ; "
http://arxiv.org/abs/1510.08578,My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em   Competition,"  The first ever human vs. computer no-limit Texas hold 'em competition took place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this article I present my thoughts on the competition design, agent architecture, and lessons learned. ",Computer Science - Computer Science and Game Theory ; Computer Science - Artificial Intelligence ; Computer Science - Multiagent Systems ; Economics - Theoretical Economics ; ,"Ganzfried, Sam ; "
http://arxiv.org/abs/1510.08779,Effect of Gromov-hyperbolicity Parameter on Cuts and Expansions in   Graphs and Some Algorithmic Implications,"  $\delta$-hyperbolic graphs, originally conceived by Gromov in 1987, occur often in many network applications; for fixed $\delta$, such graphs are simply called hyperbolic graphs and include non-trivial interesting classes of ""non-expander"" graphs. The main motivation of this paper is to investigate the effect of the hyperbolicity measure $\delta$ on expansion and cut-size bounds on graphs (here $\delta$ need not be a constant), and the asymptotic ranges of $\delta$ for which these results may provide improved approximation algorithms for related combinatorial problems. To this effect, we provide constructive bounds on node expansions for $\delta$-hyperbolic graphs as a function of $\delta$, and show that many witnesses (subsets of nodes) for such expansions can be computed efficiently even if the witnesses are required to be nested or sufficiently distinct from each other. To the best of our knowledge, these are the first such constructive bounds proven. We also show how to find a large family of s-t cuts with relatively small number of cut-edges when s and t are sufficiently far apart. We then provide algorithmic consequences of these bounds and their related proof techniques for two problems for $\delta$-hyperbolic graphs (where $\delta$ is a function $f$ of the number of nodes, the exact nature of growth of $f$ being dependent on the particular problem considered). ","Computer Science - Computational Complexity ; Computer Science - Discrete Mathematics ; Mathematics - Combinatorics ; 68Q25, 68W25, 68W40, 05C85 ; F.2.2 ; G.2.2 ; I.1.2 ; ","DasGupta, Bhaskar ; Karpinski, Marek ; Mobasheri, Nasim ; Yahyanejad, Farzaneh ; "
http://arxiv.org/abs/1510.08797,Convexity in Tree Spaces,"  We study the geometry of metrics and convexity structures on the space of phylogenetic trees, which is here realized as the tropical linear space of all \ ultrametrics. The ${\rm CAT}(0)$-metric of Billera-Holmes-Vogtman arises from the theory of orthant spaces. While its geodesics can be computed by the Owen-Provan algorithm, geodesic triangles are complicated. We show that the dimension of such a triangle can be arbitrarily high. Tropical convexity and the tropical metric behave better. They exhibit properties desirable for geometric statistics, such as geodesics of small depth. ",Mathematics - Metric Geometry ; Computer Science - Computational Geometry ; Mathematics - Combinatorics ; Quantitative Biology - Populations and Evolution ; ,"Lin, Bo ; Sturmfels, Bernd ; Tang, Xiaoxian ; Yoshida, Ruriko ; "
http://arxiv.org/abs/1510.08887,Phase Retrieval Using Unitary 2-Designs,"  We consider a variant of the phase retrieval problem, where vectors are replaced by unitary matrices, i.e., the unknown signal is a unitary matrix U, and the measurements consist of squared inner products |Tr(C*U)|^2 with unitary matrices C that are chosen by the observer. This problem has applications to quantum process tomography, when the unknown process is a unitary operation.   We show that PhaseLift, a convex programming algorithm for phase retrieval, can be adapted to this matrix setting, using measurements that are sampled from unitary 4- and 2-designs. In the case of unitary 4-design measurements, we show that PhaseLift can reconstruct all unitary matrices, using a near-optimal number of measurements. This extends previous work on PhaseLift using spherical 4-designs.   In the case of unitary 2-design measurements, we show that PhaseLift still works pretty well on average: it recovers almost all signals, up to a constant additive error, using a near-optimal number of measurements. These 2-design measurements are convenient for quantum process tomography, as they can be implemented via randomized benchmarking techniques. This is the first positive result on PhaseLift using 2-designs. ",Quantum Physics ; Computer Science - Information Theory ; Mathematics - Statistics Theory ; ,"Kimmel, Shelby ; Liu, Yi-Kai ; "
http://arxiv.org/abs/1510.08983,Highway Long Short-Term Memory RNNs for Distant Speech Recognition,"  In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and $5.3\%$ relative improvement respectively. ",Computer Science - Neural and Evolutionary Computing ; Computer Science - Artificial Intelligence ; Computer Science - Computation and Language ; Computer Science - Machine Learning ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Zhang, Yu ; Chen, Guoguo ; Yu, Dong ; Yao, Kaisheng ; Khudanpur, Sanjeev ; Glass, James ; "
http://arxiv.org/abs/1510.08985,Prediction-Adaptation-Correction Recurrent Neural Networks for   Low-Resource Language Speech Recognition,"  In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\it correction} network uses auxiliary information given by a {\it prediction} network to help estimate the state probability. The information from the correction network is also used by the prediction network in a recurrent loop. Our model outperforms other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks. Moreover, transfer learning from a language that is similar to the target language can help improve performance further. ",Computer Science - Computation and Language ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; Electrical Engineering and Systems Science - Audio and Speech Processing ; ,"Zhang, Yu ; Chuangsuwanich, Ekapol ; Glass, James ; Yu, Dong ; "
http://arxiv.org/abs/1510.09102,Trace Refinement in Labelled Markov Decision Processes,"  Given two labelled Markov decision processes (MDPs), the trace-refinement problem asks whether for all strategies of the first MDP there exists a strategy of the second MDP such that the induced labelled Markov chains are trace-equivalent. We show that this problem is decidable in polynomial time if the second MDP is a Markov chain. The algorithm is based on new results on a particular notion of bisimulation between distributions over the states. However, we show that the general trace-refinement problem is undecidable, even if the first MDP is a Markov chain. Decidability of those problems was stated as open in 2008. We further study the decidability and complexity of the trace-refinement problem provided that the strategies are restricted to be memoryless. ",Computer Science - Logic in Computer Science ; ,"Fijalkow, Nathanaël ; Kiefer, Stefan ; Shirmohammadi, Mahsa ; "
http://arxiv.org/abs/1511.00158,Prediction of Dynamical time Series Using Kernel Based Regression and   Smooth Splines,"  Prediction of dynamical time series with additive noise using support vector machines or kernel based regression has been proved to be consistent for certain classes of discrete dynamical systems. Consistency implies that these methods are effective at computing the expected value of a point at a future time given the present coordinates. However, the present coordinates themselves are noisy, and therefore, these methods are not necessarily effective at removing noise. In this article, we consider denoising and prediction as separate problems for flows, as opposed to discrete time dynamical systems, and show that the use of smooth splines is more effective at removing noise. Combination of smooth splines and kernel based regression yields predictors that are more accurate on benchmarks typically by a factor of 2 or more. We prove that kernel based regression in combination with smooth splines converges to the exact predictor for time series extracted from any compact invariant set of any sufficiently smooth flow. As a consequence of convergence, one can find examples where the combination of kernel based regression with smooth splines is superior by even a factor of $100$. The predictors that we compute operate on delay coordinate data and not the full state vector, which is typically not observable. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; ,"Navarrete, Raymundo ; Viswanath, Divakar ; "
http://arxiv.org/abs/1511.00452,Stable Matching Mechanisms are Not Obviously Strategy-Proof,"  Many two-sided matching markets, from labor markets to school choice programs, use a clearinghouse based on the applicant-proposing deferred acceptance algorithm, which is well known to be strategy-proof for the applicants. Nonetheless, a growing amount of empirical evidence reveals that applicants misrepresent their preferences when this mechanism is used. This paper shows that no mechanism that implements a stable matching is ""obviously strategy-proof"" for any side of the market, a stronger incentive property than strategy-proofness that was introduced by Li (2017). A stable mechanism that is obviously strategy-proof for applicants is introduced for the case in which agents on the other side have acyclical preferences. ",Computer Science - Computer Science and Game Theory ; ,"Ashlagi, Itai ; Gonczarowski, Yannai A. ; "
http://arxiv.org/abs/1511.00493,"Uniqueness, Spatial Mixing, and Approximation for Ferromagnetic 2-Spin   Systems","  We give fully polynomial-time approximation schemes (FPTAS) for the partition function of ferromagnetic 2-spin systems in certain parameter regimes. The threshold we obtain is almost tight up to an integrality gap. Our technique is based on the correlation decay framework. The main technical contribution is a new potential function, with which we establish a new kind of spatial mixing. ",Computer Science - Data Structures and Algorithms ; Computer Science - Discrete Mathematics ; ,"Guo, Heng ; Lu, Pinyan ; "
http://arxiv.org/abs/1511.00523,Minimizing Regret in Discounted-Sum Games,"  In this paper, we study the problem of minimizing regret in discounted-sum games played on weighted game graphs. We give algorithms for the general problem of computing the minimal regret of the controller (Eve) as well as several variants depending on which strategies the environment (Adam) is permitted to use. We also consider the problem of synthesizing regret-free strategies for Eve in each of these scenarios. ",Computer Science - Computer Science and Game Theory ; Computer Science - Formal Languages and Automata Theory ; Computer Science - Logic in Computer Science ; F.1.1 ; D.2.4 ; ,"Hunter, Paul ; Pérez, Guillermo A. ; Raskin, Jean-François ; "
http://arxiv.org/abs/1511.00546,An Impossibility Result for Reconstruction in a Degree-Corrected   Planted-Partition Model,"  We consider the Degree-Corrected Stochastic Block Model (DC-SBM): a random graph on $n$ nodes, having i.i.d. weights $(\phi_u)_{u=1}^n$ (possibly heavy-tailed), partitioned into $q \geq 2$ asymptotically equal-sized clusters. The model parameters are two constants $a,b > 0$ and the finite second moment of the weights $\Phi^{(2)}$. Vertices $u$ and $v$ are connected by an edge with probability $\frac{\phi_u \phi_v}{n}a$ when they are in the same class and with probability $\frac{\phi_u \phi_v}{n}b$ otherwise.   We prove that it is information-theoretically impossible to estimate the clusters in a way positively correlated with the true community structure when $(a-b)^2 \Phi^{(2)} \leq q(a+b)$.   As by-products of our proof we obtain $(1)$ a precise coupling result for local neighbourhoods in DC-SBM's, that we use in a follow up paper [Gulikers et al., 2017] to establish a law of large numbers for local-functionals and $(2)$ that long-range interactions are weak in (power-law) DC-SBM's. ",Mathematics - Probability ; Computer Science - Machine Learning ; Computer Science - Social and Information Networks ; Statistics - Machine Learning ; ,"Gulikers, Lennart ; Lelarge, Marc ; Massoulié, Laurent ; "
http://arxiv.org/abs/1511.00725,Toward an Efficient Multi-class Classification in an Open Universe,"  Classification is a fundamental task in machine learning and data mining. Existing classification methods are designed to classify unknown instances within a set of previously known training classes. Such a classification takes the form of a prediction within a closed-set of classes. However, a more realistic scenario that fits real-world applications is to consider the possibility of encountering instances that do not belong to any of the training classes, $i.e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances. In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unknown ones. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes. ",Computer Science - Machine Learning ; Computer Science - Artificial Intelligence ; Computer Science - Databases ; Computer Science - Information Retrieval ; ,"Dhifli, Wajdi ; Diallo, Abdoulaye Baniré ; "
http://arxiv.org/abs/1511.00736,ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction   based on Graph Embedding in Structural and Topological Space,"  Studying the function of proteins is important for understanding the molecular mechanisms of life. The number of publicly available protein structures has increasingly become extremely large. Still, the determination of the function of a protein structure remains a difficult, costly, and time consuming task. The difficulties are often due to the essential role of spatial and topological structures in the determination of protein functions in living cells. In this paper, we propose ProtNN, a novel approach for protein function prediction. Given an unannotated protein structure and a set of annotated proteins, ProtNN finds the nearest neighbor annotated structures based on protein-graph pairwise similarities. Given a query protein, ProtNN finds the nearest neighbor reference proteins based on a graph representation model and a pairwise similarity between vector embedding of both query and reference protein-graphs in structural and topological spaces. ProtNN assigns to the query protein the function with the highest number of votes across the set of k nearest neighbor reference proteins, where k is a user-defined parameter. Experimental evaluation demonstrates that ProtNN is able to accurately classify several datasets in an extremely fast runtime compared to state-of-the-art approaches. We further show that ProtNN is able to scale up to a whole PDB dataset in a single-process mode with no parallelization, with a gain of thousands order of magnitude of runtime compared to state-of-the-art approaches. ",Computer Science - Machine Learning ; Computer Science - Social and Information Networks ; ,"Dhifli, Wajdi ; Diallo, Abdoulaye Baniré ; "
http://arxiv.org/abs/1511.00867,Dynamic Gossip,"  A gossip protocol is a procedure for spreading secrets among a group of agents, using a connection graph. The goal is for all agents to get to know all secrets, in which case we call the execution of the protocol successful. We consider distributed and dynamic gossip protocols. In distributed gossip the agents themselves instead of a global scheduler determine whom to call. In dynamic gossip not only secrets are exchanged but also telephone numbers (agent identities). This results in increased graph connectivity. We define six such distributed dynamic gossip protocols, and we characterize them in terms of the topology of the graphs on which they are successful, wherein we distinguish strong success (the protocol always terminates, possibly assuming fair scheduling) from weak success (the protocol sometimes terminates). For five of these protocols strong (fair) and weak success are characterized by weakly connected graphs. This result is surprising because the protocols are fairly different. In the sixth protocol an agent may only call another agent if it does not know the other agent's secret. Strong success for this protocol is characterized by graphs for which the set of non-terminal nodes is strongly connected. Weak success for this protocol is characterized by weakly connected graphs satisfying further topological constraints that we define in the paper. One direction of this characterization is surprisingly harder to prove than the other results in this contribution. ",Computer Science - Discrete Mathematics ; ,"van Ditmarsch, Hans ; van Eijck, Jan ; Pardo, Pere ; Ramezanian, Rahim ; Schwarzentruber, François ; "
http://arxiv.org/abs/1511.00876,Beating the Harmonic lower bound for online bin packing,"  In the online bin packing problem, items of sizes in (0,1] arrive online to be packed into bins of size 1. The goal is to minimize the number of used bins. In this paper, we present an online bin packing algorithm with asymptotic competitive ratio of 1.5813. This is the first improvement in fifteen years and reduces the gap to the lower bound by 15%. Within the well-known SuperHarmonic framework, no competitive ratio below 1.58333 can be achieved.   We make two crucial changes to that framework. First, some of our algorithm's decisions depend on exact sizes of items, instead of only their types. In particular, for each item with size in (1/3,1/2], we use its exact size to determine if it can be packed together with an item of size greater than 1/2. Second, we add constraints to the linear programs considered by Seiden, in order to better lower bound the optimal solution. These extra constraints are based on marks that we give to items based on how they are packed by our algorithm. We show that for each input, a single weighting function can be constructed to upper bound the competitive ratio on it.   We use this idea to simplify the analysis of SuperHarmonic, and show that the algorithm Harmonic++ is in fact 1.58880-competitive (Seiden proved 1.58889), and that 1.5884 can be achieved within the SuperHarmonic framework. Finally, we give a lower bound of 1.5762 for our new framework. ",Computer Science - Data Structures and Algorithms ; ,"Heydrich, Sandy ; van Stee, Rob ; "
http://arxiv.org/abs/1511.00925,Do Prices Coordinate Markets?,"  Walrasian equilibrium prices can be said to coordinate markets: They support a welfare optimal allocation in which each buyer is buying bundle of goods that is individually most preferred. However, this clean story has two caveats. First, the prices alone are not sufficient to coordinate the market, and buyers may need to select among their most preferred bundles in a coordinated way to find a feasible allocation. Second, we don't in practice expect to encounter exact equilibrium prices tailored to the market, but instead only approximate prices, somehow encoding ""distributional"" information about the market. How well do prices work to coordinate markets when tie-breaking is not coordinated, and they encode only distributional information?   We answer this question. First, we provide a genericity condition such that for buyers with Matroid Based Valuations, overdemand with respect to equilibrium prices is at most 1, independent of the supply of goods, even when tie-breaking is done in an uncoordinated fashion. Second, we provide learning-theoretic results that show that such prices are robust to changing the buyers in the market, so long as all buyers are sampled from the same (unknown) distribution. ",Computer Science - Computer Science and Game Theory ; Computer Science - Machine Learning ; ,"Hsu, Justin ; Morgenstern, Jamie ; Rogers, Ryan ; Roth, Aaron ; Vohra, Rakesh ; "
http://arxiv.org/abs/1511.01238,The wisdom of networks: A general adaptation and learning mechanism of   complex systems: The network core triggers fast responses to known stimuli;   innovations require the slow network periphery and are encoded by   core-remodeling,"  I hypothesize that re-occurring prior experience of complex systems mobilizes a fast response, whose attractor is encoded by their strongly connected network core. In contrast, responses to novel stimuli are often slow and require the weakly connected network periphery. Upon repeated stimulus, peripheral network nodes remodel the network core that encodes the attractor of the new response. This ""core-periphery learning"" theory reviews and generalizes the heretofore fragmented knowledge on attractor formation by neural networks, periphery-driven innovation and a number of recent reports on the adaptation of protein, neuronal and social networks. The coreperiphery learning theory may increase our understanding of signaling, memory formation, information encoding and decision-making processes. Moreover, the power of network periphery-related 'wisdom of crowds' inventing creative, novel responses indicates that deliberative democracy is a slow yet efficient learning strategy developed as the success of a billion-year evolution. ",Quantitative Biology - Molecular Networks ; Condensed Matter - Disordered Systems and Neural Networks ; Computer Science - Social and Information Networks ; Nonlinear Sciences - Adaptation and Self-Organizing Systems ; Physics - Biological Physics ; ,"Csermely, Peter ; "
http://arxiv.org/abs/1511.01249,Privacy by Design: On the Formal Design and Conformance Check of   Personal Data Protection Policies and Architectures,"  The new General Data Protection Regulation (GDPR) will take effect in May 2018, and hence, designing compliant data protection policies and system architectures became crucial for organizations to avoid penalties. Unfortunately, the regulations given in a textual format can be easily misinterpreted by the policy and system designers, which also making the conformance check error-prone for auditors. In this paper, we apply formal approach to facilitate systematic design of policies and architectures in an unambiguous way, and provide a framework for mathematically sound conformance checks against the current data protection regulations. We propose a (semi-)formal approach for specifying and reasoning about data protection policies and architectures as well as defining conformance relations between architectures and policies. The usability of our proposed approach is demonstrated on a smart metering service case study. ",Computer Science - Cryptography and Security ; ,"Ta, Vinh Thong ; "
http://arxiv.org/abs/1511.01258,"Learn on Source, Refine on Target:A Model Transfer Learning Framework   with Random Forests","  We propose novel model transfer-learning methods that refine a decision forest model M learned within a ""source"" domain using a training set sampled from a ""target"" domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems. ",Computer Science - Machine Learning ; ,"Segev, Noam ; Harel, Maayan ; Mannor, Shie ; Crammer, Koby ; El-Yaniv, Ran ; "
http://arxiv.org/abs/1511.01807,The height of piecewise-testable languages and the complexity of the   logic of subwords,"  The height of a piecewise-testable language $L$ is the maximum length of the words needed to define $L$ by excluding and requiring given subwords. The height of $L$ is an important descriptive complexity measure that has not yet been investigated in a systematic way. This paper develops a series of new techniques for bounding the height of finite languages and of languages obtained by taking closures by subwords, superwords and related operations.   As an application of these results, we show that $\text{FO}^2(A^*,\sqsubseteq)$, the two-variable fragment of the first-order logic of sequences with the subword ordering, can only express piecewise-testable properties and has elementary complexity. ",Computer Science - Logic in Computer Science ; Computer Science - Formal Languages and Automata Theory ; F.4.1 ; F.4.3 ; F.3.1 ; ,"Karandikar, Prateek ; Schnoebelen, Philippe ; "
http://arxiv.org/abs/1511.02166,Evaluation of the Intel Xeon Phi 7120 and NVIDIA K80 as accelerators for   two-dimensional panel codes,"  To optimize the geometry of airfoils for a specific application is an important engineering problem. In this context genetic algorithms have enjoyed some success as they are able to explore the search space without getting stuck in local optima. However, these algorithms require the computation of aerodynamic properties for a significant number of airfoil geometries. Consequently, for low-speed aerodynamics, panel methods are most often used as the inner solver.   In this paper we evaluate the performance of such an optimization algorithm on modern accelerators (more specifically, the Intel Xeon Phi 7120 and the NVIDIA K80). For that purpose, we have implemented an optimized version of the algorithm on the CPU and Xeon Phi (based on OpenMP, vectorization, and the Intel MKL library) and on the GPU (based on CUDA and the MAGMA library). We present timing results for all codes and discuss the similarities and differences between the three implementations. Overall, we observe a speedup of approximately $2.5$ for adding an Intel Xeon Phi 7120 to a dual socket workstation and a speedup between $3.4$ and $3.8$ for adding a NVIDIA K80 to a dual socket workstation. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Mathematical Software ; Physics - Computational Physics ; ","Einkemmer, Lukas ; "
http://arxiv.org/abs/1511.02475,On Sylvester Colorings of Cubic Graphs,"  If $G$ and $H$ are two cubic graphs, then an $H$-coloring of $G$ is a proper edge-coloring $f$ with edges of $H$, such that for each vertex $x$ of $G$, there is a vertex $y$ of $H$ with $f(\partial_G(x))=\partial_H(y)$. If $G$ admits an $H$-coloring, then we will write $H\prec G$. The Petersen coloring conjecture of Jaeger states that for any bridgeless cubic graph $G$, one has: $P\prec G$. The second author has recently introduced the Sylvester coloring conjecture, which states that for any cubic graph $G$ one has: $S\prec G$. Here $S$ is the Sylvester graph on $10$ vertices. In this paper, we prove the analogue of Sylvester coloring conjecture for cubic pseudo-graphs. Moreover, we show that if $G$ is any connected simple cubic graph $G$ with $G\prec P$, then $G = P$. This implies that the Petersen graph does not admit an $S_{16}$-coloring, where $S_{16}$ is the smallest connected simple cubic graph without a perfect matching. $S_{16}$ has $16$ vertices. %We conjecture that there are infinitely many connected cubic simple graphs which do not admit an %$S_{16}$-coloring. Finally, we obtain $2$ results towards the Sylvester coloring conjecture. The first result states that any cubic graph $G$ has a coloring with edges of Sylvester graph $S$ such that at least $\frac45$ of vertices of $G$ meet the conditions of Sylvester coloring conjecture. The second result states that any claw-free cubic graph graph admits an $S$-coloring. This results is an application of our result on cubic pseudo-graphs. ",Mathematics - Combinatorics ; Computer Science - Discrete Mathematics ; ,"Hakobyan, Anush ; Mkrtchyan, Vahan ; "
http://arxiv.org/abs/1511.02476,Statistical physics of inference: Thresholds and algorithms,"  Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic. ",Condensed Matter - Statistical Mechanics ; Computer Science - Data Structures and Algorithms ; Statistics - Machine Learning ; ,"Zdeborová, Lenka ; Krzakala, Florent ; "
http://arxiv.org/abs/1511.02599,Waste Makes Haste: Bounded Time Protocols for Envy-Free Cake Cutting   with Free Disposal,"  We consider the classic problem of envy-free division of a heterogeneous good (""cake"") among several agents. It is known that, when the allotted pieces must be connected, the problem cannot be solved by a finite algorithm for 3 or more agents. The impossibility result, however, assumes that the entire cake must be allocated. In this paper we replace the entire-allocation requirement with a weaker \emph{partial-proportionality} requirement: the piece given to each agent must be worth for it at least a certain positive fraction of the entire cake value. We prove that this version of the problem is solvable in bounded time even when the pieces must be connected. We present simple, bounded-time envy-free cake-cutting algorithms for: (1) giving each of $n$ agents a connected piece with a positive value; (2) giving each of 3 agents a connected piece worth at least 1/3; (3) giving each of 4 agents a connected piece worth at least 1/7; (4) giving each of 4 agents a disconnected piece worth at least 1/4; (5) giving each of $n$ agents a disconnected piece worth at least $(1-\epsilon)/n$ for any positive $\epsilon$. ",Computer Science - Data Structures and Algorithms ; Computer Science - Computer Science and Game Theory ; F.2.2 ; ,"Segal-Halevi, Erel ; Hassidim, Avinatan ; Aumann, Yonatan ; "
http://arxiv.org/abs/1511.02683,A Light CNN for Deep Face Representation with Noisy Labels,"  The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on https://github.com/AlfredXiangWu/LightCNN. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Wu, Xiang ; He, Ran ; Sun, Zhenan ; Tan, Tieniu ; "
http://arxiv.org/abs/1511.02899,On the Combinatorial Version of the Slepian-Wolf Problem,"  We study the following combinatorial version of the Slepian-Wolf coding scheme. Two isolated Senders are given binary strings $X$ and $Y$ respectively; the length of each string is equal to $n$, and the Hamming distance between the strings is at most $\alpha n$. The Senders compress their strings and communicate the results to the Receiver. Then the Receiver must reconstruct both strings $X$ and $Y$. The aim is to minimise the lengths of the transmitted messages.   For an asymmetric variant of this problem (where one of the Senders transmits the input string to the Receiver without compression) with deterministic encoding a nontrivial lower bound was found by A.Orlitsky and K.Viswanathany. In our paper we prove a new lower bound for the schemes with syndrome coding, where at least one of the Senders uses linear encoding of the input string.   For the combinatorial Slepian-Wolf problem with randomized encoding the theoretical optimum of communication complexity was recently found by the first author, though effective protocols with optimal lengths of messages remained unknown. We close this gap and present a polynomial time randomized protocol that achieves the optimal communication complexity. ",Computer Science - Information Theory ; Computer Science - Discrete Mathematics ; ,"Chumbalov, Daniyar ; Romashchenko, Andrei ; "
http://arxiv.org/abs/1511.03005,ELDA: Towards Efficient and Lightweight Detection of Cache Pollution   Attacks in NDN,"  As a promising architectural design for future Internet, named data networking (NDN) relies on in-network caching to efficiently deliver name-based content. However, the in-network caching is vulnerable to cache pollution attacks (CPA), which can reduce cache hits by violating cache locality and significantly degrade the overall performance of NDN. To defend against CPA attacks, the most effective way is to first detect the attacks and then throttle them. Since the CPA attack itself has already imposed a huge burden on victims, to avoid exhausting the remaining resources on the victims for detection purpose, we expect a lightweight detection solution. We thus propose ELDA, an Efficient and Lightweight Detection scheme against cache pollution Attacks, in which we design a Lightweight Flajolet-Martin (LFM) sketch to monitor the interest traffic. Our analysis and simulations demonstrate that, by consuming a few computation and memory resources, ELDA can effectively and efficiently detect CPA attacks. ",Computer Science - Cryptography and Security ; Computer Science - Networking and Internet Architecture ; Computer Science - Performance ; B.2.4 ; C.4 ; ,"Xu, Zhiwei ; Chen, Bo ; Wang, Ninghan ; Zhang, Yujun ; Li, Zhongcheng ; "
http://arxiv.org/abs/1511.03234,A general framework for Noetherian well ordered polynomial reductions,"  Polynomial reduction is one of the main tools in computational algebra with innumerable applications in many areas, both pure and applied. Since many years both the theory and an efficient design of the related algorithm have been solidly established.   This paper presents a general definition of polynomial reduction structure, studies its features and highlights the aspects needed in order to grant and to efficiently test the main properties (noetherianity, confluence, ideal membership).   The most significant aspect of this analysis is a negative reappraisal of the role of the notion of term order which is usually considered a central and crucial tool in the theory. In fact, as it was already established in the computer science context in relation with termination of algorithms, most of the properties can be obtained simply considering a well-founded ordering, while the classical requirement that it be preserved by multiplication is irrelevant.   The last part of the paper shows how the polynomial basis concepts present in literature are interpreted in our language and their properties are consequences of the general results established in the first part of the paper. ","Mathematics - Commutative Algebra ; Computer Science - Symbolic Computation ; 14C05, 14Q20, 13P10 ; ","Ceria, Michela ; Mora, Teo ; Roggero, Margherita ; "
http://arxiv.org/abs/1511.03407,Reorganizing topologies of Steiner trees to accelerate their elimination,"  We describe a technique to reorganize topologies of Steiner trees by exchanging neighbors of adjacent Steiner points. We explain how to use the systematic way of building trees, and therefore topologies, to find the correct topology after nodes have been exchanged. Topology reorganizations can be inserted into the enumeration scheme commonly used by exact algorithms for the Euclidean Steiner tree problem in $d$-space, providing a method of improvement different than the usual approaches. As an example, we show how topology reorganizations can be used to dynamically change the exploration of the usual branch-and-bound tree when two Steiner points collide during the optimization process. We also turn our attention to the erroneous use of a pre-optimization lower bound in the original algorithm and give an example to confirm its usage is incorrect. In order to provide numerical results on correct solutions, we use planar equilateral points to quickly compute this lower bound, even in dimensions higher than two. Finally, we describe planar twin trees, identical trees yielded by different topologies, whose generalization to higher dimensions could open a new way of building Steiner trees. ","Computer Science - Data Structures and Algorithms ; 90C35, 05C05, 90C57 ; ","Grodet, Aymeric ; Tsuchiya, Takuya ; "
http://arxiv.org/abs/1511.03501,"Eliminating Higher-Multiplicity Intersections, III. Codimension 2","  We study conditions under which a finite simplicial complex $K$ can be mapped to $\mathbb R^d$ without higher-multiplicity intersections. An almost $r$-embedding is a map $f: K\to \mathbb R^d$ such that the images of any $r$ pairwise disjoint simplices of $K$ do not have a common point. We show that if $r$ is not a prime power and $d\geq 2r+1$, then there is a counterexample to the topological Tverberg conjecture, i.e., there is an almost $r$-embedding of the $(d+1)(r-1)$-simplex in $\mathbb R^d$. This improves on previous constructions of counterexamples (for $d\geq 3r$) based on a series of papers by M. \""Ozaydin, M. Gromov, P. Blagojevi\'c, F. Frick, G. Ziegler, and the second and fourth present authors.   The counterexamples are obtained by proving the following algebraic criterion in codimension 2: If $r\ge3$ and if $K$ is a finite $2(r-1)$-complex then there exists an almost $r$-embedding $K\to \mathbb R^{2r}$ if and only if there exists a general position PL map $f:K\to \mathbb R^{2r}$ such that the algebraic intersection number of the $f$-images of any $r$ pairwise disjoint simplices of $K$ is zero. This result can be restated in terms of cohomological obstructions or equivariant maps, and extends an analogous codimension 3 criterion by the second and fourth authors. As another application we classify ornaments $f:S^3 \sqcup S^3\sqcup S^3\to \mathbb R^5$ up to ornament concordance.   It follows from work of M. Freedman, V. Krushkal and P. Teichner that the analogous criterion for $r=2$ is false. We prove a lemma on singular higher-dimensional Borromean rings, yielding an elementary proof of the counterexample. ","Mathematics - Geometric Topology ; Computer Science - Computational Geometry ; Mathematics - Combinatorics ; 57Q35, 55S91, 52A35 ; ","Avvakumov, S. ; Mabillard, I. ; Skopenkov, A. ; Wagner, U. ; "
http://arxiv.org/abs/1511.03518,Diffusion-like recommendation with enhanced similarity of objects,"  In last decades, diversity and accuracy have been regarded as two important measures in evaluating a recommendation model. However, a clear concern is that a model focusing excessively on one measure will put the other one at risk, thus it is not easy to greatly improve diversity and accuracy simultaneously. In this paper, we propose to enhance the Resource-Allocation (RA) similarity in resource transfer equations of diffusion-like models, by giving a tunable exponent to the RA similarity, and traversing the value of the exponent to achieve the optimal recommendation results. In this way, we can increase the recommendation scores (allocated resource) of many unpopular objects. Experiments on three benchmark data sets, MovieLens, Netflix, and RateYourMusic show that the modified models can yield remarkable performance improvement compared with the original ones. ",Computer Science - Information Retrieval ; Computer Science - Social and Information Networks ; ,"An, Ya-Hui ; Dong, Qiang ; Sun, Chong-Jing ; Nie, Da-Cheng ; Fu, Yan ; "
http://arxiv.org/abs/1511.03693,Game characterizations and lower cones in the Weihrauch degrees,"  We introduce a parametrized version of the Wadge game for functions and show that each lower cone in the Weihrauch degrees is characterized by such a game. These parametrized Wadge games subsume the original Wadge game, the eraser and backtrack games as well as Semmes's tree games. In particular, we propose that the lower cones in the Weihrauch degrees are the answer to Andretta's question on which classes of functions admit game characterizations. We then discuss some applications of such parametrized Wadge games. Using machinery from Weihrauch reducibility theory, we introduce games characterizing every (transfinite) level of the Baire hierarchy via an iteration of a pruning derivative on countably branching trees. ","Mathematics - Logic ; Computer Science - Logic in Computer Science ; 03E15, 54H05, 03D60, 03F15 ; ","Nobrega, Hugo ; Pauly, Arno ; "
http://arxiv.org/abs/1511.03894,The Game of Phishing,"  The current implementation of TLS involves your browser displaying a padlock, and a green bar, after successfully verifying the digital signature on the TLS certificate. Proposed is a solution where your browser's response to successful verification of a TLS certificate is to display a login window. That login window displays the identity credentials from the TLS certificate, to allow the user to authenticate Bob. It also displays a 'user-browser' shared secret i.e. a specific picture from your hard disk. This is not SiteKey, the image is shared between the computer user and their browser. It is never transmitted over the internet. Since sandboxed websites cannot access your hard disk this image cannot be counterfeited by phishing websites. Basically if you view the installed software component of your browser as an actor in the cryptography protocol, then the solution to phishing attacks is classic cryptography, as documented in any cryptography textbook. ",Computer Science - Cryptography and Security ; ,"Kilcullen, Joseph ; "
http://arxiv.org/abs/1511.04731,Hardness of RNA Folding Problem with Four Symbols,"  An RNA sequence is a string composed of four types of nucleotides, $A, C, G$, and $U$. The goal of the RNA folding problem is to find a maximum cardinality set of crossing-free pairs of the form $\{A,U\}$ or $\{C,G\}$ in a given RNA sequence. The problem is central in bioinformatics and has received much attention over the years. Abboud, Backurs, and Williams (FOCS 2015) demonstrated a conditional lower bound for a generalized version of the RNA folding problem based on a conjectured hardness of the $k$-clique problem. Their lower bound requires the RNA sequence to have at least 36 types of symbols, making the result not applicable to the RNA folding problem in real life (i.e., alphabet size 4). In this paper, we present an improved lower bound that works for the alphabet size 4 case.   We also investigate the Dyck edit distance problem, which is a string problem closely related to RNA folding. We demonstrate a reduction from RNA folding to Dyck edit distance with alphabet size 10. This leads to a much simpler proof of the conditional lower bound for Dyck edit distance problem given by Abboud, Backurs, and Williams (FOCS 2015), and lowers the alphabet size requirement. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; ,"Chang, Yi-Jun ; "
http://arxiv.org/abs/1511.04798,"Heterogeneous Knowledge Transfer in Video Emotion Recognition,   Attribution and Summarization","  Emotion is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames expressing emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in understanding video emotion: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpora for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Artificial Intelligence ; Computer Science - Multimedia ; ,"Xu, Baohan ; Fu, Yanwei ; Jiang, Yu-Gang ; Li, Boyang ; Sigal, Leonid ; "
http://arxiv.org/abs/1511.04855,"Deep learning is a good steganalysis tool when embedding key is reused   for different images, even if there is a cover source-mismatch","  Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best "" shape "" of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained. ",Computer Science - Multimedia ; Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Pibre, Lionel ; Jérôme, Pasquet ; Ienco, Dino ; Chaumont, Marc ; "
http://arxiv.org/abs/1511.04944,NASCUP: Nucleic Acid Sequence Classification by Universal Probability,"  Motivated by the need for fast and accurate classification of unlabeled nucleotide sequences on a large scale, we developed NASCUP, a new classification method that captures statistical structures of nucleotide sequences by compact context-tree models and universal probability from information theory. NASCUP achieved BLAST-like classification accuracy consistently for several large-scale databases in orders-of-magnitude reduced runtime, and was applied to other bioinformatics tasks such as outlier detection and synthetic sequence generation. ",Quantitative Biology - Genomics ; Computer Science - Information Theory ; ,"Kwon, Sunyoung ; Kim, Gyuwan ; Lee, Byunghan ; Chun, Jongsik ; Yoon, Sungroh ; Kim, Young-Han ; "
http://arxiv.org/abs/1511.05174,Cross-scale predictive dictionaries,"  Sparse representations using data dictionaries provide an efficient model particularly for signals that do not enjoy alternate analytic sparsifying transformations. However, solving inverse problems with sparsifying dictionaries can be computationally expensive, especially when the dictionary under consideration has a large number of atoms. In this paper, we incorporate additional structure on to dictionary-based sparse representations for visual signals to enable speedups when solving sparse approximation problems. The specific structure that we endow onto sparse models is that of a multi-scale modeling where the sparse representation at each scale is constrained by the sparse representation at coarser scales. We show that this cross-scale predictive model delivers significant speedups, often in the range of 10-60$\times$, with little loss in accuracy for linear inverse problems associated with images, videos, and light fields. ",Computer Science - Computer Vision and Pattern Recognition ; Statistics - Machine Learning ; ,"Saragadam, Vishwanath ; Li, Xin ; Sankaranarayanan, Aswin ; "
http://arxiv.org/abs/1511.05432,Understanding Adversarial Training: Increasing Local Stability of Neural   Nets through Robust Optimization,"  We propose a general framework for increasing local stability of Artificial Neural Nets (ANNs) using Robust Optimization (RO). We achieve this through an alternating minimization-maximization procedure, in which the loss of the network is minimized over perturbed examples that are generated at each parameter update. We show that adversarial training of ANNs is in fact robustification of the network optimization, and that our proposed framework generalizes previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the network also on the original test data. ",Statistics - Machine Learning ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Shaham, Uri ; Yamada, Yutaro ; Negahban, Sahand ; "
http://arxiv.org/abs/1511.05546,Complexity and Approximability of Parameterized MAX-CSPs,"  We study the optimization version of constraint satisfaction problems (Max-CSPs) in the framework of parameterized complexity; the goal is to compute the maximum fraction of constraints that can be satisfied simultaneously. In standard CSPs, we want to decide whether this fraction equals one. The parameters we investigate are structural measures, such as the treewidth or the clique-width of the variable-constraint incidence graph of the CSP instance.   We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY, and with various parameters k, and we attempt to fully classify them into the following three cases: 1. The exact optimum can be computed in FPT time. 2. It is W[1]-hard to compute the exact optimum, but there is a randomized FPT approximation scheme (FPTAS), which computes a $(1-\epsilon)$-approximation in time $f(k,\epsilon)\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].   For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness results. ",Computer Science - Computational Complexity ; Computer Science - Data Structures and Algorithms ; ,"Dell, Holger ; Kim, Eun Jung ; Lampis, Michael ; Mitsou, Valia ; Mömke, Tobias ; "
http://arxiv.org/abs/1511.05635,Competitive Multi-scale Convolution,"  In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; Computer Science - Neural and Evolutionary Computing ; ,"Liao, Zhibin ; Carneiro, Gustavo ; "
http://arxiv.org/abs/1511.05646,The Invisible Hand of Dynamic Market Pricing,"  Walrasian prices, if they exist, have the property that one can assign every buyer some bundle in her demand set, such that the resulting assignment will maximize social welfare. Unfortunately, this assumes carefully breaking ties amongst different bundles in the buyer demand set. Presumably, the shopkeeper cleverly convinces the buyer to break ties in a manner consistent with maximizing social welfare. Lacking such a shopkeeper, if buyers arrive sequentially and simply choose some arbitrary bundle in their demand set, the social welfare may be arbitrarily bad. In the context of matching markets, we show how to compute dynamic prices, based upon the current inventory, that guarantee that social welfare is maximized. Such prices are set without knowing the identity of the next buyer to arrive. We also show that this is impossible in general (e.g., for coverage valuations), but consider other scenarios where this can be done. We further extend our results to Bayesian and bounded rationality models. ",Computer Science - Computer Science and Game Theory ; Computer Science - Data Structures and Algorithms ; ,"Cohen-Addad, Vincent ; Eden, Alon ; Feldman, Michal ; Fiat, Amos ; "
http://arxiv.org/abs/1511.05710,Complex-Valued Gaussian Processes for Regression,"  In this paper we propose a novel Bayesian solution for nonlinear regression in complex fields. Previous solutions for kernels methods usually assume a complexification approach, where the real-valued kernel is replaced by a complex-valued one. This approach is limited. Based on results in complex-valued linear theory and Gaussian random processes we show that a pseudo-kernel must be included. This is the starting point to develop the new complex-valued formulation for Gaussian process for regression (CGPR). We face the design of the covariance and pseudo-covariance based on a convolution approach and for several scenarios. Just in the particular case where the outputs are proper, the pseudo-kernel cancels. Also, the hyperparameters of the covariance {can be learnt} maximizing the marginal likelihood using Wirtinger's calculus and patterned complex-valued matrix derivatives. In the experiments included, we show how CGPR successfully solve systems where real and imaginary parts are correlated. Besides, we successfully solve the nonlinear channel equalization problem by developing a recursive solution with basis removal. We report remarkable improvements compared to previous solutions: a 2-4 dB reduction of the MSE with {just a quarter} of the training samples used by previous approaches. ",Computer Science - Machine Learning ; ,"Boloix-Tortosa, Rafael ; Arias-de-Reyna, Eva ; Payan-Somet, F. Javier ; Murillo-Fuentes, Juan J. ; "
http://arxiv.org/abs/1511.06030,BIRDNEST: Bayesian Inference for Ratings-Fraud Detection,"  Review fraud is a pervasive problem in online commerce, in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services. Fake reviews are often detected based on several signs, including 1) they occur in short bursts of time; 2) fraudulent user accounts have skewed rating distributions. However, these may both be true in any given dataset. Hence, in this paper, we propose an approach for detecting fraudulent reviews which combines these 2 approaches in a principled manner, allowing successful detection even when one of these signs is not present. To combine these 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD) model, a flexible Bayesian model of user rating behavior. Based on our model we formulate a likelihood-based suspiciousness metric, Normalized Expected Surprise Total (NEST). We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric. Experiments on real data show that BIRDNEST successfully spots review fraud in large, real-world graphs: the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart. ",Computer Science - Artificial Intelligence ; Computer Science - Social and Information Networks ; ,"Hooi, Bryan ; Shah, Neil ; Beutel, Alex ; Gunnemann, Stephan ; Akoglu, Leman ; Kumar, Mohit ; Makhija, Disha ; Faloutsos, Christos ; "
http://arxiv.org/abs/1511.06033,EigenRec: Generalizing PureSVD for Effective and Efficient Top-N   Recommendations,"  We introduce EigenRec; a versatile and efficient Latent-Factor framework for Top-N Recommendations that includes the well-known PureSVD algorithm as a special case. EigenRec builds a low dimensional model of an inter-item proximity matrix that combines a similarity component, with a scaling operator, designed to control the influence of the prior item popularity on the final model. Seeing PureSVD within our framework provides intuition about its inner workings, exposes its inherent limitations, and also, paves the path towards painlessly improving its recommendation performance. A comprehensive set of experiments on the MovieLens and the Yahoo datasets based on widely applied performance metrics, indicate that EigenRec outperforms several state-of-the-art algorithms, in terms of Standard and Long-Tail recommendation accuracy, exhibiting low susceptibility to sparsity, even in its most extreme manifestations -- the Cold-Start problems. At the same time EigenRec has an attractive computational profile and it can apply readily in large-scale recommendation settings. ","Computer Science - Information Retrieval ; Computer Science - Databases ; Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Numerical Analysis ; Computer Science - Social and Information Networks ; H.3.3 ; H.2.8 ; G.1.3 ; ","Nikolakopoulos, Athanasios N. ; Kalantzis, Vassilis ; Gallopoulos, Efstratios ; Garofalakis, John D. ; "
http://arxiv.org/abs/1511.06324,Global Convergence of ADMM in Nonconvex Nonsmooth Optimization,"  In this paper, we analyze the convergence of the alternating direction method of multipliers (ADMM) for minimizing a nonconvex and possibly nonsmooth objective function, $\phi(x_0,\ldots,x_p,y)$, subject to coupled linear equality constraints. Our ADMM updates each of the primal variables $x_0,\ldots,x_p,y$, followed by updating the dual variable. We separate the variable $y$ from $x_i$'s as it has a special role in our analysis.   The developed convergence guarantee covers a variety of nonconvex functions such as piecewise linear functions, $\ell_q$ quasi-norm, Schatten-$q$ quasi-norm ($0<q<1$), minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) penalty. It also allows nonconvex constraints such as compact manifolds (e.g., spherical, Stiefel, and Grassman manifolds) and linear complementarity constraints. Also, the $x_0$-block can be almost any lower semi-continuous function.   By applying our analysis, we show, for the first time, that several ADMM algorithms applied to solve nonconvex models in statistical learning, optimization on manifold, and matrix decomposition are guaranteed to converge.   Our results provide sufficient conditions for ADMM to converge on (convex or nonconvex) monotropic programs with three or more blocks, as they are special cases of our model.   ADMM has been regarded as a variant to the augmented Lagrangian method (ALM). We present a simple example to illustrate how ADMM converges but ALM diverges with bounded penalty parameter $\beta$. Indicated by this example and other analysis in this paper, ADMM might be a better choice than ALM for some nonconvex \emph{nonsmooth} problems, because ADMM is not only easier to implement, it is also more likely to converge for the concerned scenarios. ",Mathematics - Optimization and Control ; Computer Science - Numerical Analysis ; Mathematics - Numerical Analysis ; ,"Wang, Yu ; Yin, Wotao ; Zeng, Jinshan ; "
http://arxiv.org/abs/1511.06382,Iterative Refinement of the Approximate Posterior for Directed Belief   Networks,"  Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates. ",Computer Science - Machine Learning ; Statistics - Machine Learning ; ,"Hjelm, R Devon ; Cho, Kyunghyun ; Chung, Junyoung ; Salakhutdinov, Russ ; Calhoun, Vince ; Jojic, Nebojsa ; "
http://arxiv.org/abs/1511.06436,"On the robust hardness of Gr\""obner basis computation","  The computation of Gr\""obner bases is an established hard problem. By contrast with many other problems, however, there has been little investigation of whether this hardness is robust. In this paper, we frame and present results on the problem of approximate computation of Gr\""obner bases. We show that it is NP-hard to construct a Gr\""obner basis of the ideal generated by a set of polynomials, even when the algorithm is allowed to discard a $(1 - \epsilon)$ fraction of the generators, and likewise when the algorithm is allowed to discard variables (and the generators containing them). Our results shows that computation of Gr\""obner bases is robustly hard even for simple polynomial systems (e.g. maximum degree 2, with at most 3 variables per generator). We conclude by greatly strengthening results for the Strong $c$-Partial Gr\""obner problem posed by De Loera et al. Our proofs also establish interesting connections between the robust hardness of Gr\""obner bases and that of SAT variants and graph-coloring. ",Computer Science - Symbolic Computation ; ,"Spencer, Gwen ; Rolnick, David ; "
http://arxiv.org/abs/1511.06444,Universal halting times in optimization and machine learning,"  The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed. We observe two qualitative classes: A Gumbel-like distribution that appears in Google searches, human decision times, the QR eigenvalue algorithm and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions. ","Computer Science - Machine Learning ; Mathematics - Numerical Analysis ; Mathematics - Probability ; 65K10, 82D30, 37E20 ; ","Sagun, Levent ; Trogdon, Thomas ; LeCun, Yann ; "
http://arxiv.org/abs/1511.06489,A Simple Hierarchical Pooling Data Structure for Loop Closure,"  We propose a data structure obtained by hierarchically averaging bag-of-word descriptors during a sequence of views that achieves average speedups in large-scale loop closure applications ranging from 4 to 20 times on benchmark datasets. Although simple, the method works as well as sophisticated agglomerative schemes at a fraction of the cost with minimal loss of performance. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Robotics ; ,"Fei, Xiaohan ; Tsotsos, Konstantine ; Soatto, Stefano ; "
http://arxiv.org/abs/1511.06653,Recurrent Semi-supervised Classification and Constrained Adversarial   Generation with Motion Capture Data,"  We explore recurrent encoder multi-decoder neural network architectures for semi-supervised sequence classification and reconstruction. We find that the use of multiple reconstruction modules helps models generalize in a classification task when only a small amount of labeled data is available, which is often the case in practice. Such models provide useful high-level representations of motions allowing clustering, searching and faster labeling of new sequences. We also propose a new, realistic partitioning of a well-known, high quality motion-capture dataset for better evaluations. We further explore a novel formulation for future-predicting decoders based on conditional recurrent generative adversarial networks, for which we propose both soft and hard constraints for transition generation derived from desired physical properties of synthesized future movements and desired animation goals. We find that using such constraints allow to stabilize the training of recurrent adversarial architectures for animation generation. ",Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Machine Learning ; ,"Harvey, Félix G. ; Roy, Julien ; Kanaa, David ; Pal, Christopher ; "
http://arxiv.org/abs/1511.06860,Convex Sparse Spectral Clustering: Single-view to Multi-view,"  Spectral Clustering (SC) is one of the most widely used methods for data clustering. It first finds a low-dimensonal embedding $U$ of data by computing the eigenvectors of the normalized Laplacian matrix, and then performs k-means on $U^\top$ to get the final clustering result. In this work, we observe that, in the ideal case, $UU^\top$ should be block diagonal and thus sparse. Therefore we propose the Sparse Spectral Clustering (SSC) method which extends SC with sparse regularization on $UU^\top$. To address the computational issue of the nonconvex SSC model, we propose a novel convex relaxation of SSC based on the convex hull of the fixed rank projection matrices. Then the convex SSC model can be efficiently solved by the Alternating Direction Method of \canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse Spectral Clustering (PSSC) which extends SSC to boost the clustering performance by using the multi-view information of data. Experimental comparisons with several baselines on real-world datasets testify to the efficacy of our proposed methods. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Lu, Canyi ; Yan, Shuicheng ; Lin, Zhouchen ; "
http://arxiv.org/abs/1511.06971,A General Framework for the Design and Analysis of Sparse FIR Linear   Equalizers,"  Complexity of linear finite-impulse-response (FIR) equalizers is proportional to the square of the number of nonzero taps in the filter. This makes equalization of channels with long impulse responses using either zero-forcing or minimum mean square error (MMSE) filters computationally expensive. Sparse equalization is a widely-used technique to solve this problem. In this paper, a general framework is provided that transforms the problem of sparse linear equalizers (LEs) design into the problem of sparsest-approximation of a vector in different dictionaries. In addition, some possible choices of sparsifying dictionaries in this framework are discussed. Furthermore, the worst-case coherence of some of these dictionaries, which determines their sparsifying strength, are analytically and/or numerically evaluated. Finally, the usefulness of the proposed framework for the design of sparse FIR LEs is validated through numerical experiments. ",Computer Science - Information Theory ; ,"Al-Abbasi, Abubakr O. ; Hamila, Ridha ; Bajwa, Waheed U. ; Al-Dhahir, Naofal ; "
http://arxiv.org/abs/1511.07174,Developing a High Performance Software Library with MPI and CUDA for   Matrix Computations,"  Nowadays, the paradigm of parallel computing is changing. CUDA is now a popular programming model for general purpose computations on GPUs and a great number of applications were ported to CUDA obtaining speedups of orders of magnitude comparing to optimized CPU implementations. Hybrid approaches that combine the message passing model with the shared memory model for parallel computing are a solution for very large applications. We considered a heterogeneous cluster that combines the CPU and GPU computations using MPI and CUDA for developing a high performance linear algebra library. Our library deals with large linear systems solvers because they are a common problem in the fields of science and engineering. Direct methods for computing the solution of such systems can be very expensive due to high memory requirements and computational cost. An efficient alternative are iterative methods which computes only an approximation of the solution. In this paper we present an implementation of a library that uses a hybrid model of computation using MPI and CUDA implementing both direct and iterative linear systems solvers. Our library implements LU and Cholesky factorization based solvers and some of the non-stationary iterative methods using the MPI/CUDA combination. We compared the performance of our MPI/CUDA implementation with classic programs written to be run on a single CPU. ","Computer Science - Distributed, Parallel, and Cluster Computing ; Computer Science - Mathematical Software ; ","Oancea, Bogdan ; Andrei, Tudorel ; "
http://arxiv.org/abs/1511.07212,Face Alignment Across Large Poses: A 3D Solution,"  Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45 degree), lacking the ability to align faces in large poses up to 90 degree. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Zhu, Xiangyu ; Lei, Zhen ; Liu, Xiaoming ; Shi, Hailin ; Li, Stan Z. ; "
http://arxiv.org/abs/1511.07860,Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-Two   and Depth-Three Threshold Circuits,"  In order to formally understand the power of neural computing, we first need to crack the frontier of threshold circuits with two and three layers, a regime that has been surprisingly intractable to analyze. We prove the first super-linear gate lower bounds and the first super-quadratic wire lower bounds for depth-two linear threshold circuits with arbitrary weights, and depth-three majority circuits computing an explicit function.   $\bullet$ We prove that for all $\epsilon\gg \sqrt{\log(n)/n}$, the linear-time computable Andreev's function cannot be computed on a $(1/2+\epsilon)$-fraction of $n$-bit inputs by depth-two linear threshold circuits of $o(\epsilon^3 n^{3/2}/\log^3 n)$ gates, nor can it be computed with $o(\epsilon^{3} n^{5/2}/\log^{7/2} n)$ wires. This establishes an average-case ``size hierarchy'' for threshold circuits, as Andreev's function is computable by uniform depth-two circuits of $o(n^3)$ linear threshold gates, and by uniform depth-three circuits of $O(n)$ majority gates.   $\bullet$ We present a new function in $P$ based on small-biased sets, which we prove cannot be computed by a majority vote of depth-two linear threshold circuits with $o(n^{3/2}/\log^3 n)$ gates, nor with $o(n^{5/2}/\log^{7/2}n)$ wires.   $\bullet$ We give tight average-case (gate and wire) complexity results for computing PARITY with depth-two threshold circuits; the answer turns out to be the same as for depth-two majority circuits.   The key is a new random restriction lemma for linear threshold functions. Our main analytical tool is the Littlewood-Offord Lemma from additive combinatorics. ",Computer Science - Computational Complexity ; Computer Science - Neural and Evolutionary Computing ; 68Q17 ; C.1.3 ; F.1.3 ; ,"Kane, Daniel M. ; Williams, Ryan ; "
http://arxiv.org/abs/1511.08152,Max-Cut under Graph Constraints,"  An instance of the graph-constrained max-cut (GCMC) problem consists of (i) an undirected graph G and (ii) edge-weights on a complete undirected graph on the same vertex set. The objective is to find a subset of vertices satisfying some graph-based constraint in G that maximizes the total weight of edges in the cut. The types of graph constraints we can handle include independent set, vertex cover, dominating set and connectivity. Our main results are for the case when G is a graph with bounded treewidth, where we obtain a 0.5-approximation algorithm. Our algorithm uses an LP relaxation based on the Sherali-Adams hierarchy. It can handle any graph constraint for which there is a (certain type of) dynamic program that exactly optimizes linear objectives. Using known decomposition results, these imply essentially the same approximation ratio for GCMC under constraints such as independent set, dominating set and connectivity on a planar graph G (more generally for bounded-genus or excluded-minor graphs). ",Computer Science - Data Structures and Algorithms ; ,"Lee, Jon ; Nagarajan, Viswanath ; Shen, Xiangkun ; "
http://arxiv.org/abs/1511.08189,Graph Isomorphism and Circuit Size,"  It is well-known [KST93] that the complexity of the Graph Automorphism problem is characterized by a special case of Graph Isomorphism, where the input graphs satisfy the ""promise"" of being rigid (that is, having no nontrivial automorphisms). In this brief note, we observe that the reduction of Graph Automorphism to the Rigid Graph Ismorphism problem can be accomplished even using Grollman and Selman's notion of a ""smart reduction"". ",Computer Science - Computational Complexity ; ,"Allender, Eric ; Grochow, Joshua A. ; van Melkebeek, Dieter ; Moore, Cristopher ; Morgan, Andrew ; "
http://arxiv.org/abs/1511.08280,Welfare of Sequential Allocation Mechanisms for Indivisible Goods,"  Sequential allocation is a simple and attractive mechanism for the allocation of indivisible goods. Agents take turns, according to a policy, to pick items. Sequential allocation is guaranteed to return an allocation which is efficient but may not have an optimal social welfare. We consider therefore the relation between welfare and efficiency. We study the (computational) questions of what welfare is possible or necessary depending on the choice of policy. We also consider a novel control problem in which the chair chooses a policy to improve social welfare. ",Computer Science - Artificial Intelligence ; Computer Science - Computer Science and Game Theory ; ,"Aziz, Haris ; Kalinowski, Thomas ; Walsh, Toby ; Xia, Lirong ; "
http://arxiv.org/abs/1511.08416,Who Can Win a Single-Elimination Tournament?,"  A single-elimination (SE) tournament is a popular way to select a winner in both sports competitions and in elections. A natural and well-studied question is the tournament fixing problem (TFP): given the set of all pairwise match outcomes, can a tournament organizer rig an SE tournament by adjusting the initial seeding so that their favorite player wins? We prove new sufficient conditions on the pairwise match outcome information and the favorite player, under which there is guaranteed to be a seeding where the player wins the tournament. Our results greatly generalize previous results. We also investigate the relationship between the set of players that can win an SE tournament under some seeding (so called SE winners) and other traditional tournament solutions. In addition, we generalize and strengthen prior work on probabilistic models for generating tournaments. For instance, we show that \emph{every} player in an $n$ player tournament generated by the Condorcet Random Model will be an SE winner even when the noise is as small as possible, $p=\Theta(\ln n/n)$; prior work only had such results for $p\geq \Omega(\sqrt{\ln n/n})$. We also establish new results for significantly more general generative models. ",Computer Science - Computer Science and Game Theory ; ,"Kim, Michael P. ; Suksompong, Warut ; Williams, Virginia Vassilevska ; "
http://arxiv.org/abs/1511.08575,A Modified Multiple OLS (m$^2$OLS) Algorithm for Signal Recovery in   Compressive Sensing,"  Orthogonal least square (OLS) is an important sparse signal recovery algorithm for compressive sensing, which enjoys superior probability of success over other well-known recovery algorithms under conditions of correlated measurement matrices. Multiple OLS (mOLS) is a recently proposed improved version of OLS which selects multiple candidates per iteration by generalizing the greedy selection principle used in OLS and enjoys faster convergence than OLS. In this paper, we present a refined version of the mOLS algorithm where at each step of the iteration, we first preselect a submatrix of the measurement matrix suitably and then apply the mOLS computations to the chosen submatrix. Since mOLS now works only on a submatrix and not on the overall matrix, computations reduce drastically. Convergence of the algorithm, however, requires ensuring passage of true candidates through the two stages of preselection and mOLS based selection successively. This paper presents convergence conditions for both noisy and noise free signal models. The proposed algorithm enjoys faster convergence properties similar to mOLS, at a much reduced computational complexity. ",Computer Science - Information Theory ; Statistics - Methodology ; ,"Mukhopadhyay, Samrat ; Satpathi, Siddhartha ; Chakraborty, Mrityunjoy ; "
http://arxiv.org/abs/1511.08585,Real-Time Residential-Side Joint Energy Storage Management and Load   Scheduling with Renewable Integration,"  We consider joint energy storage management and load scheduling at a residential site with integrated renewable generation. Assuming unknown arbitrary dynamics of renewable source, loads, and electricity price, we aim at optimizing the load scheduling and energy storage control simultaneously in order to minimize the overall system cost within a finite time period. Besides incorporating battery operational constraints and costs, we model each individual load task by its requested power intensity and service durations, as well as the maximum and average delay requirements. To tackle this finite time horizon stochastic problem, we propose a real-time scheduling and storage control solution by applying a sequence of modification and transformation to employ Lyapunov optimization that otherwise is not directly applicable. With our proposed algorithm, we show that the joint load scheduling and energy storage control can in fact be separated and sequentially determined. Furthermore, both scheduling and energy control decisions have closed-form solutions for simple implementation. Through analysis, we show that our proposed real-time algorithm has a bounded performance guarantee from the optimal T-slot look-ahead solution and is asymptotically equivalent to it as the battery capacity and time period goes to infinity. The effectiveness of joint load scheduling and energy storage control by our proposed algorithm is demonstrated through simulation as compared with alternative algorithms. ",Computer Science - Systems and Control ; ,"Li, Tianyi ; Dong, Min ; "
http://arxiv.org/abs/1511.08861,Loss Functions for Neural Networks for Image Processing,"  Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is L2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged. ",Computer Science - Computer Vision and Pattern Recognition ; ,"Zhao, Hang ; Gallo, Orazio ; Frosio, Iuri ; Kautz, Jan ; "
http://arxiv.org/abs/1511.08887,On the Degrees of Freedom of the Symmetric Multi-Relay MIMO Y Channel,"  In this paper, we study the degrees of freedom (DoF) of the symmetric multi-relay multiple-input multiple-output (MIMO) Y channel, where three user nodes, each with M antennas, communicate via K geographically separated relay nodes, each with N antennas. For this model, we establish a general DoF achievability framework based on linear precoding and post-processing methods. The framework poses a nonlinear problem with respect to user precoders and post-processors, as well as relay precoders. To solve this problem, we adopt an uplink-downlink asymmetric strategy, where the user precoders are designed for signal alignment and the user post-processors are used for interference neutralization. With the user precoder and post-processor designs fixed as such, the original problem then reduces to a problem of relay precoder design. To address the solvability of the system, we propose a general method for solving matrix equations. This method is also useful to the DoF analysis of many other multiway relay networks. Together with the techniques of antenna disablement and symbol extension, an achievable DoF of the symmetric multi-relay MIMO Y channel is derived for an arbitrary setup of (K, M, N). We show that, for K >= 2, the optimal DoF is achieved for M/N in [0, max{sqrt(3K)/3,1}) and [(3K+sqrt(9K^2-12K))/6,infinity). We also show that the uplink-downlink asymmetric design proposed in this paper considerably outperforms the conventional approach based on uplink-downlink symmetry. ",Computer Science - Information Theory ; ,"Ding, Tian ; Yuan, Xiaojun ; Liew, Soung Chang ; "
http://arxiv.org/abs/1511.09156,Constant-approximation algorithms for highly connected multi-dominating   sets in unit disk graphs,"  Given an undirected graph on a node set $V$ and positive integers $k$ and $m$, a $k$-connected $m$-dominating set ($(k,m)$-CDS) is defined as a subset $S$ of $V$ such that each node in $V \setminus S$ has at least $m$ neighbors in $S$, and a $k$-connected subgraph is induced by $S$. The weighted $(k,m)$-CDS problem is to find a minimum weight $(k,m)$-CDS in a given node-weighted graph. The problem is called the unweighted $(k,m)$-CDS problem if the objective is to minimize the cardinality of a $(k,m)$-CDS. These problems have been actively studied for unit disk graphs, motivated by the application of constructing a virtual backbone in a wireless ad hoc network. However, constant-approximation algorithms are known only for $k \leq 3$ in the unweighted $(k,m)$-CDS problem, and for $(k,m)=(1,1)$ in the weighted $(k,m)$-CDS problem. In this paper, we consider the case in which $m \geq k$, and we present a simple $O(5^k k!)$-approximation algorithm for the unweighted $(k,m)$-CDS problem, and a primal-dual $O(k^2 \log k)$-approximation algorithm for the weighted $(k,m)$-CDS problem. Both algorithms achieve constant approximation factors when $k$ is a fixed constant. ",Computer Science - Data Structures and Algorithms ; ,"Fukunaga, Takuro ; "
http://arxiv.org/abs/1511.09259,The Alternating Stock Size Problem and the Gasoline Puzzle,"  Given a set S of integers whose sum is zero, consider the problem of finding a permutation of these integers such that: (i) all prefix sums of the ordering are nonnegative, and (ii) the maximum value of a prefix sum is minimized. Kellerer et al. referred to this problem as the ""Stock Size Problem"" and showed that it can be approximated to within 3/2. They also showed that an approximation ratio of 2 can be achieved via several simple algorithms.   We consider a related problem, which we call the ""Alternating Stock Size Problem"", where the number of positive and negative integers in the input set S are equal. The problem is the same as above, but we are additionally required to alternate the positive and negative numbers in the output ordering. This problem also has several simple 2-approximations. We show that it can be approximated to within 1.79.   Then we show that this problem is closely related to an optimization version of the gasoline puzzle due to Lov\'asz, in which we want to minimize the size of the gas tank necessary to go around the track. We present a 2-approximation for this problem, using a natural linear programming relaxation whose feasible solutions are doubly stochastic matrices. Our novel rounding algorithm is based on a transformation that yields another doubly stochastic matrix with special properties, from which we can extract a suitable permutation. ",Computer Science - Data Structures and Algorithms ; ,"Newman, Alantha ; Röglin, Heiko ; Seif, Johanna ; "
http://arxiv.org/abs/1511.09324,Isomorphisms considered as equalities: Projecting functions and   enhancing partial application through and implementation of lambda+,"  We propose an implementation of lambda+, a recently introduced simply typed lambda-calculus with pairs where isomorphic types are made equal. The rewrite system of lambda+ is a rewrite system modulo an equivalence relation, which makes its implementation non-trivial. We also extend lambda+ with natural numbers and general recursion and use Beki\'c's theorem to split mutual recursions. This splitting, together with the features of lambda+, allows for a novel way of program transformation by reduction, by projecting a function before it is applied in order to simplify it. Also, currying together with the associativity and commutativity of pairs gives an enhanced form of partial application. ",Computer Science - Logic in Computer Science ; F.4.1 ; ,"Díaz-Caro, Alejandro ; López, Pablo E. Martínez ; "
http://arxiv.org/abs/1512.00135,Entropies of weighted sums in cyclic groups and an application to polar   codes,"  In this note, the following basic question is explored: in a cyclic group, how are the Shannon entropies of the sum and difference of i.i.d. random variables related to each other? For the integer group, we show that they can differ by any real number additively, but not too much multiplicatively; on the other hand, for $\mathbb{Z}/3\mathbb{Z}$, the entropy of the difference is always at least as large as that of the sum. These results are closely related to the study of more-sum-than-difference (i.e. MSTD) sets in additive combinatorics. We also investigate polar codes for $q$-ary input channels using non-canonical kernels to construct the generator matrix, and present applications of our results to constructing polar codes with significantly improved error probability compared to the canonical construction. ",Computer Science - Information Theory ; ,"Abbe, Emmanuel ; Li, Jiange ; Madiman, Mokshay ; "
http://arxiv.org/abs/1512.00327,Technical Privacy Metrics: a Systematic Survey,"  The goal of privacy metrics is to measure the degree of privacy enjoyed by users in a system and the amount of protection offered by privacy-enhancing technologies. In this way, privacy metrics contribute to improving user privacy in the digital world. The diversity and complexity of privacy metrics in the literature makes an informed choice of metrics challenging. As a result, instead of using existing metrics, new metrics are proposed frequently, and privacy studies are often incomparable. In this survey we alleviate these problems by structuring the landscape of privacy metrics. To this end, we explain and discuss a selection of over eighty privacy metrics and introduce categorizations based on the aspect of privacy they measure, their required inputs, and the type of data that needs protection. In addition, we present a method on how to choose privacy metrics based on nine questions that help identify the right privacy metrics for a given scenario, and highlight topics where additional work on privacy metrics is needed. Our survey spans multiple privacy domains and can be understood as a general framework for privacy measurement. ",Computer Science - Cryptography and Security ; Computer Science - Information Theory ; Computer Science - Performance ; ,"Wagner, Isabel ; Eckhoff, David ; "
